{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. ì¶”ë¡  (Inference)\n",
    "### í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒˆ ì˜ìƒ ê°•ì¡° êµ¬ê°„ ê²€ì¶œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ ì¶”ë¡  ì‹œì‘: 20251215_195125.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Segments: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 303/303 [30:30<00:00,  6.04s/it]\n",
      "/tmp/ipykernel_4154608/3190385592.py:234: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y_full, sr = librosa.load(video_path, sr=AUDIO_CONFIG['sample_rate'])\n",
      "/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì¤‘: /home/stu/ai_project/ì‹œì—°/total_tensor.pt\n",
      "ğŸ”§ ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "  âœ… Gesture Model ë¡œë“œ ì™„ë£Œ: /home/stu/ai_project/ëª¨ë¸ì§‘í•©/gesture_model.pt\n",
      "  âœ… Audio Model ë¡œë“œ ì™„ë£Œ: /home/stu/ai_project/ëª¨ë¸ì§‘í•©/best_bi_lstm.pth\n",
      "  â„¹ï¸ Text Model: klue/roberta-large (Dim: 1024)\n",
      "âœ… Fusion ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ“Š ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: 303\n",
      "\n",
      "ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼(ì ìˆ˜)ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: /home/stu/ai_project/ì‹œì—°/20251215_195125_results.json\n",
      "\n",
      "[ğŸ¯ ê°•ì¡° êµ¬ê°„ ì˜ˆì¸¡ ê²°ê³¼ (ì„ê³„ê°’ 0.5 ê¸°ì¤€)]\n",
      "â° 2.50ì´ˆ ~ 3.50ì´ˆ | ì ìˆ˜: 0.8511\n",
      "â° 3.50ì´ˆ ~ 4.50ì´ˆ | ì ìˆ˜: 0.6189\n",
      "â° 5.00ì´ˆ ~ 6.00ì´ˆ | ì ìˆ˜: 0.8406\n",
      "â° 6.00ì´ˆ ~ 7.00ì´ˆ | ì ìˆ˜: 0.8173\n",
      "â° 7.00ì´ˆ ~ 8.00ì´ˆ | ì ìˆ˜: 0.8308\n",
      "â° 9.00ì´ˆ ~ 10.00ì´ˆ | ì ìˆ˜: 0.7637\n",
      "â° 11.00ì´ˆ ~ 12.00ì´ˆ | ì ìˆ˜: 0.8489\n",
      "â° 12.50ì´ˆ ~ 13.50ì´ˆ | ì ìˆ˜: 0.8030\n",
      "â° 16.00ì´ˆ ~ 17.00ì´ˆ | ì ìˆ˜: 0.9051\n",
      "â° 19.50ì´ˆ ~ 20.50ì´ˆ | ì ìˆ˜: 0.7852\n",
      "â° 22.50ì´ˆ ~ 23.50ì´ˆ | ì ìˆ˜: 0.8407\n",
      "â° 24.00ì´ˆ ~ 25.00ì´ˆ | ì ìˆ˜: 0.8473\n",
      "â° 27.50ì´ˆ ~ 28.50ì´ˆ | ì ìˆ˜: 0.7552\n",
      "â° 29.00ì´ˆ ~ 30.00ì´ˆ | ì ìˆ˜: 1.0000\n",
      "â° 29.50ì´ˆ ~ 30.50ì´ˆ | ì ìˆ˜: 0.9979\n",
      "â° 30.00ì´ˆ ~ 31.00ì´ˆ | ì ìˆ˜: 0.7849\n",
      "â° 40.00ì´ˆ ~ 41.00ì´ˆ | ì ìˆ˜: 0.5301\n",
      "â° 40.50ì´ˆ ~ 41.50ì´ˆ | ì ìˆ˜: 0.8126\n",
      "â° 43.00ì´ˆ ~ 44.00ì´ˆ | ì ìˆ˜: 0.8506\n",
      "â° 44.00ì´ˆ ~ 45.00ì´ˆ | ì ìˆ˜: 0.7772\n",
      "â° 45.00ì´ˆ ~ 46.00ì´ˆ | ì ìˆ˜: 0.8167\n",
      "â° 46.00ì´ˆ ~ 47.00ì´ˆ | ì ìˆ˜: 0.8820\n",
      "â° 49.50ì´ˆ ~ 50.50ì´ˆ | ì ìˆ˜: 0.7881\n",
      "â° 50.50ì´ˆ ~ 51.50ì´ˆ | ì ìˆ˜: 0.8068\n",
      "â° 52.50ì´ˆ ~ 53.50ì´ˆ | ì ìˆ˜: 0.6230\n",
      "â° 53.00ì´ˆ ~ 54.00ì´ˆ | ì ìˆ˜: 0.8055\n",
      "â° 54.50ì´ˆ ~ 55.50ì´ˆ | ì ìˆ˜: 0.8610\n",
      "â° 58.50ì´ˆ ~ 59.50ì´ˆ | ì ìˆ˜: 0.8690\n",
      "â° 60.00ì´ˆ ~ 61.00ì´ˆ | ì ìˆ˜: 0.7102\n",
      "â° 61.00ì´ˆ ~ 62.00ì´ˆ | ì ìˆ˜: 0.8491\n",
      "â° 61.50ì´ˆ ~ 62.50ì´ˆ | ì ìˆ˜: 0.8749\n",
      "â° 62.00ì´ˆ ~ 63.00ì´ˆ | ì ìˆ˜: 0.8672\n",
      "â° 64.00ì´ˆ ~ 65.00ì´ˆ | ì ìˆ˜: 0.8639\n",
      "â° 64.50ì´ˆ ~ 65.50ì´ˆ | ì ìˆ˜: 0.8455\n",
      "â° 65.00ì´ˆ ~ 66.00ì´ˆ | ì ìˆ˜: 0.8467\n",
      "â° 68.50ì´ˆ ~ 69.50ì´ˆ | ì ìˆ˜: 0.8350\n",
      "â° 69.50ì´ˆ ~ 70.50ì´ˆ | ì ìˆ˜: 0.8682\n",
      "â° 70.00ì´ˆ ~ 71.00ì´ˆ | ì ìˆ˜: 0.8478\n",
      "â° 71.50ì´ˆ ~ 72.50ì´ˆ | ì ìˆ˜: 0.8982\n",
      "â° 73.50ì´ˆ ~ 74.50ì´ˆ | ì ìˆ˜: 0.8489\n",
      "â° 74.50ì´ˆ ~ 75.50ì´ˆ | ì ìˆ˜: 0.8532\n",
      "â° 75.50ì´ˆ ~ 76.50ì´ˆ | ì ìˆ˜: 0.7963\n",
      "â° 76.50ì´ˆ ~ 77.50ì´ˆ | ì ìˆ˜: 0.9859\n",
      "â° 77.50ì´ˆ ~ 78.50ì´ˆ | ì ìˆ˜: 0.8674\n",
      "â° 78.00ì´ˆ ~ 79.00ì´ˆ | ì ìˆ˜: 0.8257\n",
      "â° 79.50ì´ˆ ~ 80.50ì´ˆ | ì ìˆ˜: 0.7815\n",
      "â° 84.00ì´ˆ ~ 85.00ì´ˆ | ì ìˆ˜: 0.8648\n",
      "â° 84.50ì´ˆ ~ 85.50ì´ˆ | ì ìˆ˜: 0.8226\n",
      "â° 86.00ì´ˆ ~ 87.00ì´ˆ | ì ìˆ˜: 0.9785\n",
      "â° 87.50ì´ˆ ~ 88.50ì´ˆ | ì ìˆ˜: 0.8065\n",
      "â° 90.00ì´ˆ ~ 91.00ì´ˆ | ì ìˆ˜: 0.8998\n",
      "â° 92.50ì´ˆ ~ 93.50ì´ˆ | ì ìˆ˜: 0.7497\n",
      "â° 94.50ì´ˆ ~ 95.50ì´ˆ | ì ìˆ˜: 0.8432\n",
      "â° 95.00ì´ˆ ~ 96.00ì´ˆ | ì ìˆ˜: 0.8086\n",
      "â° 97.00ì´ˆ ~ 98.00ì´ˆ | ì ìˆ˜: 0.6479\n",
      "â° 98.50ì´ˆ ~ 99.50ì´ˆ | ì ìˆ˜: 0.7971\n",
      "â° 99.00ì´ˆ ~ 100.00ì´ˆ | ì ìˆ˜: 0.8279\n",
      "â° 101.50ì´ˆ ~ 102.50ì´ˆ | ì ìˆ˜: 0.8424\n",
      "â° 103.50ì´ˆ ~ 104.50ì´ˆ | ì ìˆ˜: 0.7447\n",
      "â° 105.00ì´ˆ ~ 106.00ì´ˆ | ì ìˆ˜: 0.8357\n",
      "â° 105.50ì´ˆ ~ 106.50ì´ˆ | ì ìˆ˜: 0.6368\n",
      "â° 106.00ì´ˆ ~ 107.00ì´ˆ | ì ìˆ˜: 0.8480\n",
      "â° 106.50ì´ˆ ~ 107.50ì´ˆ | ì ìˆ˜: 1.0000\n",
      "â° 108.00ì´ˆ ~ 109.00ì´ˆ | ì ìˆ˜: 0.5459\n",
      "â° 108.50ì´ˆ ~ 109.50ì´ˆ | ì ìˆ˜: 0.7313\n",
      "â° 110.50ì´ˆ ~ 111.50ì´ˆ | ì ìˆ˜: 0.7899\n",
      "â° 112.00ì´ˆ ~ 113.00ì´ˆ | ì ìˆ˜: 0.8565\n",
      "â° 112.50ì´ˆ ~ 113.50ì´ˆ | ì ìˆ˜: 0.8705\n",
      "â° 113.50ì´ˆ ~ 114.50ì´ˆ | ì ìˆ˜: 0.8283\n",
      "â° 114.50ì´ˆ ~ 115.50ì´ˆ | ì ìˆ˜: 0.8294\n",
      "â° 115.50ì´ˆ ~ 116.50ì´ˆ | ì ìˆ˜: 0.8254\n",
      "â° 116.00ì´ˆ ~ 117.00ì´ˆ | ì ìˆ˜: 0.8567\n",
      "â° 118.50ì´ˆ ~ 119.50ì´ˆ | ì ìˆ˜: 0.8163\n",
      "â° 120.00ì´ˆ ~ 121.00ì´ˆ | ì ìˆ˜: 0.9116\n",
      "â° 120.50ì´ˆ ~ 121.50ì´ˆ | ì ìˆ˜: 0.7789\n",
      "â° 123.50ì´ˆ ~ 124.50ì´ˆ | ì ìˆ˜: 0.9988\n",
      "â° 126.00ì´ˆ ~ 127.00ì´ˆ | ì ìˆ˜: 0.8138\n",
      "â° 131.50ì´ˆ ~ 132.50ì´ˆ | ì ìˆ˜: 0.7757\n",
      "â° 134.00ì´ˆ ~ 135.00ì´ˆ | ì ìˆ˜: 0.8582\n",
      "â° 134.50ì´ˆ ~ 135.50ì´ˆ | ì ìˆ˜: 0.8253\n",
      "â° 135.50ì´ˆ ~ 136.50ì´ˆ | ì ìˆ˜: 0.8419\n",
      "â° 137.50ì´ˆ ~ 138.50ì´ˆ | ì ìˆ˜: 0.7930\n",
      "â° 141.50ì´ˆ ~ 142.50ì´ˆ | ì ìˆ˜: 0.8093\n",
      "â° 143.00ì´ˆ ~ 144.00ì´ˆ | ì ìˆ˜: 0.8467\n",
      "â° 144.00ì´ˆ ~ 145.00ì´ˆ | ì ìˆ˜: 0.8438\n",
      "â° 148.00ì´ˆ ~ 149.00ì´ˆ | ì ìˆ˜: 0.8518\n",
      "â° 149.00ì´ˆ ~ 150.00ì´ˆ | ì ìˆ˜: 0.8706\n",
      "\n",
      "ì´ 87/303 êµ¬ê°„ ê°•ì¡°ë¨.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# 1. ì„¤ì • (í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ë§ì¶¤)\n",
    "# =========================================================\n",
    "SEGMENT_DURATION = 1.0\n",
    "OVERLAP = 0.5\n",
    "RESIZE_HW = (112, 112)\n",
    "CLIP_LEN = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ì˜¤ë””ì˜¤ ì„¤ì •\n",
    "AUDIO_CONFIG = {\n",
    "    'sample_rate': 16000,\n",
    "    'n_mels': 80,\n",
    "    'n_fft': 1024,\n",
    "    'hop_length': 512,\n",
    "    'window_size': 50\n",
    "}\n",
    "\n",
    "# í“¨ì „ ëª¨ë¸ ì„¤ì • (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ FUSION_CONFIG)\n",
    "FUSION_CONFIG = {\n",
    "    \"gesture_dim\": 512,\n",
    "    \"audio_dim\": 128,\n",
    "    \"text_dim\": 1024,\n",
    "    \"hidden_dim\": 256,\n",
    "}\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)\n",
    "MODEL_DIR = \"/home/stu/ai_project/ëª¨ë¸ì§‘í•©\"\n",
    "GESTURE_MODEL_PATH = os.path.join(MODEL_DIR, \"gesture_model.pt\")\n",
    "AUDIO_MODEL_PATH = os.path.join(MODEL_DIR, \"best_bi_lstm.pth\")\n",
    "FUSION_MODEL_PATH = os.path.join(MODEL_DIR, \"fusion_model_gate.pt\")\n",
    "\n",
    "# =========================================================\n",
    "# 2. í´ë˜ìŠ¤ ì •ì˜ (í•™ìŠµ ì½”ë“œ ë³µì‚¬)\n",
    "# =========================================================\n",
    "\n",
    "class AudioFeatureExtractorInferenceStyle:\n",
    "    \"\"\"í•™ìŠµ ì½”ë“œì™€ 100% ë™ì¼í•œ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "    def __init__(self, config=None):\n",
    "        if config is None: config = AUDIO_CONFIG\n",
    "        self.sr = config.get('sample_rate', 16000)\n",
    "        self.n_mels = config.get('n_mels', 80)\n",
    "        self.n_fft = config.get('n_fft', 1024)\n",
    "        self.hop_length = config.get('hop_length', 512)\n",
    "        self.window_size = config.get('window_size', 50)\n",
    "    \n",
    "    def process_full_audio(self, y_audio):\n",
    "        # 1. Waveform Normalization\n",
    "        y_audio = librosa.util.normalize(y_audio)\n",
    "        \n",
    "        # 2. Mel-Spectrogram\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=y_audio, sr=self.sr, n_mels=self.n_mels,\n",
    "            n_fft=self.n_fft, hop_length=self.hop_length\n",
    "        )\n",
    "        features = librosa.power_to_db(mel, ref=np.max).T \n",
    "        \n",
    "        # 3. Global Scaling (íŒŒì¼ ì „ì²´ ê¸°ì¤€)\n",
    "        scaler = StandardScaler()\n",
    "        try:\n",
    "            features_norm = scaler.fit_transform(features)\n",
    "        except ValueError:\n",
    "            return np.zeros((0, self.n_mels), dtype=np.float32)\n",
    "\n",
    "        return features_norm.astype(np.float32)\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    \"\"\"í•™ìŠµëœ Fusion ëª¨ë¸ êµ¬ì¡°\"\"\"\n",
    "    def __init__(self, gesture_dim, audio_dim, text_dim, hidden_dim, num_classes=1, use_text=True, dropout=0.5):\n",
    "        super(GatedFusion, self).__init__()\n",
    "        self.use_text = use_text\n",
    "        \n",
    "        self.g_net = nn.Sequential(\n",
    "            nn.Linear(gesture_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim), nn.Dropout(p=dropout)\n",
    "        )\n",
    "        self.a_net = nn.Sequential(\n",
    "            nn.Linear(audio_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim), nn.Dropout(p=dropout)\n",
    "        )\n",
    "        \n",
    "        if use_text:\n",
    "            self.t_net = nn.Sequential(\n",
    "                nn.Linear(text_dim, hidden_dim), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim), nn.Dropout(p=dropout)\n",
    "            )\n",
    "            gate_input_dim = hidden_dim * 3\n",
    "        else:\n",
    "            gate_input_dim = hidden_dim * 2\n",
    "\n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(gate_input_dim, hidden_dim), nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 3 if use_text else 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, gesture, audio, text=None):\n",
    "        h_g = self.g_net(gesture)\n",
    "        h_a = self.a_net(audio)\n",
    "        features = [h_g, h_a]\n",
    "        \n",
    "        if self.use_text and text is not None:\n",
    "            h_t = self.t_net(text)\n",
    "            features.append(h_t)\n",
    "            \n",
    "        concat_feat = torch.cat(features, dim=1)\n",
    "        gates = self.gate_net(concat_feat)\n",
    "        \n",
    "        h_fused = gates[:, 0:1] * h_g + gates[:, 1:2] * h_a\n",
    "        if self.use_text and text is not None:\n",
    "            h_fused += gates[:, 2:3] * h_t\n",
    "            \n",
    "        return self.classifier(h_fused)\n",
    "\n",
    "# =========================================================\n",
    "# 3. ëª¨ë¸ ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# =========================================================\n",
    "\n",
    "def load_all_models():\n",
    "    \"\"\"ëª¨ë“  ì„œë¸Œ ëª¨ë¸ê³¼ í“¨ì „ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    # 1. Utilsì—ì„œ ì„œë¸Œ ëª¨ë¸ ë¡œë” ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ì í™˜ê²½ì˜ utils.py ê°€ì •)\n",
    "    try:\n",
    "        from utils import load_gesture_model, load_audio_model, create_text_model\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ utils.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ë¥¼ ì§ì ‘ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(\"ğŸ”§ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    \n",
    "    # Gesture & Audio Model Load\n",
    "    g_model = load_gesture_model(GESTURE_MODEL_PATH, DEVICE)\n",
    "    a_model = load_audio_model(AUDIO_MODEL_PATH, DEVICE)\n",
    "    t_model = create_text_model().to(DEVICE) # Text Model (RoBERTa ë“±)\n",
    "\n",
    "    # Fusion Model Load\n",
    "    f_model = GatedFusion(\n",
    "        gesture_dim=FUSION_CONFIG['gesture_dim'],\n",
    "        audio_dim=FUSION_CONFIG['audio_dim'],\n",
    "        text_dim=FUSION_CONFIG['text_dim'],\n",
    "        hidden_dim=FUSION_CONFIG['hidden_dim'],\n",
    "        use_text=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "    if os.path.exists(FUSION_MODEL_PATH):\n",
    "        f_model.load_state_dict(torch.load(FUSION_MODEL_PATH, map_location=DEVICE))\n",
    "        print(\"âœ… Fusion ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(f\"âŒ Fusion ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {FUSION_MODEL_PATH}\")\n",
    "\n",
    "    g_model.eval()\n",
    "    a_model.eval()\n",
    "    t_model.eval()\n",
    "    f_model.eval()\n",
    "    \n",
    "    return g_model, a_model, t_model, f_model\n",
    "\n",
    "def preprocess_new_video(video_path):\n",
    "    \"\"\"\n",
    "    ìƒˆë¡œìš´ ë¹„ë””ì˜¤ì— ëŒ€í•´ í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    Video: Resize -> Normalize\n",
    "    Audio: Scaler -> Slice\n",
    "    Text: (Optional) Placeholder or Embedding extraction\n",
    "    \"\"\"\n",
    "    # 1. ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    step = SEGMENT_DURATION * (1 - OVERLAP)\n",
    "    segments = []\n",
    "    current = 0\n",
    "    while current + SEGMENT_DURATION <= duration:\n",
    "        segments.append((current, current + SEGMENT_DURATION))\n",
    "        current += step\n",
    "\n",
    "    if not segments:\n",
    "        return None, None, None, []\n",
    "\n",
    "    # 2. ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ (process_video_fast ë¡œì§ ê°„ì†Œí™”)\n",
    "    video_tensors = []\n",
    "    \n",
    "    # í”„ë ˆì„ ìºì‹±ì„ ìœ„í•´ ì „ì²´ë¥¼ ì½ê±°ë‚˜, í•„ìš”í•œ ê²ƒë§Œ ì½ìŠµë‹ˆë‹¤. \n",
    "    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì„¸ê·¸ë¨¼íŠ¸ë³„ë¡œ ì²˜ë¦¬ (ì†ë„ë¥¼ ìœ„í•´ ìµœì í™” ê°€ëŠ¥)\n",
    "    for start_sec, end_sec in tqdm(segments, desc=\"Processing Segments\"):\n",
    "        start_frame = int(start_sec * fps)\n",
    "        end_frame = int(end_sec * fps)\n",
    "        \n",
    "        # í”„ë ˆì„ ì¸ë±ìŠ¤ ê³„ì‚° (16ê°œ ê· ë“± ì¶”ì¶œ)\n",
    "        seg_len = end_frame - start_frame\n",
    "        indices = np.linspace(start_frame, end_frame-1, num=CLIP_LEN, dtype=int)\n",
    "        \n",
    "        frames = []\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, RESIZE_HW)\n",
    "                frames.append(frame)\n",
    "            else:\n",
    "                # ì‹¤íŒ¨ì‹œ ê²€ì€ í™”ë©´\n",
    "                frames.append(np.zeros((RESIZE_HW[0], RESIZE_HW[1], 3), dtype=np.uint8))\n",
    "        \n",
    "        # ì •ê·œí™” (0~1 -> -1~1) ë° ì°¨ì› ë³€ê²½ (C, T, H, W)\n",
    "        frames = np.stack(frames).astype(np.float32) / 255.0\n",
    "        frames = np.transpose(frames, (3, 0, 1, 2)) # (C, T, H, W)\n",
    "        frames = (frames - 0.5) / 0.5\n",
    "        video_tensors.append(torch.from_numpy(frames))\n",
    "        \n",
    "    cap.release()\n",
    "\n",
    "    # 3. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (Global Scaling ì ìš©)\n",
    "    audio_extractor = AudioFeatureExtractorInferenceStyle(AUDIO_CONFIG)\n",
    "    audio_tensors = []\n",
    "    \n",
    "    try:\n",
    "        y_full, sr = librosa.load(video_path, sr=AUDIO_CONFIG['sample_rate'])\n",
    "        full_feat = audio_extractor.process_full_audio(y_full)\n",
    "        \n",
    "        frames_per_sec = sr / AUDIO_CONFIG['hop_length']\n",
    "        \n",
    "        for start_sec, end_sec in segments:\n",
    "            start_idx = int(start_sec * frames_per_sec)\n",
    "            end_idx = start_idx + AUDIO_CONFIG['window_size']\n",
    "            \n",
    "            feat_seg = full_feat[start_idx:end_idx]\n",
    "            \n",
    "            # íŒ¨ë”©\n",
    "            if feat_seg.shape[0] < AUDIO_CONFIG['window_size']:\n",
    "                pad = AUDIO_CONFIG['window_size'] - feat_seg.shape[0]\n",
    "                feat_seg = np.pad(feat_seg, ((0, pad), (0, 0)), mode='constant')\n",
    "            \n",
    "            feat_seg = feat_seg[:AUDIO_CONFIG['window_size']]\n",
    "            audio_tensors.append(torch.from_numpy(feat_seg).float())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Audio Error: {e}\")\n",
    "        audio_tensors = [torch.zeros(AUDIO_CONFIG['window_size'], AUDIO_CONFIG['n_mels'])] * len(segments)\n",
    "# ---------------------------------------------------------\n",
    "    # 4. í…ìŠ¤íŠ¸ ì²˜ë¦¬ (List íƒ€ì… ì—ëŸ¬ ìˆ˜ì • ë²„ì „)\n",
    "    # ---------------------------------------------------------\n",
    "    text_tensor_path = \"/home/stu/ai_project/ì‹œì—°/total_tensor.pt\"  # ê²½ë¡œ í™•ì¸\n",
    "    \n",
    "    text_tensors = []\n",
    "    \n",
    "    if os.path.exists(text_tensor_path):\n",
    "        print(f\"â„¹ï¸ í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì¤‘: {text_tensor_path}\")\n",
    "        try:\n",
    "            # 1. íŒŒì¼ ë¡œë“œ\n",
    "            loaded_data = torch.load(text_tensor_path, map_location='cpu')\n",
    "            \n",
    "            # [ë””ë²„ê¹…ìš© ì¶œë ¥] ë°ì´í„° íƒ€ì… í™•ì¸\n",
    "            # print(f\"DEBUG: Loaded data type: {type(loaded_data)}\")\n",
    "            \n",
    "            # 2. ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í…ì„œë¡œ ë³€í™˜ (Stack)\n",
    "            if isinstance(loaded_data, list):\n",
    "                if len(loaded_data) > 0:\n",
    "                    # ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ í…ì„œë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹¨\n",
    "                    # ì˜ˆ: [Tensor(1024), Tensor(1024)] -> Tensor(2, 1024)\n",
    "                    loaded_text = torch.stack(loaded_data)\n",
    "                else:\n",
    "                    # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ 0ìœ¼ë¡œ ì±„ì›€\n",
    "                    loaded_text = torch.zeros(1024)\n",
    "            else:\n",
    "                # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ ì´ë¯¸ í…ì„œë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "                loaded_text = loaded_data\n",
    "\n",
    "            # 3. ì°¨ì› ì¶•ì†Œ (í‰ê· )\n",
    "            # ì´ì œ loaded_textëŠ” ë¬´ì¡°ê±´ Tensorì…ë‹ˆë‹¤.\n",
    "            loaded_text = loaded_text.float()\n",
    "            \n",
    "            # (N, 1024) ë˜ëŠ” (N, 1, 1024) í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í‰ê· ì„ ë‚´ì„œ (1024,)ë¡œ ë§Œë“¦\n",
    "            if loaded_text.dim() > 1:\n",
    "                # ì°¨ì›ì´ (N, 1024) ë“±ì´ë©´ 0ë²ˆ ì°¨ì›(ë¬¸ì¥ ê°œìˆ˜)ì— ëŒ€í•´ í‰ê· \n",
    "                loaded_text = loaded_text.mean(dim=0)\n",
    "            \n",
    "            # í˜¹ì‹œ (1, 1024) ì²˜ëŸ¼ ë‚¨ì•„ìˆìœ¼ë©´ Squeeze\n",
    "            if loaded_text.dim() == 2 and loaded_text.shape[0] == 1:\n",
    "                 loaded_text = loaded_text.squeeze(0)\n",
    "\n",
    "            # 4. ìµœì¢… í˜•íƒœ í™•ì¸ ë° ì ìš©\n",
    "            if loaded_text.shape[0] != 1024:\n",
    "                print(f\"âš ï¸ í…ìŠ¤íŠ¸ ì°¨ì› ì´ìƒ ({loaded_text.shape}). 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\")\n",
    "                loaded_text = torch.zeros(1024)\n",
    "\n",
    "            # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì— ë™ì¼í•œ í…ìŠ¤íŠ¸ í”¼ì²˜ ì ìš©\n",
    "            for _ in segments:\n",
    "                text_tensors.append(loaded_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            print(\"âš ï¸ Zero Tensorë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "            text_tensors = [torch.zeros(1024) for _ in segments]\n",
    "            \n",
    "    else:\n",
    "        print(f\"âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {text_tensor_path}\")\n",
    "        print(\"âš ï¸ Zero Tensorë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "        text_tensors = [torch.zeros(1024) for _ in segments]\n",
    "\n",
    "    return torch.stack(video_tensors), torch.stack(audio_tensors), torch.stack(text_tensors), segments\n",
    "\n",
    "# =========================================================\n",
    "# 4. ì¶”ë¡  ì‹¤í–‰ í•¨ìˆ˜\n",
    "# =========================================================\n",
    "\n",
    "import json\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference_fusion(video_path):\n",
    "    print(f\"\\nğŸš€ ì¶”ë¡  ì‹œì‘: {os.path.basename(video_path)}\")\n",
    "    \n",
    "    # 1. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    v_data, a_data, t_data, segments = preprocess_new_video(video_path)\n",
    "    \n",
    "    if v_data is None:\n",
    "        print(\"âŒ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨\")\n",
    "        return\n",
    "\n",
    "    # 2. ëª¨ë¸ ë¡œë“œ\n",
    "    g_model, a_model, t_model, f_model = load_all_models()\n",
    "    \n",
    "    # 3. ë°°ì¹˜ ë‹¨ìœ„ ì¶”ë¡ \n",
    "    batch_size = 32\n",
    "    num_samples = len(v_data)\n",
    "    results = []\n",
    "    \n",
    "    print(f\"ğŸ“Š ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {num_samples}\")\n",
    "    \n",
    "    # ë°°ì¹˜ ì²˜ë¦¬\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        v_batch = v_data[i:i+batch_size].to(DEVICE)\n",
    "        a_batch = a_data[i:i+batch_size].to(DEVICE)\n",
    "        t_batch = t_data[i:i+batch_size].to(DEVICE)\n",
    "        \n",
    "        _, g_feats = g_model(v_batch, return_feature=True)\n",
    "        _, a_feats = a_model(a_batch, return_feature=True)\n",
    "        \n",
    "        if hasattr(t_model, 'forward'):\n",
    "             _, t_feats = t_model(t_batch, return_feature=True)\n",
    "        else:\n",
    "             t_feats = t_batch \n",
    "\n",
    "        logits = f_model(g_feats, a_feats, t_feats)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "        \n",
    "        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°\n",
    "        for j, prob in enumerate(probs):\n",
    "            idx = i + j\n",
    "            start, end = segments[idx]\n",
    "            \n",
    "            # 'label' í‚¤ ì—†ì´ 'score'ë§Œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "            results.append({\n",
    "                \"start\": round(start, 2),\n",
    "                \"end\": round(end, 2),\n",
    "                \"score\": float(prob)\n",
    "            })\n",
    "            \n",
    "    # 4. ê²°ê³¼ ì €ì¥ (JSON)\n",
    "    save_path = video_path.replace(\".mp4\", \"_results.json\")\n",
    "    # í™•ì¥ì ëŒ€ì†Œë¬¸ì ë¬¸ì œ ë°©ì§€ (.MP4 ë“±)\n",
    "    if save_path == video_path: save_path = video_path + \"_results.json\"\n",
    "        \n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"\\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼(ì ìˆ˜)ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}\")\n",
    "\n",
    "    # 5. ê²°ê³¼ í™”ë©´ ì¶œë ¥ (ìˆ˜ì •ëœ ë¶€ë¶„)\n",
    "    print(\"\\n[ğŸ¯ ê°•ì¡° êµ¬ê°„ ì˜ˆì¸¡ ê²°ê³¼ (ì„ê³„ê°’ 0.5 ê¸°ì¤€)]\")\n",
    "    \n",
    "    # ìˆ˜ì • í•µì‹¬: 'label' í‚¤ ëŒ€ì‹  'score' ì ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    emphasized_segments = [r for r in results if r['score'] >= 0.5]\n",
    "    \n",
    "    for res in emphasized_segments:\n",
    "        print(f\"â° {res['start']:.2f}ì´ˆ ~ {res['end']:.2f}ì´ˆ | ì ìˆ˜: {res['score']:.4f}\")\n",
    "        \n",
    "    print(f\"\\nì´ {len(emphasized_segments)}/{len(results)} êµ¬ê°„ ê°•ì¡°ë¨.\")\n",
    "\n",
    "# =========================================================\n",
    "# ì‹¤í–‰\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # ì¶”ë¡ í•  ìƒˆë¡œìš´ ì˜ìƒ ê²½ë¡œ\n",
    "    new_video = \"/home/stu/ai_project/ì‹œì—°/20251215_195125.mp4\" \n",
    "    \n",
    "    if os.path.exists(new_video):\n",
    "        inference_fusion(new_video)\n",
    "    else:\n",
    "        print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {new_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
