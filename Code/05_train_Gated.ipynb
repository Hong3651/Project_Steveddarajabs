{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion ëª¨ë¸ í•™ìŠµ (ìˆ˜ì • ë²„ì „ v2)\n",
    "### Audio Scaler ì‚¬ìš© + í…ìŠ¤íŠ¸ í”¼ì²˜ ì‹¤ì œ ì‚¬ìš©\n",
    "\n",
    "**ìˆ˜ì •ì‚¬í•­:**\n",
    "- â­ Audio Scaler ì‚¬ìš© (03_train_audioì™€ ë™ì¼í•˜ê²Œ!)\n",
    "- â­ AUDIO_EMPHASIS_CLASSES = [1, 2, 3] (í´ë˜ìŠ¤ 1ë„ ê°•ì¡°!)\n",
    "- í…ìŠ¤íŠ¸ í”¼ì²˜ë¥¼ `tensor.pt` + `text.json`ì—ì„œ ì‹¤ì œë¡œ ë¡œë“œ\n",
    "- í…ìŠ¤íŠ¸ ê°•ì¡°ì ìˆ˜(>=0.7)ë„ ìë™ ë¼ë²¨ë§ì— ë°˜ì˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config.py ì•ˆì— í•™ìŠµ íŒŒë¼ë¯¸í„° ì´ê±°ë¡œ ìˆ˜ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUSION_CONFIG = {\n",
    "    \"weights\": {'gesture': 0.4, 'audio': 0.35, 'text': 0.25},\n",
    "    \"threshold\": 0.6,\n",
    "    \"min_duration\": 0.3,\n",
    "    \"batch_size\": 180, \n",
    "    \"num_epochs\": 100,\n",
    "    \"lr\": 1e-4,         \n",
    "    \"gesture_dim\": 512,\n",
    "    \"audio_dim\": 128,\n",
    "    \"text_dim\": 1024,\n",
    "    \"hidden_dim\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from config import (\n",
    "    GESTURE_CONFIG, AUDIO_CONFIG, VIDEO_RAW_DIR,\n",
    "    GESTURE_MODEL_PATH, AUDIO_MODEL_PATH, FUSION_MODEL_PATH, AUDIO_SCALER_PATH,\n",
    "    MODEL_DIR, ensure_dirs, TEXT_TENSORS_PATH, TEXT_SCORES_PATH\n",
    ") \n",
    "from utils import (\n",
    "    load_gesture_model, load_audio_model, create_text_model,\n",
    "    create_fusion_model\n",
    ")\n",
    "\n",
    "ensure_dirs()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ìˆ˜: {torch.cuda.device_count()}\")\n",
    "print(f\"í˜„ì¬ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Audio Feature Extractor (â­ Scaler ì‚¬ìš©!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from config import AUDIO_CONFIG\n",
    "\n",
    "class AudioFeatureExtractorInferenceStyle:\n",
    "    \"\"\"\n",
    "    Inference ì½”ë“œì™€ 100% ë™ì¼í•œ ë¡œì§ì„ ì ìš©í•œ í´ë˜ìŠ¤\n",
    "    1. ì „ì²´ ì˜¤ë””ì˜¤ Waveform Normalize\n",
    "    2. Mel-Spectrogram -> dB\n",
    "    3. ì „ì²´ ì˜¤ë””ì˜¤ì— ëŒ€í•´ StandardScaler ì ìš© (Global Scaling)\n",
    "    \"\"\"\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            config = AUDIO_CONFIG\n",
    "        \n",
    "        self.sr = config.get('sample_rate', 16000)\n",
    "        self.n_mels = config.get('n_mels', 80)\n",
    "        self.n_fft = config.get('n_fft', 1024)\n",
    "        self.hop_length = config.get('hop_length', 512)\n",
    "        self.window_size = config.get('window_size', 50)\n",
    "        \n",
    "        print(\"âœ… Audio Extractor ì´ˆê¸°í™” (Inferenceì™€ ë™ì¼í•œ Global Scaling ëª¨ë“œ)\")\n",
    "    \n",
    "    def process_full_audio(self, y_audio):\n",
    "        \"\"\"ì „ì²´ ì˜¤ë””ì˜¤ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ í”¼ì²˜ë§µ ìƒì„±\"\"\"\n",
    "        y_audio = librosa.util.normalize(y_audio)\n",
    "        \n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=y_audio, sr=self.sr,\n",
    "            n_mels=self.n_mels,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length\n",
    "        )\n",
    "        features = librosa.power_to_db(mel, ref=np.max).T \n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        try:\n",
    "            features_norm = scaler.fit_transform(features)\n",
    "        except ValueError:\n",
    "            return np.zeros((0, self.n_mels), dtype=np.float32)\n",
    "\n",
    "        return features_norm.astype(np.float32)\n",
    "\n",
    "try:\n",
    "    audio_extractor = AudioFeatureExtractorInferenceStyle(config=AUDIO_CONFIG)\n",
    "    print(\"âœ… Audio Extractor ì ìš© ì™„ë£Œ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_data(tensor_path, json_path):\n",
    "    \"\"\"\n",
    "    04_make_text.ipynbì—ì„œ ìƒì„±í•œ í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "    \"\"\"\n",
    "    text_tensors = []\n",
    "    text_info = []\n",
    "    \n",
    "    if os.path.exists(tensor_path):\n",
    "        tensors = torch.load(tensor_path)\n",
    "        text_tensors = [t.squeeze(0) if t.dim() > 1 else t for t in tensors]\n",
    "        print(f\"  âœ… í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ: {len(text_tensors)}ê°œ\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ í…ìŠ¤íŠ¸ í…ì„œ ì—†ìŒ: {tensor_path}\")\n",
    "    \n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        text_info = data.get('analysis', [])\n",
    "        print(f\"  âœ… í…ìŠ¤íŠ¸ ì •ë³´ ë¡œë“œ: {len(text_info)}ê°œ ë¬¸ì¥\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ í…ìŠ¤íŠ¸ ì •ë³´ ì—†ìŒ: {json_path}\")\n",
    "    \n",
    "    return text_tensors, text_info\n",
    "\n",
    "\n",
    "def get_text_feature_for_segment(start_sec, end_sec, text_tensors, text_info, current_video_path):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ì‹œê°„ êµ¬ê°„ + í•´ë‹¹ ë¹„ë””ì˜¤ íŒŒì¼ì— ë§ëŠ” í…ìŠ¤íŠ¸ í”¼ì²˜ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    if not text_tensors or not text_info:\n",
    "        return torch.zeros(1024)\n",
    "    \n",
    "    matching_tensors = []\n",
    "    \n",
    "    for i, info in enumerate(text_info):\n",
    "\n",
    "        if 'video_path' in info:\n",
    "\n",
    "            if os.path.basename(info['video_path']) != os.path.basename(current_video_path):\n",
    "                continue\n",
    "\n",
    "        sent_start = info.get('start_sec', 0)\n",
    "        sent_end = info.get('end_sec', 0)\n",
    "        \n",
    "        if not (sent_end <= start_sec or sent_start >= end_sec):\n",
    "            if i < len(text_tensors):\n",
    "                matching_tensors.append(text_tensors[i])\n",
    "    \n",
    "    if matching_tensors:\n",
    "        stacked = torch.stack(matching_tensors)\n",
    "        return stacked.mean(dim=0)\n",
    "    else:\n",
    "        return torch.zeros(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì„¤ì • (â­ AUDIO_EMPHASIS_CLASSES ìˆ˜ì •!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_RAW_DIR = \"/home/stu/ai_project/ì˜ìƒraw\" \n",
    "SEGMENT_DURATION = 1.0\n",
    "OVERLAP = 0.5\n",
    "\n",
    "\n",
    "GESTURE_EMPHASIS_CLASSES = [1]      \n",
    "AUDIO_EMPHASIS_CLASSES = [1, 2, 3]  \n",
    "\n",
    "batch_size = FUSION_CONFIG.get('batch_size', 32)\n",
    "num_epochs = FUSION_CONFIG.get('num_epochs', 20)\n",
    "lr = FUSION_CONFIG.get('lr', 1e-3)\n",
    "\n",
    "GPU_BATCH_SIZE = 32\n",
    "\n",
    "print(f\"VIDEO_RAW_DIR: {VIDEO_RAW_DIR}\")\n",
    "print(f\"ì„¸ê·¸ë¨¼íŠ¸: {SEGMENT_DURATION}ì´ˆ, ê²¹ì¹¨: {OVERLAP*100}%\")\n",
    "print(f\"\\nâ­ GESTURE_EMPHASIS_CLASSES: {GESTURE_EMPHASIS_CLASSES}\")\n",
    "print(f\"â­ AUDIO_EMPHASIS_CLASSES: {AUDIO_EMPHASIS_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ”§ ëª¨ë¸ ë¡œë“œ...\")\n",
    "\n",
    "g_model = load_gesture_model(GESTURE_MODEL_PATH, device)\n",
    "a_model = load_audio_model(AUDIO_MODEL_PATH, device)\n",
    "t_model = create_text_model().to(device)\n",
    "\n",
    "g_model.eval()\n",
    "a_model.eval()\n",
    "t_model.eval()\n",
    "\n",
    "for m in [g_model, a_model, t_model]:\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ“ í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ...\")\n",
    "TEXT_TENSORS, TEXT_INFO = load_text_data(TEXT_TENSORS_PATH, TEXT_SCORES_PATH)\n",
    "\n",
    "print(f\"\\ní…ìŠ¤íŠ¸ í…ì„œ ê°œìˆ˜: {len(TEXT_TENSORS)}\")\n",
    "print(f\"í…ìŠ¤íŠ¸ ì •ë³´ ê°œìˆ˜: {len(TEXT_INFO)}\")\n",
    "\n",
    "if TEXT_INFO:\n",
    "    print(f\"\\nì²« ë²ˆì§¸ ë¬¸ì¥ ì˜ˆì‹œ:\")\n",
    "    print(f\"  - ì‹œì‘: {TEXT_INFO[0].get('start_sec', 0):.2f}ì´ˆ\")\n",
    "    print(f\"  - ë: {TEXT_INFO[0].get('end_sec', 0):.2f}ì´ˆ\")\n",
    "    print(f\"  - ê°•ì¡°ì ìˆ˜: {TEXT_INFO[0].get('emphasis_score', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í—¬í¼ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_emphasis_score(start_sec, end_sec, current_video_path):\n",
    "    \"\"\"\n",
    "    í•´ë‹¹ êµ¬ê°„ + í•´ë‹¹ ë¹„ë””ì˜¤ì˜ í…ìŠ¤íŠ¸ ê°•ì¡° ì ìˆ˜ ë°˜í™˜ (ë¹„ë””ì˜¤ ë§¤ì¹­ ì¶”ê°€!)\n",
    "    \"\"\"\n",
    "    if not TEXT_INFO:\n",
    "        return 0.0\n",
    "    \n",
    "    current_file_id = os.path.splitext(os.path.basename(current_video_path))[0]\n",
    "    \n",
    "    scores = []\n",
    "    for info in TEXT_INFO:\n",
    "\n",
    "        info_file_id = info.get('video_filename', '')\n",
    "        if info_file_id != current_file_id:\n",
    "            continue\n",
    "\n",
    "        sent_start = info.get('start_sec', 0)\n",
    "        sent_end = info.get('end_sec', 0)\n",
    "        \n",
    "        if not (sent_end <= start_sec or sent_start >= end_sec):\n",
    "            scores.append(info.get('emphasis_score', 0.0))\n",
    "    \n",
    "    return max(scores) if scores else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë¹„ë””ì˜¤ ì²˜ë¦¬ í•¨ìˆ˜ (â­ Scaler ì ìš©!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_fast(video_path, segment_duration=1.0, overlap=0.5, \n",
    "                       clip_len=16, resize_hw=(112, 112)):\n",
    "    \"\"\"\n",
    "    ë¹„ë””ì˜¤ ì²˜ë¦¬ í•¨ìˆ˜ (ìˆ˜ì •ë¨): \n",
    "    ì˜¤ë””ì˜¤ë¥¼ ì „ì²´ ë³€í™˜ í›„ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ Inference ì½”ë“œì™€ ì •í•©ì„± í™•ë³´\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None, None, []\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    if duration < segment_duration:\n",
    "        cap.release()\n",
    "        return None, None, []\n",
    "    \n",
    "    step = segment_duration * (1 - overlap)\n",
    "    segments = []\n",
    "    current = 0\n",
    "    while current + segment_duration <= duration:\n",
    "        segments.append((current, current + segment_duration))\n",
    "        current += step\n",
    "    \n",
    "    if len(segments) == 0:\n",
    "        cap.release()\n",
    "        return None, None, []\n",
    "\n",
    "    all_needed_frames = set()\n",
    "    segment_frame_indices = []\n",
    "    \n",
    "    for start_sec, end_sec in segments:\n",
    "        start_frame = int(start_sec * fps)\n",
    "        end_frame = int(end_sec * fps)\n",
    "        start_frame = max(0, min(start_frame, total_frames - 1))\n",
    "        end_frame = max(start_frame + 1, min(end_frame, total_frames - 1))\n",
    "        \n",
    "        seg_len = end_frame - start_frame + 1\n",
    "        if seg_len >= clip_len:\n",
    "            indices = np.linspace(start_frame, end_frame, num=clip_len, dtype=int).tolist()\n",
    "        else:\n",
    "            base = np.linspace(start_frame, end_frame, num=seg_len, dtype=int).tolist()\n",
    "            indices = (base * (clip_len // len(base) + 1))[:clip_len]\n",
    "        \n",
    "        segment_frame_indices.append(indices)\n",
    "        all_needed_frames.update(indices)\n",
    "    \n",
    "    all_needed_frames = sorted(all_needed_frames)\n",
    "    frame_cache = {}\n",
    "    current_frame = 0\n",
    "    needed_idx = 0\n",
    "    \n",
    "    while needed_idx < len(all_needed_frames):\n",
    "        target_frame = all_needed_frames[needed_idx]\n",
    "        while current_frame < target_frame:\n",
    "            cap.grab()\n",
    "            current_frame += 1\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, resize_hw)\n",
    "            frame_cache[target_frame] = frame\n",
    "        current_frame += 1\n",
    "        needed_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    video_clips = []\n",
    "    dummy_frame = np.zeros((resize_hw[0], resize_hw[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    for indices in segment_frame_indices:\n",
    "        frames = [frame_cache.get(i, dummy_frame) for i in indices]\n",
    "        frames = np.stack(frames, axis=0).astype(np.float32) / 255.0\n",
    "        frames = np.transpose(frames, (3, 0, 1, 2))\n",
    "        frames = (frames - 0.5) / 0.5\n",
    "        video_clips.append(torch.from_numpy(frames))\n",
    "\n",
    "        audio_features_list = []\n",
    "    try:\n",
    "        y_full, sr = librosa.load(video_path, sr=AUDIO_CONFIG['sample_rate'])\n",
    "        \n",
    "        full_audio_features = audio_extractor.process_full_audio(y_full)\n",
    "        \n",
    "        frames_per_sec = sr / AUDIO_CONFIG['hop_length']\n",
    "        window_size = AUDIO_CONFIG['window_size'] # 50\n",
    "        \n",
    "        for start_sec, end_sec in segments:\n",
    "            start_frame_idx = int(start_sec * frames_per_sec)\n",
    "\n",
    "            if start_frame_idx < len(full_audio_features):\n",
    "                feat_seg = full_audio_features[start_frame_idx : start_frame_idx + window_size]\n",
    "            else:\n",
    "                feat_seg = np.zeros((0, AUDIO_CONFIG['n_mels']))\n",
    "\n",
    "            if feat_seg.shape[0] < window_size:\n",
    "                pad_len = window_size - feat_seg.shape[0]\n",
    "                feat_seg = np.pad(feat_seg, ((0, pad_len), (0, 0)), mode='constant')\n",
    "\n",
    "            feat_seg = feat_seg[:window_size]\n",
    "            \n",
    "            audio_features_list.append(torch.from_numpy(feat_seg).float())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Audio processing error: {e}\")\n",
    "        audio_features_list = [torch.zeros(AUDIO_CONFIG['window_size'], AUDIO_CONFIG['n_mels'])] * len(segments)\n",
    "    \n",
    "    return video_clips, audio_features_list, segments\n",
    "\n",
    "print(\"âœ… ë¹„ë””ì˜¤ ì²˜ë¦¬ í•¨ìˆ˜ ì¬ì •ì˜ ì™„ë£Œ (Audio Global Scaling ì ìš©)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ë°ì´í„° ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_all_data(video_raw_dir):\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬í•˜ì—¬ í•™ìŠµ ë°ì´í„° ìƒì„±\n",
    "    \"\"\"\n",
    "    video_files = sorted(glob.glob(os.path.join(video_raw_dir, \"*.mp4\")))\n",
    "    print(f\"\\nğŸ“¹ ë¹„ë””ì˜¤ ìˆ˜: {len(video_files)}ê°œ\")\n",
    "    \n",
    "    all_data = []\n",
    "\n",
    "    audio_class_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "    \n",
    "    for vid_idx, video_path in enumerate(video_files):\n",
    "        video_name = os.path.basename(video_path)\n",
    "        \n",
    "        video_clips, audio_features, segments = process_video_fast(\n",
    "            video_path,\n",
    "            segment_duration=SEGMENT_DURATION,\n",
    "            overlap=OVERLAP,\n",
    "            clip_len=GESTURE_CONFIG['clip_len'],\n",
    "            resize_hw=GESTURE_CONFIG['resize_hw']\n",
    "        )\n",
    "        \n",
    "        if video_clips is None or len(video_clips) == 0:\n",
    "            print(f\"  [{vid_idx+1}/{len(video_files)}] {video_name}: ìŠ¤í‚µ\")\n",
    "            continue\n",
    "        \n",
    "        n_segs = len(video_clips)\n",
    "        print(f\"  [{vid_idx+1}/{len(video_files)}] {video_name}: {n_segs} ì„¸ê·¸ë¨¼íŠ¸\", end=\" \")\n",
    "        \n",
    "        for batch_start in range(0, n_segs, GPU_BATCH_SIZE):\n",
    "            batch_end = min(batch_start + GPU_BATCH_SIZE, n_segs)\n",
    "            \n",
    "            batch_video = torch.stack(video_clips[batch_start:batch_end]).to(device)\n",
    "            batch_audio = torch.stack(audio_features[batch_start:batch_end]).to(device)\n",
    "\n",
    "            batch_text_list = []\n",
    "            for j in range(batch_start, batch_end):\n",
    "                start_sec, end_sec = segments[j]\n",
    "\n",
    "                text_feat = get_text_feature_for_segment(\n",
    "                    start_sec, end_sec, TEXT_TENSORS, TEXT_INFO, video_path\n",
    "                )\n",
    "                batch_text_list.append(text_feat)\n",
    "            \n",
    "            batch_text = torch.stack(batch_text_list).to(device)\n",
    "\n",
    "            g_logits, g_feats = g_model(batch_video, return_feature=True)\n",
    "            a_logits, a_feats = a_model(batch_audio, return_feature=True)\n",
    "            _, t_feats = t_model(batch_text, return_feature=True)\n",
    "            \n",
    "            g_preds = torch.argmax(g_logits, dim=1).cpu().numpy()\n",
    "            a_preds = torch.argmax(a_logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            for j in range(batch_end - batch_start):\n",
    "                seg_idx = batch_start + j\n",
    "                g_pred = int(g_preds[j])\n",
    "                a_pred = int(a_preds[j])\n",
    "                start_sec, end_sec = segments[seg_idx]\n",
    "\n",
    "                audio_class_counts[a_pred] = audio_class_counts.get(a_pred, 0) + 1\n",
    "\n",
    "                text_emphasis = get_text_emphasis_score(start_sec, end_sec, video_path)\n",
    "\n",
    "                vote_score = 0\n",
    "\n",
    "                if g_pred in GESTURE_EMPHASIS_CLASSES:\n",
    "                    vote_score += 1\n",
    "\n",
    "                if a_pred in AUDIO_EMPHASIS_CLASSES:\n",
    "                    vote_score += 1\n",
    "\n",
    "                if text_emphasis >= 0.8:\n",
    "                    vote_score += 1\n",
    "\n",
    "                auto_label = 1 if vote_score >= 2 else 0\n",
    "                \n",
    "                all_data.append({\n",
    "                    'gesture_feat': g_feats[j].cpu(),\n",
    "                    'audio_feat': a_feats[j].cpu(),\n",
    "                    'text_feat': t_feats[j].cpu(),\n",
    "                    'gesture_pred': g_pred,\n",
    "                    'audio_pred': a_pred,\n",
    "                    'text_emphasis': text_emphasis,\n",
    "                    'auto_label': auto_label,\n",
    "                    'video_path': video_path,\n",
    "                    'start_sec': start_sec,\n",
    "                    'end_sec': end_sec,\n",
    "                })\n",
    "            \n",
    "        \n",
    "        print(\"âœ“\")\n",
    "        \n",
    "        del video_clips, audio_features\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Audio í´ë˜ìŠ¤ ë¶„í¬: {audio_class_counts}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ë°ì´í„° ìƒì„± ë˜ëŠ” ë¡œë“œ\n",
    "\n",
    "âš ï¸ **ì¤‘ìš”**: ì´ì „ì— ìƒì„±í•œ `auto_labeled_data_with_text.pt`ê°€ ìˆìœ¼ë©´ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(MODEL_DIR, \"auto_labeled_data_gate.pt\")\n",
    "\n",
    "if os.path.exists(SAVE_PATH):\n",
    "    print(f\"ğŸ—‘ï¸ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì¤‘: {SAVE_PATH}\")\n",
    "    os.remove(SAVE_PATH)\n",
    "\n",
    "print(\"\\nğŸš€ ë°ì´í„° ìƒì„± ì‹œì‘...\")\n",
    "all_data = generate_all_data(VIDEO_RAW_DIR)\n",
    "print(f\"\\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘... ({SAVE_PATH})\")\n",
    "torch.save(all_data, SAVE_PATH)\n",
    "print(\"âœ… ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ğŸ“Š ì´ ì„¸ê·¸ë¨¼íŠ¸: {len(all_data)}\")\n",
    "\n",
    "labels = [d['auto_label'] for d in all_data]\n",
    "num_pos = sum(labels)\n",
    "num_neg = len(labels) - num_pos\n",
    "print(f\"ğŸ“Š ê°•ì¡°(1): {num_pos}, ë¹„ê°•ì¡°(0): {num_neg}\")\n",
    "\n",
    "g_preds = [d['gesture_pred'] for d in all_data]\n",
    "a_preds = [d['audio_pred'] for d in all_data]\n",
    "print(f\"ğŸ“Š Gesture: {dict(zip(*np.unique(g_preds, return_counts=True)))}\")\n",
    "print(f\"ğŸ“Š Audio: {dict(zip(*np.unique(a_preds, return_counts=True)))}\")\n",
    "\n",
    "text_emph = [d.get('text_emphasis', 0) for d in all_data]\n",
    "high_text = sum(1 for t in text_emph if t >= 0.8)\n",
    "print(f\"ğŸ“Š Text (>=0.8): {high_text}ê°œ\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Audio í´ë˜ìŠ¤ë³„ ë¶„í¬:\")\n",
    "print(f\"   - í´ë˜ìŠ¤ 0 (Normal/ë¹„ê°•ì¡°): {a_preds.count(0)}ê°œ\")\n",
    "print(f\"   - í´ë˜ìŠ¤ 1 (Pause_Talk/ê°•ì¡°): {a_preds.count(1)}ê°œ\")\n",
    "print(f\"   - í´ë˜ìŠ¤ 2 (High_Tone/ê°•ì¡°): {a_preds.count(2)}ê°œ\")\n",
    "print(f\"   - í´ë˜ìŠ¤ 3 (Loud/ê°•ì¡°): {a_preds.count(3)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        d = self.data[idx]\n",
    "        return (\n",
    "            d['gesture_feat'], \n",
    "            d['audio_feat'], \n",
    "            d['text_feat'], \n",
    "            torch.tensor(d['auto_label'], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "video_groups = {}\n",
    "for item in all_data:\n",
    "    vp = item['video_path']\n",
    "    if vp not in video_groups:\n",
    "        video_groups[vp] = []\n",
    "    video_groups[vp].append(item)\n",
    "\n",
    "videos = list(video_groups.keys())\n",
    "random.shuffle(videos)\n",
    "split_idx = int(len(videos) * 0.8)\n",
    "train_videos = set(videos[:split_idx])\n",
    "\n",
    "train_data = [item for v, items in video_groups.items() if v in train_videos for item in items]\n",
    "val_data = [item for v, items in video_groups.items() if v not in train_videos for item in items]\n",
    "\n",
    "print(f\"ğŸ“Š Train: {len(train_data)}, Val: {len(val_data)}\")\n",
    "\n",
    "train_labels = [d['auto_label'] for d in train_data]\n",
    "n_neg, n_pos = train_labels.count(0), train_labels.count(1)\n",
    "pos_weight = torch.tensor([n_neg/n_pos if n_pos > 0 else 1.0]).to(device)\n",
    "print(f\"ğŸ“Š Pos weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "train_loader = DataLoader(FusionDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(FusionDataset(val_data), batch_size=batch_size, shuffle=False)\n",
    "print(f\"ğŸ“Š Batches - Train: {len(train_loader)}, Val: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Fusion ëª¨ë¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# =============================================================================\n",
    "# Gated Fusion ê¸°ë°˜ ëª¨ë¸ ì •ì˜\n",
    "# =============================================================================\n",
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, gesture_dim, audio_dim, text_dim, hidden_dim, num_classes, \n",
    "                 use_text=True, dropout=0.5):\n",
    "        super(GatedFusion, self).__init__()\n",
    "        self.use_text = use_text\n",
    "        \n",
    "        self.g_net = nn.Sequential(\n",
    "            nn.Linear(gesture_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "        self.a_net = nn.Sequential(\n",
    "            nn.Linear(audio_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "        \n",
    "        if use_text:\n",
    "            self.t_net = nn.Sequential(\n",
    "                nn.Linear(text_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(p=dropout)\n",
    "            )\n",
    "            gate_input_dim = hidden_dim * 3\n",
    "        else:\n",
    "            gate_input_dim = hidden_dim * 2\n",
    "\n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(gate_input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 3 if use_text else 2), \n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, gesture, audio, text=None):\n",
    "        h_g = self.g_net(gesture)\n",
    "        h_a = self.a_net(audio)\n",
    "        \n",
    "        features = [h_g, h_a]\n",
    "        \n",
    "        if self.use_text and text is not None:\n",
    "            h_t = self.t_net(text)\n",
    "            features.append(h_t)\n",
    "            \n",
    "        concat_feat = torch.cat(features, dim=1)\n",
    "        gates = self.gate_net(concat_feat) \n",
    "        \n",
    "        h_fused = gates[:, 0:1] * h_g + gates[:, 1:2] * h_a\n",
    "        \n",
    "        if self.use_text and text is not None:\n",
    "            h_fused += gates[:, 2:3] * h_t\n",
    "            \n",
    "        return self.classifier(h_fused)\n",
    "\n",
    "def create_fusion_model(gesture_dim, audio_dim, text_dim, hidden_dim, num_classes, use_text=True, dropout=0.5):\n",
    "    return GatedFusion(\n",
    "        gesture_dim, audio_dim, text_dim, hidden_dim, num_classes, \n",
    "        use_text=use_text, dropout=dropout\n",
    "    )\n",
    "\n",
    "print(\"âœ… Gated Fusion ê¸°ë°˜ ëª¨ë¸ ì •ì˜ ì™„ë£Œ\")\n",
    "\n",
    "# =============================================================================\n",
    "# ëª¨ë¸ ìƒì„± ë° ì„¤ì • \n",
    "# =============================================================================\n",
    "\n",
    "f_model = create_fusion_model(\n",
    "    gesture_dim=FUSION_CONFIG['gesture_dim'],\n",
    "    audio_dim=FUSION_CONFIG['audio_dim'],\n",
    "    text_dim=FUSION_CONFIG['text_dim'],\n",
    "    hidden_dim=FUSION_CONFIG['hidden_dim'], \n",
    "    num_classes=1,\n",
    "    use_text=True,\n",
    "    dropout=0.5 \n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = optim.Adam(f_model.parameters(), lr=FUSION_CONFIG['lr'], weight_decay=1e-3)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š íŒŒë¼ë¯¸í„° ê°œìˆ˜: {sum(p.numel() for p in f_model.parameters()):,}\")\n",
    "print(f_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    ì •ë‹µ(y_true)ê³¼ ì˜ˆì¸¡(y_pred)ì„ ë°›ì•„ Acc, Precision, Recall, F1(Macro)ì„ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [], \n",
    "    'train_acc': [], 'val_acc': [],\n",
    "    'val_f1': [], 'val_precision': [], 'val_recall': [] # ê¸°ë¡ìš© ë¦¬ìŠ¤íŠ¸ ì¶”ê°€\n",
    "}\n",
    "\n",
    "best_val_f1 = 0.0  \n",
    "patience_counter = 0\n",
    "patience = 10\n",
    "\n",
    "print(f\"\\nğŸš€ í•™ìŠµ ì‹œì‘! (epochs={num_epochs})\")\n",
    "print(\"=\" * 90) \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    f_model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "    \n",
    "    for g_feat, a_feat, t_feat, labels in train_loader:\n",
    "        g_feat = g_feat.to(device)\n",
    "        a_feat = a_feat.to(device)\n",
    "        t_feat = t_feat.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = f_model(g_feat, a_feat, t_feat).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc, train_prec, train_rec, train_f1 = calculate_metrics(train_targets, train_preds)\n",
    "    \n",
    "    f_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for g_feat, a_feat, t_feat, labels in val_loader:\n",
    "            g_feat = g_feat.to(device)\n",
    "            a_feat = a_feat.to(device)\n",
    "            t_feat = t_feat.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = f_model(g_feat, a_feat, t_feat).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc, val_prec, val_rec, val_f1 = calculate_metrics(val_targets, val_preds)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1:03d}/{num_epochs}] \"\n",
    "          f\"Loss: {train_loss:.4f}/{val_loss:.4f} | \"\n",
    "          f\"Acc: {train_acc*100:.1f}%/{val_acc*100:.1f}% | \"\n",
    "          f\"F1(Macro): {val_f1:.4f} | Pre: {val_prec:.4f} | Rec: {val_rec:.4f}\", end=\"\")\n",
    "    \n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(f_model.state_dict(), FUSION_MODEL_PATH)\n",
    "        print(f\" âœ… Best F1!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print()\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nğŸ›‘ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ! Best Val F1: {best_val_f1:.4f}\")\n",
    "print(f\"ëª¨ë¸ ì €ì¥: {FUSION_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, \"fusion_history.json\"), 'w') as f:\n",
    "    json.dump(history, f)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(history['train_loss'], label='Train')\n",
    "ax[0].plot(history['val_loss'], label='Val')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "\n",
    "ax[1].plot(history['train_acc'], label='Train')\n",
    "ax[1].plot(history['val_acc'], label='Val')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'fusion_curves.png'))\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š ê·¸ë˜í”„ ì €ì¥: {os.path.join(MODEL_DIR, 'fusion_curves.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì™„ë£Œ!\n",
    "\n",
    "### ìˆ˜ì •ì‚¬í•­ (v2):\n",
    "1. âœ… **Audio Scaler ì‚¬ìš©** - `audio_scaler.pkl` ë¡œë“œí•´ì„œ ì ìš©\n",
    "2. âœ… **AUDIO_EMPHASIS_CLASSES = [1, 2, 3]** - í´ë˜ìŠ¤ 1ë„ ê°•ì¡°ë¡œ ì¸ì‹\n",
    "3. âœ… í…ìŠ¤íŠ¸ í”¼ì²˜ë¥¼ `tensor.pt`ì—ì„œ ì‹¤ì œë¡œ ë¡œë“œ\n",
    "4. âœ… í…ìŠ¤íŠ¸ ê°•ì¡°ì ìˆ˜(>=0.7)ë„ ìë™ ë¼ë²¨ë§ì— ë°˜ì˜\n",
    "\n",
    "### ìƒì„±ëœ íŒŒì¼:\n",
    "- `ëª¨ë¸ì§‘í•©/fusion_model.pt`\n",
    "- `ëª¨ë¸ì§‘í•©/auto_labeled_data_with_text_v2.pt` (ìƒˆ ë²„ì „)\n",
    "- `ëª¨ë¸ì§‘í•©/fusion_history.json`\n",
    "- `ëª¨ë¸ì§‘í•©/fusion_curves.png`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
