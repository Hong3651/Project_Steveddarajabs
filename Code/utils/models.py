"""
models.py - ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì •ì˜
ê¸°ì¡´ íŒŒì¼ì˜ ë§¨ ì•„ë˜ì— ë‹¤ìŒ ì½”ë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
"""
import os
import torch
import torch.nn as nn
from torchvision.models.video import r3d_18, R3D_18_Weights


# =============================================================================
# ê³µí†µ ìœ í‹¸: ì•ˆì „í•œ ê°€ì¤‘ì¹˜ ë¡œë“œ í•¨ìˆ˜
# =============================================================================
def load_state_dict_safe(model, path, device):
    """
    DataParallelë¡œ ì €ì¥ëœ ëª¨ë¸(module. prefix)ë„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    if not os.path.exists(path):
        print(f"  âš ï¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ì—†ìŒ: {path}")
        return model
        
    state_dict = torch.load(path, map_location=device)
    new_state_dict = {}
    
    for k, v in state_dict.items():
        if k.startswith("module."):
            name = k[7:]
        else:
            name = k
        new_state_dict[name] = v
            
    try:
        model.load_state_dict(new_state_dict, strict=False)
        print(f"  âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {os.path.basename(path)}")
    except Exception as e:
        print(f"  âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        
    return model


# =============================================================================
# 1. ì œìŠ¤ì²˜ ëª¨ë¸ (Video)
# =============================================================================
class GestureModel(nn.Module):
    def __init__(self, num_classes=2, pretrained=True):
        super(GestureModel, self).__init__()
        try:
            weights = R3D_18_Weights.KINETICS400_V1 if pretrained else None
            self.backbone = r3d_18(weights=weights)
        except:
            self.backbone = r3d_18(pretrained=pretrained)

        in_features = self.backbone.fc.in_features  # 512
        self.backbone.fc = nn.Identity()
        self.classifier = nn.Linear(in_features, num_classes)
        self.feature_dim = in_features  # Fusionìš©

    def forward(self, x, return_feature=False):
        features = self.backbone(x)  # (B, 512)
        logits = self.classifier(features)
        if return_feature:
            return logits, features
        return logits


def create_gesture_model(num_classes=2, pretrained=True):
    return GestureModel(num_classes=num_classes, pretrained=pretrained)


def load_gesture_model(path, device, num_classes=2):
    model = create_gesture_model(num_classes=num_classes)
    if os.path.exists(path):
        state = torch.load(path, map_location=device)
        new_state = {}
        for k, v in state.items():
            k_new = k
            if k_new.startswith("module."):
                k_new = k_new[7:]
            if 'fc' in k_new and 'backbone' not in k_new:
                k_new = k_new.replace('fc', 'classifier')
            new_state[k_new] = v
                
        try:
            model.load_state_dict(new_state, strict=False)
            print(f"  âœ… Gesture Model ë¡œë“œ ì™„ë£Œ: {path}")
        except Exception as e:
            print(f"  âŒ Gesture Model ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        print(f"  âš ï¸ Gesture Model íŒŒì¼ ì—†ìŒ: {path}")
        
    model.to(device).eval()
    return model


# =============================================================================
# 2. ì˜¤ë””ì˜¤ ëª¨ë¸ (Audio BiLSTM)
# =============================================================================
class AudioLSTMModel(nn.Module):
    def __init__(self, input_size=80, hidden_size=64, num_layers=2,
                 num_classes=4, dropout=0.2):
        super(AudioLSTMModel, self).__init__()

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )

        self.feature_dim = hidden_size * 2  # 128 (BiLSTM)

        self.classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(dropout),
            nn.Linear(64, num_classes)
        )
        self.fc = nn.Linear(self.feature_dim, num_classes)
        self.use_simple_fc = True

    def forward(self, x, return_feature=False):
        lstm_out, _ = self.lstm(x)
        features = lstm_out[:, -1, :]
        
        if self.use_simple_fc:
            logits = self.fc(features)
        else:
            logits = self.classifier(features)
        
        if return_feature:
            return logits, features
        return logits


class BiLSTM(nn.Module):
    def __init__(self, input_dim=80, hidden_size=64, num_classes=4):
        super(BiLSTM, self).__init__()
        self.lstm = nn.LSTM(
            input_dim, hidden_size, 
            num_layers=2, 
            batch_first=True, 
            dropout=0.2, 
            bidirectional=True
        )
        self.fc = nn.Linear(hidden_size * 2, num_classes)
        self.feature_dim = hidden_size * 2

    def forward(self, x, return_feature=False):
        out, _ = self.lstm(x)
        features = out[:, -1, :]
        logits = self.fc(features)
        if return_feature:
            return logits, features
        return logits


def create_audio_model(input_size=80, num_classes=4, hidden_size=64,
                       num_layers=2, dropout=0.2):
    return AudioLSTMModel(
        input_size=input_size,
        hidden_size=hidden_size,
        num_layers=num_layers,
        num_classes=num_classes,
        dropout=dropout
    )


def load_audio_model(path, device, input_size=80, num_classes=4,hidden_size=64,num_layers=2,dropout=0.3):
    model = create_audio_model(input_size=input_size, num_classes=num_classes)
    
    if os.path.exists(path):
        state = torch.load(path, map_location=device)
        if isinstance(state, dict):
            new_state = {}
            for k, v in state.items():
                if k.startswith("module."):
                    new_state[k[7:]] = v
                else:
                    new_state[k] = v
            model.load_state_dict(new_state, strict=False)
            print(f"  âœ… Audio Model ë¡œë“œ ì™„ë£Œ: {path}")
        else:
            print(f"  âš ï¸ Invalid state format: {path}")
    else:
        print(f"  âš ï¸ Audio Model íŒŒì¼ ì—†ìŒ: {path}")
    
    return model.to(device).eval()


AUDIO_CLASS_NAMES = ['Normal', 'Pause_Talk', 'High_Tone', 'Loud']


# =============================================================================
# 3. í…ìŠ¤íŠ¸ ëª¨ë¸ (Text - Dummy BERT)
# =============================================================================
class TextBERTModel(nn.Module):
    def __init__(self, input_dim=1024, num_classes=2):
        super(TextBERTModel, self).__init__()
        self.fc = nn.Linear(input_dim, num_classes)
        self.feature_dim = input_dim

    def forward(self, x, return_feature=False):
        logits = self.fc(x)
        if return_feature:
            return logits, x
        return logits


def create_text_model(model_name="klue/roberta-large", num_classes=2):
    if "large" in model_name:
        input_dim = 1024
    else:
        input_dim = 768
    print(f"  â„¹ï¸ Text Model: {model_name} (Dim: {input_dim})")
    return TextBERTModel(input_dim=input_dim, num_classes=num_classes)


def load_text_model(path, device, model_name="klue/roberta-large"):
    model = create_text_model(model_name)
    if os.path.exists(path):
        state = torch.load(path, map_location=device)
        model.load_state_dict(state, strict=False)
        print(f"  âœ… Text Model ë¡œë“œ ì™„ë£Œ: {path}")
    else:
        print(f"  âš ï¸ Text Model íŒŒì¼ ì—†ìŒ (ë”ë¯¸ ì‚¬ìš©): {path}")
    model.to(device).eval()
    return model


# =============================================================================
# 4. Fusion MLP (í…ìŠ¤íŠ¸ í¬í•¨ ë²„ì „)
# =============================================================================
class FusionMLP(nn.Module):
    """
    Gesture + Audio + Text Featureë¥¼ ê²°í•©í•˜ëŠ” MLP
    """
    def __init__(self, gesture_dim=512, audio_dim=128, text_dim=1024, 
                 hidden_dim=256, num_classes=1, dropout=0.3):
        super(FusionMLP, self).__init__()
        
        self.gesture_dim = gesture_dim
        self.audio_dim = audio_dim
        self.text_dim = text_dim
        
        input_dim = gesture_dim + audio_dim + text_dim
        
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.Dropout(dropout * 0.7),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, gesture_feat, audio_feat, text_feat):
        combined = torch.cat((gesture_feat, audio_feat, text_feat), dim=1)
        return self.net(combined)


# =============================================================================
# 5. Fusion MLP (í…ìŠ¤íŠ¸ ì œì™¸ ë²„ì „)
# =============================================================================
class FusionMLPNoText(nn.Module):
    """
    Gesture + Audio Featureë§Œ ê²°í•©í•˜ëŠ” MLP (í…ìŠ¤íŠ¸ ì œì™¸)
    """
    def __init__(self, gesture_dim=512, audio_dim=128, hidden_dim=256, 
                 num_classes=1, dropout=0.3):
        super(FusionMLPNoText, self).__init__()
        
        self.gesture_dim = gesture_dim
        self.audio_dim = audio_dim
        
        input_dim = gesture_dim + audio_dim
        
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.Dropout(dropout * 0.7),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, gesture_feat, audio_feat, text_feat=None):
        combined = torch.cat((gesture_feat, audio_feat), dim=1)
        return self.net(combined)


# =============================================================================
# 6. í†µí•© Multimodal Fusion Model (End-to-End)
# =============================================================================
class MultimodalFusionModel(nn.Module):
    """
    End-to-end Multimodal Fusion: ì…ë ¥ë¶€í„° ì¶œë ¥ê¹Œì§€
    """
    def __init__(self, gesture_model, audio_model, fusion_mlp, text_model=None):
        super(MultimodalFusionModel, self).__init__()
        self.gesture_model = gesture_model
        self.audio_model = audio_model
        self.text_model = text_model
        self.fusion_mlp = fusion_mlp
        
    def forward(self, video_input, audio_input, text_input=None):
        _, gesture_feat = self.gesture_model(video_input, return_feature=True)
        _, audio_feat = self.audio_model(audio_input, return_feature=True)
        
        if self.text_model is not None and text_input is not None:
            _, text_feat = self.text_model(text_input, return_feature=True)
        else:
            text_feat = torch.zeros(gesture_feat.size(0), 1024).to(gesture_feat.device)
        
        output = self.fusion_mlp(gesture_feat, audio_feat, text_feat)
        return output


# =============================================================================
# â­ Factory Functions (ì¤‘ìš”! - ì´ í•¨ìˆ˜ë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤)
# =============================================================================
def create_fusion_model(gesture_dim=512, audio_dim=128, text_dim=1024,
                        hidden_dim=256, num_classes=1, use_text=True):
    """
    Fusion MLP ìƒì„± í•¨ìˆ˜
    """
    if use_text:
        return FusionMLP(
            gesture_dim=gesture_dim,
            audio_dim=audio_dim,
            text_dim=text_dim,
            hidden_dim=hidden_dim,
            num_classes=num_classes
        )
    else:
        return FusionMLPNoText(
            gesture_dim=gesture_dim,
            audio_dim=audio_dim,
            hidden_dim=hidden_dim,
            num_classes=num_classes
        )


def load_fusion_model(path, device, gesture_dim=512, audio_dim=128, 
                      text_dim=1024, hidden_dim=256, num_classes=1, use_text=True):
    """
    ì €ì¥ëœ Fusion ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
    """
    model = create_fusion_model(
        gesture_dim=gesture_dim,
        audio_dim=audio_dim,
        text_dim=text_dim,
        hidden_dim=hidden_dim,
        num_classes=num_classes,
        use_text=use_text
    )
    
    if os.path.exists(path):
        state = torch.load(path, map_location=device)
        new_state = {}
        for k, v in state.items():
            if k.startswith("module."):
                new_state[k[7:]] = v
            else:
                new_state[k] = v
        model.load_state_dict(new_state, strict=False)
        print(f"  âœ… Fusion Model ë¡œë“œ ì™„ë£Œ: {path}")
    else:
        print(f"  âš ï¸ Fusion Model íŒŒì¼ ì—†ìŒ: {path}")
    
    return model.to(device).eval()


def generate_segments(video_path, segment_duration=1.0, overlap=0.5):
    """
    ë¹„ë””ì˜¤ë¥¼ ê³ ì • ê¸¸ì´ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í• 
    
    segment_duration: ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
    overlap: ê²¹ì¹¨ ë¹„ìœ¨ (0.5 = 50%)
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"[ERROR] Cannot open video: {video_path}")
        return []
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    video_duration = total_frames / fps if fps > 0 else 0
    cap.release()
    
    if video_duration <= 0:
        return []
    
    segments = []
    step = segment_duration * (1 - overlap)
    current = 0
    
    while current + segment_duration <= video_duration:
        segments.append({
            'video_path': video_path,
            'start_sec': current,
            'end_sec': current + segment_duration,
            'label': 0,  # ì¶”ë¡  ì‹œì—ëŠ” ì•Œ ìˆ˜ ì—†ìŒ
        })
        current += step
    
    # ë§ˆì§€ë§‰ êµ¬ê°„
    if current < video_duration:
        segments.append({
            'video_path': video_path,
            'start_sec': max(0, video_duration - segment_duration),
            'end_sec': video_duration,
            'label': 0,
        })
    
    return segments

# =============================================================================
# 5. Transformer Fusion ëª¨ë¸
# =============================================================================
class TransformerFusion(nn.Module):
    def __init__(self, gesture_dim=512, audio_dim=128, text_dim=1024, 
                 hidden_dim=256, num_classes=1, use_text=True, 
                 dropout=0.5, nhead=4, num_layers=2):
        super(TransformerFusion, self).__init__()
        self.use_text = use_text
        
        # 1. ê° ëª¨ë‹¬ë¦¬í‹°ë¥¼ ê°™ì€ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜
        self.g_proj = nn.Linear(gesture_dim, hidden_dim)
        self.a_proj = nn.Linear(audio_dim, hidden_dim)
        if use_text:
            self.t_proj = nn.Linear(text_dim, hidden_dim)
            self.num_tokens = 3
        else:
            self.num_tokens = 2

        # 2. ëª¨ë‹¬ë¦¬í‹° ì„ë² ë”©
        self.modality_embed = nn.Parameter(torch.randn(1, self.num_tokens, hidden_dim))

        # 3. Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim, 
            nhead=nhead, 
            dim_feedforward=hidden_dim * 4, 
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # 4. ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * self.num_tokens, hidden_dim),
            nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, gesture, audio, text=None):
        g_emb = self.g_proj(gesture).unsqueeze(1)
        a_emb = self.a_proj(audio).unsqueeze(1)
        
        tokens = [g_emb, a_emb]
        
        if self.use_text and text is not None:
            t_emb = self.t_proj(text).unsqueeze(1)
            tokens.append(t_emb)
            
        x = torch.cat(tokens, dim=1)
        x = x + self.modality_embed
        x = self.transformer(x)
        x = x.reshape(x.size(0), -1)
        
        return self.classifier(x)


def create_fusion_model(gesture_dim=512, audio_dim=128, text_dim=1024, 
                        hidden_dim=256, num_classes=1, use_text=True, 
                        dropout=0.5, model_type='transformer'):
    """
    model_type: 'mlp' ë˜ëŠ” 'transformer'
    """
    if model_type == 'transformer':
        return TransformerFusion(
            gesture_dim, audio_dim, text_dim, hidden_dim, num_classes,
            use_text=use_text, dropout=dropout, nhead=4, num_layers=2
        )
    else:
        return FusionMLP(
            gesture_dim, audio_dim, text_dim, hidden_dim, num_classes,
            use_text=use_text, dropout=dropout
        )


def load_fusion_model(path, device, model_type='transformer', **kwargs):
    """Fusion ëª¨ë¸ ë¡œë“œ"""
    model = create_fusion_model(model_type=model_type, **kwargs)
    if os.path.exists(path):
        state = torch.load(path, map_location=device)
        model.load_state_dict(state, strict=False)
        print(f"  âœ… Fusion Model ({model_type}) ë¡œë“œ ì™„ë£Œ: {path}")
    else:
        print(f"  âš ï¸ Fusion Model íŒŒì¼ ì—†ìŒ: {path}")
    return model.to(device).eval()

# =============================================================================
# ê²°ê³¼ í›„ì²˜ë¦¬ í•¨ìˆ˜
# =============================================================================
def merge_overlapping_segments(segments, threshold=0.3):
    """
    ê²¹ì¹˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•©
    
    segments: list of (start, end, score)
    """
    if not segments:
        return []
    
    # ì‹œì‘ ì‹œê°„ìœ¼ë¡œ ì •ë ¬
    sorted_segs = sorted(segments, key=lambda x: x[0])
    
    merged = [list(sorted_segs[0])]
    
    for start, end, score in sorted_segs[1:]:
        prev_start, prev_end, prev_score = merged[-1]
        
        # ê²¹ì¹¨ í™•ì¸
        if start <= prev_end + threshold:
            # ë³‘í•©: êµ¬ê°„ í™•ì¥, ì ìˆ˜ëŠ” ìµœëŒ€ê°’
            merged[-1] = [prev_start, max(prev_end, end), max(prev_score, score)]
        else:
            merged.append([start, end, score])
    
    return [(s, e, sc) for s, e, sc in merged]


def format_time(seconds):
    """ì´ˆë¥¼ MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    minutes = int(seconds // 60)
    secs = int(seconds % 60)
    return f"{minutes:02d}:{secs:02d}"


def format_timestamp(seconds):
    """ì´ˆë¥¼ HH:MM:SS.mmm í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    return f"{hours:02d}:{minutes:02d}:{secs:06.3f}"


# =============================================================================
# JSON ê²°ê³¼ ì €ì¥
# =============================================================================
def save_results_json(results, output_path):
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    
    results: dict containing analysis results
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"[INFO] Results saved to: {output_path}")


def load_results_json(input_path):
    """JSON ê²°ê³¼ ë¡œë“œ"""
    with open(input_path, 'r', encoding='utf-8') as f:
        return json.load(f)
    
def load_text_analysis_data(tensors_path, scores_path):
    """
    ì €ì¥ëœ í…ìŠ¤íŠ¸ í…ì„œ(.pt)ì™€ ë¶„ì„ ê²°ê³¼(.json)ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        tensors_path (str): .pt íŒŒì¼ ê²½ë¡œ (ì„ë² ë”© í…ì„œ ë¦¬ìŠ¤íŠ¸)
        scores_path (str): .json íŒŒì¼ ê²½ë¡œ (ë¶„ì„ ê²°ê³¼ ë©”íƒ€ë°ì´í„°)
        
    Returns:
        tuple: (text_tensors, text_scores)
    """
    # 1. í…ì„œ ë¡œë“œ (.pt)
    if os.path.exists(tensors_path):
        try:
            # CPUë¡œ ë§¤í•‘í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë¡œë“œ
            text_tensors = torch.load(tensors_path, map_location='cpu')
            # ë§Œì•½ í…ì„œê°€ í•˜ë‚˜ë¡œ ë­‰ì³ì ¸ ìˆë‹¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í•„ìš”í•  ìˆ˜ ìˆìŒ (ìƒí™©ì— ë”°ë¼)
            print(f"  âœ… í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì™„ë£Œ: {tensors_path}")
        except Exception as e:
            print(f"  âŒ í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            text_tensors = []
    else:
        print(f"  âš ï¸ í…ìŠ¤íŠ¸ í…ì„œ íŒŒì¼ ì—†ìŒ: {tensors_path}")
        text_tensors = []

    # 2. ì ìˆ˜ ë¡œë“œ (.json)
    if os.path.exists(scores_path):
        try:
            with open(scores_path, 'r', encoding='utf-8') as f:
                text_scores = json.load(f)
            print(f"  âœ… í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {scores_path}")
        except Exception as e:
            print(f"  âŒ í…ìŠ¤íŠ¸ ë¶„ì„ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
            text_scores = {}
    else:
        print(f"  âš ï¸ í…ìŠ¤íŠ¸ ë¶„ì„ íŒŒì¼ ì—†ìŒ: {scores_path}")
        text_scores = {}

    return text_tensors, text_scores   



# =============================================================================
# í´ë˜ìŠ¤ ì´ë¦„ ìƒìˆ˜
# =============================================================================
GESTURE_CLASS_NAMES = ['No_Gesture', 'Gesture']
FUSION_CLASS_NAMES = ['No_Emphasis', 'Emphasis']
# import os
# import torch
# import torch.nn as nn
# from torchvision.models.video import r3d_18, R3D_18_Weights

# # ---------------------------------------------------------
# # ğŸ”§ [ê³µí†µ ìœ í‹¸] ì•ˆì „í•œ ê°€ì¤‘ì¹˜ ë¡œë“œ í•¨ìˆ˜ (module. ì œê±°)
# # ---------------------------------------------------------
# def load_state_dict_safe(model, path, device):
#     """
#     DataParallelë¡œ ì €ì¥ëœ ëª¨ë¸(module. prefix)ë„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
#     """
#     if not os.path.exists(path):
#         print(f"  âš ï¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ì—†ìŒ: {path}")
#         return model
        
#     state_dict = torch.load(path, map_location=device)
#     new_state_dict = {}
    
#     for k, v in state_dict.items():
#         # 'module.' ì ‘ë‘ì‚¬ê°€ ìˆìœ¼ë©´ ì œê±°
#         if k.startswith("module."):
#             name = k[7:] 
#         else:
#             name = k
#         new_state_dict[name] = v
            
#     try:
#         model.load_state_dict(new_state_dict, strict=False)
#         print(f"  âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {os.path.basename(path)}")
#     except Exception as e:
#         print(f"  âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        
#     return model
# # =============================================================================
# # 1. ì œìŠ¤ì²˜ ëª¨ë¸ (Video)
# # =============================================================================
# class GestureModel(nn.Module):
#     def __init__(self, num_classes=2, pretrained=True):
#         super(GestureModel, self).__init__()
#         try:
#             weights = R3D_18_Weights.KINETICS400_V1 if pretrained else None
#             self.backbone = r3d_18(weights=weights)
#         except:
#             self.backbone = r3d_18(pretrained=pretrained)

#         in_features = self.backbone.fc.in_features
#         self.backbone.fc = nn.Identity()
#         self.classifier = nn.Linear(in_features, num_classes)

#     def forward(self, x, return_feature=False):
#         features = self.backbone(x)
#         logits = self.classifier(features)
#         if return_feature:
#             return logits, features
#         return logits


# def create_gesture_model(num_classes=2, pretrained=True):
#     return GestureModel(num_classes=num_classes, pretrained=pretrained)


# def load_gesture_model(path, device):
#     model = create_gesture_model()
#     if os.path.exists(path):
#         state = torch.load(path, map_location=device)
#         new_state = {}
#         for k, v in state.items():
#             k_new = k
#             # 1. module. ì œê±°
#             if k_new.startswith("module."):
#                 k_new = k_new[7:]
            
#             # 2. fc -> classifier ì´ë¦„ ë³€ê²½ í˜¸í™˜ì„± ì²˜ë¦¬
#             if 'fc' in k_new and 'backbone' not in k_new:
#                 k_new = k_new.replace('fc', 'classifier')
#             new_state[k_new] = v
                
#        # 3. ìˆ˜ì •ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
#         try:
#             model.load_state_dict(new_state, strict=False)
#             print(f"  âœ… Gesture Model ë¡œë“œ ì™„ë£Œ: {path}")
#         except Exception as e:
#             print(f"  âŒ Gesture Model ë¡œë“œ ì‹¤íŒ¨: {e}")
            
#     else:
#         print(f"  âš ï¸ Gesture Model íŒŒì¼ ì—†ìŒ: {path}")
        
#     model.to(device).eval()
#     return model


# # =============================================================================
# # 2. ì˜¤ë””ì˜¤ ëª¨ë¸ (Audio)
# # =============================================================================
# class AudioLSTMModel(nn.Module):

#     def __init__(self, input_size=80, hidden_size=64, num_layers=2,
#                  num_classes=4, dropout=0.2):
#         super(AudioLSTMModel, self).__init__()

#         self.lstm = nn.LSTM(
#             input_size=input_size,
#             hidden_size=hidden_size,
#             num_layers=num_layers,
#             batch_first=True,
#             bidirectional=True,
#             dropout=dropout if num_layers > 1 else 0
#         )

#         # BiLSTMì´ë¯€ë¡œ hidden_size * 2
#         self.feature_dim = hidden_size * 2  # 128
#         self.use_simple_fc = True

#         #Fusionìš© classifier 
#         self.classifier = nn.Sequential(
#             nn.Linear(self.feature_dim, 64),
#             nn.ReLU(),
#             nn.BatchNorm1d(64),
#             nn.Dropout(dropout),
#             nn.Linear(64, num_classes)
#         )
#         self.fc = nn.Linear(self.feature_dim, num_classes)

#     def forward(self, x, return_feature=False):
#         """
#         x: (Batch, SeqLen, 80)  - Mel-Spectrogram
        
#         return_feature=False: A.pyì™€ ë™ì¼í•˜ê²Œ ë™ì‘
#         return_feature=True: Fusionìš© feature ë°˜í™˜
#         """
#         lstm_out, _ = self.lstm(x)  # (Batch, SeqLen, hidden*2)
#         features = lstm_out[:, -1, :]  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… (Batch, 128)
        
#         # ë¯¼ì„± ë°©ì‹: ë‹¨ìˆœ fc
#         if self.use_simple_fc:
#             logits = self.fc(features)
#         else:
#             logits = self.classifier(features)
        
#         if return_feature:
#             return logits, features
#         return logits


# class BiLSTM(nn.Module):
#     def __init__(self, input_dim, hidden_size=64, num_classes=4):
#         super(BiLSTM, self).__init__()
#         self.lstm = nn.LSTM(
#             input_dim, hidden_size, 
#             num_layers=2, 
#             batch_first=True, 
#             dropout=0.2, 
#             bidirectional=True
#         )
#         self.fc = nn.Linear(hidden_size * 2, num_classes)
#         self.feature_dim = hidden_size * 2  # Fusionìš©

#     def forward(self, x, return_feature=False):
#         out, _ = self.lstm(x)
#         features = out[:, -1, :]
#         logits = self.fc(features)
#         if return_feature:
#             return logits, features
#         return logits


# def create_audio_model(input_size=80, num_classes=4, hidden_size=64,
#                        num_layers=2, dropout=0.2):
   
#     return AudioLSTMModel(
#         input_size=input_size,
#         hidden_size=hidden_size,
#         num_layers=num_layers,
#         num_classes=num_classes,
#         dropout=dropout
#     )


# def load_audio_model(path, device, input_size=80, num_classes=4):
#     # [ìˆ˜ì •] BiLSTM í´ë˜ìŠ¤ ëŒ€ì‹  AudioLSTMModelë¡œ í†µì¼ (ì¼ê´€ì„± ìœ ì§€)
#     model = create_audio_model(input_size=input_size, num_classes=num_classes)
    
#     if os.path.exists(path):
#         state = torch.load(path, map_location=device)
#         if isinstance(state, dict):
#             model.load_state_dict(state, strict=False)
#             print(f"[INFO] Loaded audio model from: {path}")
#         else:
#             print(f"[WARN] Invalid state format in: {path}")
#     else:
#         print(f"[WARN] Audio model not found: {path}")
    
#     return model.to(device).eval()

# AUDIO_CLASS_NAMES = ['Normal', 'Pause_Talk', 'High_Tone', 'Loud']
# # =============================================================================
# # 3. í…ìŠ¤íŠ¸ ëª¨ë¸ (Text)
# # =============================================================================
# class TextBERTModel(nn.Module):
#     def __init__(self, input_dim=1024, num_classes=2):
#         super(TextBERTModel, self).__init__()
#         self.fc = nn.Linear(input_dim, num_classes)

#     def forward(self, x, return_feature=False):
#         logits = self.fc(x)
#         if return_feature:
#             return logits, x
#         return logits


# def create_text_model(model_name="klue/roberta-large", num_classes=2):
#     if "large" in model_name:
#         input_dim = 1024
#     else:
        
#         input_dim = 768
#     print(f"â„¹ï¸ Text Model: {model_name} (Dim: {input_dim})")
#     return TextBERTModel(input_dim=input_dim, num_classes=num_classes)


# def load_text_model(path, device):
#     model = create_text_model("klue/roberta-large")
#     if os.path.exists(path):
#         state = torch.load(path, map_location=device)
#         model.load_state_dict(state, strict=False)
#     model.to(device).eval()
#     print(f"Loaded text model from: {path}")
#     return model


# # =============================================================================
# # 4. í“¨ì „ ëª¨ë¸ (MLP)
# # =============================================================================
# class FusionMLP(nn.Module):
#     def __init__(self, gesture_model, audio_model, fusion_mlp):
#         super(MultimodalFusionModel, self).__init__()
#         self.gesture_model = gesture_model
#         self.audio_model = audio_model
#         self.fusion_mlp = fusion_mlp
        
#     def forward(self, video_input, audio_input):
#         """
#         video_input: (B, 3, T, H, W) - ë¹„ë””ì˜¤ í´ë¦½
#         audio_input: (B, SeqLen, 80) - Mel-Spectrogram
#         """
#         # Gesture feature ì¶”ì¶œ
#         _, gesture_feat = self.gesture_model(video_input, return_feature=True)
        
#         # Audio feature ì¶”ì¶œ
#         _, audio_feat = self.audio_model(audio_input, return_feature=True)
        
#         # Fusion
#         # text_feat ì—†ì´ gesture + audioë§Œ ì‚¬ìš©
#         output = self.fusion_mlp(gesture_feat, audio_feat, torch.zeros(gesture_feat.size(0), 0).to(gesture_feat.device))
        
#         return output
# # =============================================================================
# # Fusion MLP (í…ìŠ¤íŠ¸ ì œì™¸ ë²„ì „)
# # =============================================================================
# class FusionMLPNoText(nn.Module):
#     def __init__(self, gesture_dim=512, audio_dim=128, hidden_dim=256, num_classes=1):
#         super(FusionMLPNoText, self).__init__()
#         input_dim = gesture_dim + audio_dim
#         self.net = nn.Sequential(
#             nn.Linear(input_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Dropout(0.3),
#             nn.Linear(hidden_dim, hidden_dim // 2),
#             nn.ReLU(),
#             nn.Dropout(0.2),
#             nn.Linear(hidden_dim // 2, num_classes)
#         )

#     def forward(self, gesture_feat, audio_feat, text_feat=None):
#         combined = torch.cat((gesture_feat, audio_feat), dim=1)
#         return self.net(combined)
# # =============================================================================
# # ë°ì´í„° ë¡œë“œ
# # =============================================================================
# def prepare_datasets():
#     """Multimodal ë°ì´í„°ì…‹ ì¤€ë¹„"""
#     train_dataset = MultimodalDataset(
#         video_dir=CONFIG['video_dir'],
#         label_dir=CONFIG['label_dir'],
#         split='train'
#     )
    
#     val_dataset = MultimodalDataset(
#         video_dir=CONFIG['video_dir'],
#         label_dir=CONFIG['label_dir'],
#         split='val'
#     )
    
#     train_loader = DataLoader(
#         train_dataset,
#         batch_size=CONFIG['batch_size'],
#         shuffle=True,
#         num_workers=4,
#         pin_memory=True
#     )
    
#     val_loader = DataLoader(
#         val_dataset,
#         batch_size=CONFIG['batch_size'],
#         shuffle=False,
#         num_workers=4,
#         pin_memory=True
#     )
    
#     print(f"âœ… Train samples: {len(train_dataset)}")
#     print(f"âœ… Val samples: {len(val_dataset)}")
    
#     return train_loader, val_loader

# # =============================================================================
# # í•™ìŠµ í•¨ìˆ˜
# # =============================================================================
# def train_epoch(model, loader, criterion, optimizer, device):
#     """1 ì—í­ í•™ìŠµ"""
#     model.train()
#     total_loss = 0
    
#     pbar = tqdm(loader, desc='Training')
#     for batch in pbar:
#         video_input = batch['video'].to(device)
#         audio_input = batch['audio'].to(device)
#         labels = batch['label'].to(device).float()
        
#         optimizer.zero_grad()
#         outputs = model(video_input, audio_input).squeeze()
#         loss = criterion(outputs, labels)
        
#         loss.backward()
#         optimizer.step()
        
#         total_loss += loss.item()
        
#         pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
#     return total_loss / len(loader)

# def validate(model, loader, criterion, device):
#     """ê²€ì¦"""
#     model.eval()
#     total_loss = 0
    
#     with torch.no_grad():
#         for batch in tqdm(loader, desc='Validation'):
#             video_input = batch['video'].to(device)
#             audio_input = batch['audio'].to(device)
#             labels = batch['label'].to(device).float()
            
#             outputs = model(video_input, audio_input).squeeze()
#             loss = criterion(outputs, labels)
            
#             total_loss += loss.item()
    
#     return total_loss / len(loader)