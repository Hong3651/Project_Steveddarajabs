# app.py - 06_inference_model_binary.ipynb ì—°ë™ ë²„ì „
import os
import sys
import json
import uuid
import torch
import cv2
import librosa
import numpy as np
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from werkzeug.utils import secure_filename
from tqdm import tqdm

# â˜… AI í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
AI_PROJECT_PATH = '/home/stu/ai_project'
sys.path.insert(0, AI_PROJECT_PATH)

# utils.pyì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ import
from utils import load_gesture_model, load_audio_model, create_text_model

app = Flask(__name__)
CORS(app)

# ===== ì„¤ì • =====
UPLOAD_FOLDER = os.path.join(AI_PROJECT_PATH, 'new_video')
RESULT_FOLDER = os.path.join(AI_PROJECT_PATH, 'ê²°ê³¼')
MODEL_FOLDER = os.path.join(AI_PROJECT_PATH, 'ëª¨ë¸ì§‘í•©')
ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov', 'mkv'}

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024  # 500MB ì œí•œ

# ===== 06ë²ˆ íŒŒì¼ ì„¤ì • ë³µì‚¬ =====
SEGMENT_DURATION = 1.0
OVERLAP = 0.5
RESIZE_HW = (112, 112)
CLIP_LEN = 16
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì˜¤ë””ì˜¤ ì„¤ì •
AUDIO_CONFIG = {
    'sample_rate': 16000,
    'n_mels': 80,
    'n_fft': 1024,
    'hop_length': 512,
    'window_size': 50
}

# í“¨ì „ ëª¨ë¸ ì„¤ì • (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ FUSION_CONFIG)
FUSION_CONFIG = {
    "gesture_dim": 512,
    "audio_dim": 128,
    "text_dim": 1024,
    "hidden_dim": 256,
}

# ê²½ë¡œ ì„¤ì •
GESTURE_MODEL_PATH = os.path.join(MODEL_FOLDER, "gesture_model.pt")
AUDIO_MODEL_PATH = os.path.join(MODEL_FOLDER, "best_bi_lstm.pth")
FUSION_MODEL_PATH = os.path.join(MODEL_FOLDER, "fusion_model_gate.pt")

print(f"[Server] Device: {DEVICE}")

# ===== 06ë²ˆ íŒŒì¼ í´ë˜ìŠ¤ ë³µì‚¬ =====
class AudioFeatureExtractorInferenceStyle:
    """í•™ìŠµ ì½”ë“œì™€ 100% ë™ì¼í•œ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self, config=None):
        if config is None: config = AUDIO_CONFIG
        self.sr = config.get('sample_rate', 16000)
        self.n_mels = config.get('n_mels', 80)
        self.n_fft = config.get('n_fft', 1024)
        self.hop_length = config.get('hop_length', 512)
        self.window_size = config.get('window_size', 50)
    
    def process_full_audio(self, y_audio):
        # 1. Waveform Normalization
        y_audio = librosa.util.normalize(y_audio)
        
        # 2. Mel-Spectrogram
        mel = librosa.feature.melspectrogram(
            y=y_audio, sr=self.sr, n_mels=self.n_mels,
            n_fft=self.n_fft, hop_length=self.hop_length
        )
        features = librosa.power_to_db(mel, ref=np.max).T 
        
        # 3. Global Scaling (íŒŒì¼ ì „ì²´ ê¸°ì¤€)
        scaler = StandardScaler()
        try:
            features_norm = scaler.fit_transform(features)
        except ValueError:
            return np.zeros((0, self.n_mels), dtype=np.float32)

        return features_norm.astype(np.float32)


class GatedFusion(nn.Module):
    """í•™ìŠµëœ Fusion ëª¨ë¸ êµ¬ì¡°"""
    def __init__(self, gesture_dim, audio_dim, text_dim, hidden_dim, num_classes=1, use_text=True, dropout=0.5):
        super(GatedFusion, self).__init__()
        self.use_text = use_text
        
        self.g_net = nn.Sequential(
            nn.Linear(gesture_dim, hidden_dim), nn.ReLU(),
            nn.BatchNorm1d(hidden_dim), nn.Dropout(p=dropout)
        )
        self.a_net = nn.Sequential(
            nn.Linear(audio_dim, hidden_dim), nn.ReLU(),
            nn.BatchNorm1d(hidden_dim), nn.Dropout(p=dropout)
        )
        
        if use_text:
            self.t_net = nn.Sequential(
                nn.Linear(text_dim, hidden_dim), nn.ReLU(),
                nn.BatchNorm1d(hidden_dim), nn.Dropout(p=dropout)
            )
            gate_input_dim = hidden_dim * 3
        else:
            gate_input_dim = hidden_dim * 2

        self.gate_net = nn.Sequential(
            nn.Linear(gate_input_dim, hidden_dim), nn.Tanh(),
            nn.Linear(hidden_dim, 3 if use_text else 2),
            nn.Softmax(dim=1)
        )

        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, gesture, audio, text=None):
        h_g = self.g_net(gesture)
        h_a = self.a_net(audio)
        features = [h_g, h_a]
        
        if self.use_text and text is not None:
            h_t = self.t_net(text)
            features.append(h_t)
            
        concat_feat = torch.cat(features, dim=1)
        gates = self.gate_net(concat_feat)
        
        h_fused = gates[:, 0:1] * h_g + gates[:, 1:2] * h_a
        if self.use_text and text is not None:
            h_fused += gates[:, 2:3] * h_t
            
        return self.classifier(h_fused)


# ===== 06ë²ˆ íŒŒì¼ í•¨ìˆ˜ ë³µì‚¬ =====
def load_all_models():
    """ëª¨ë“  ì„œë¸Œ ëª¨ë¸ê³¼ í“¨ì „ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("[Server] ğŸ”§ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    # Gesture & Audio Model Load
    g_model = load_gesture_model(GESTURE_MODEL_PATH, DEVICE)
    a_model = load_audio_model(AUDIO_MODEL_PATH, DEVICE)
    t_model = create_text_model().to(DEVICE)  # Text Model (RoBERTa ë“±)

    # Fusion Model Load
    f_model = GatedFusion(
        gesture_dim=FUSION_CONFIG['gesture_dim'],
        audio_dim=FUSION_CONFIG['audio_dim'],
        text_dim=FUSION_CONFIG['text_dim'],
        hidden_dim=FUSION_CONFIG['hidden_dim'],
        use_text=True
    ).to(DEVICE)
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    if os.path.exists(FUSION_MODEL_PATH):
        f_model.load_state_dict(torch.load(FUSION_MODEL_PATH, map_location=DEVICE))
        print("[Server] âœ… Fusion ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
    else:
        print(f"[Server] âŒ Fusion ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {FUSION_MODEL_PATH}")

    g_model.eval()
    a_model.eval()
    t_model.eval()
    f_model.eval()
    
    return g_model, a_model, t_model, f_model


def preprocess_new_video(video_path, text_tensor_path=None):
    """
    ìƒˆë¡œìš´ ë¹„ë””ì˜¤ì— ëŒ€í•´ í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    Video: Resize -> Normalize
    Audio: Scaler -> Slice
    Text: (Optional) Placeholder or Embedding extraction
    """
    # 1. ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0
    
    step = SEGMENT_DURATION * (1 - OVERLAP)
    segments = []
    current = 0
    while current + SEGMENT_DURATION <= duration:
        segments.append((current, current + SEGMENT_DURATION))
        current += step

    if not segments:
        return None, None, None, []

    # 2. ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ
    video_tensors = []
    
    print("[API] Processing video segments...")
    for start_sec, end_sec in tqdm(segments, desc="Processing Segments"):
        start_frame = int(start_sec * fps)
        end_frame = int(end_sec * fps)
        
        # í”„ë ˆì„ ì¸ë±ìŠ¤ ê³„ì‚° (16ê°œ ê· ë“± ì¶”ì¶œ)
        seg_len = end_frame - start_frame
        indices = np.linspace(start_frame, end_frame-1, num=CLIP_LEN, dtype=int)
        
        frames = []
        for idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame = cv2.resize(frame, RESIZE_HW)
                frames.append(frame)
            else:
                # ì‹¤íŒ¨ì‹œ ê²€ì€ í™”ë©´
                frames.append(np.zeros((RESIZE_HW[0], RESIZE_HW[1], 3), dtype=np.uint8))
        
        # ì •ê·œí™” (0~1 -> -1~1) ë° ì°¨ì› ë³€ê²½ (C, T, H, W)
        frames = np.stack(frames).astype(np.float32) / 255.0
        frames = np.transpose(frames, (3, 0, 1, 2))  # (C, T, H, W)
        frames = (frames - 0.5) / 0.5
        video_tensors.append(torch.from_numpy(frames))
        
    cap.release()

    # 3. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (Global Scaling ì ìš©)
    audio_extractor = AudioFeatureExtractorInferenceStyle(AUDIO_CONFIG)
    audio_tensors = []
    
    try:
        y_full, sr = librosa.load(video_path, sr=AUDIO_CONFIG['sample_rate'])
        full_feat = audio_extractor.process_full_audio(y_full)
        
        frames_per_sec = sr / AUDIO_CONFIG['hop_length']
        
        for start_sec, end_sec in segments:
            start_idx = int(start_sec * frames_per_sec)
            end_idx = start_idx + AUDIO_CONFIG['window_size']
            
            feat_seg = full_feat[start_idx:end_idx]
            
            # íŒ¨ë”©
            if feat_seg.shape[0] < AUDIO_CONFIG['window_size']:
                pad = AUDIO_CONFIG['window_size'] - feat_seg.shape[0]
                feat_seg = np.pad(feat_seg, ((0, pad), (0, 0)), mode='constant')
            
            feat_seg = feat_seg[:AUDIO_CONFIG['window_size']]
            audio_tensors.append(torch.from_numpy(feat_seg).float())
            
    except Exception as e:
        print(f"[API] Audio Error: {e}")
        audio_tensors = [torch.zeros(AUDIO_CONFIG['window_size'], AUDIO_CONFIG['n_mels'])] * len(segments)

    # 4. í…ìŠ¤íŠ¸ ì²˜ë¦¬
    if text_tensor_path is None:
        text_tensor_path = os.path.join(AI_PROJECT_PATH, 'ì‹œì—°', 'total_tensor.pt')
    
    text_tensors = []
    
    if os.path.exists(text_tensor_path):
        print(f"[API] â„¹ï¸ í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì¤‘: {text_tensor_path}")
        try:
            loaded_data = torch.load(text_tensor_path, map_location='cpu')
            
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í…ì„œë¡œ ë³€í™˜ (Stack)
            if isinstance(loaded_data, list):
                if len(loaded_data) > 0:
                    loaded_text = torch.stack(loaded_data)
                else:
                    loaded_text = torch.zeros(1024)
            else:
                loaded_text = loaded_data

            # ì°¨ì› ì¶•ì†Œ (í‰ê· )
            loaded_text = loaded_text.float()
            
            if loaded_text.dim() > 1:
                loaded_text = loaded_text.mean(dim=0)
            
            if loaded_text.dim() == 2 and loaded_text.shape[0] == 1:
                 loaded_text = loaded_text.squeeze(0)

            # ìµœì¢… í˜•íƒœ í™•ì¸ ë° ì ìš©
            if loaded_text.shape[0] != 1024:
                print(f"[API] âš ï¸ í…ìŠ¤íŠ¸ ì°¨ì› ì´ìƒ ({loaded_text.shape}). 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                loaded_text = torch.zeros(1024)

            # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì— ë™ì¼í•œ í…ìŠ¤íŠ¸ í”¼ì²˜ ì ìš©
            for _ in segments:
                text_tensors.append(loaded_text)
                
        except Exception as e:
            print(f"[API] âŒ í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("[API] âš ï¸ Zero Tensorë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            text_tensors = [torch.zeros(1024) for _ in segments]
            
    else:
        print(f"[API] âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {text_tensor_path}")
        print("[API] âš ï¸ Zero Tensorë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        text_tensors = [torch.zeros(1024) for _ in segments]

    return torch.stack(video_tensors), torch.stack(audio_tensors), torch.stack(text_tensors), segments


@torch.no_grad()
def inference_fusion(video_path):
    """
    06ë²ˆ íŒŒì¼ì˜ ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜
    """
    print(f"[API] ğŸš€ ì¶”ë¡  ì‹œì‘: {os.path.basename(video_path)}")
    
    # 1. ë°ì´í„° ì „ì²˜ë¦¬
    v_data, a_data, t_data, segments = preprocess_new_video(video_path)
    
    if v_data is None:
        print("[API] âŒ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
        return None

    # 2. ëª¨ë¸ ë¡œë“œ
    g_model, a_model, t_model, f_model = load_all_models()
    
    # 3. ë°°ì¹˜ ë‹¨ìœ„ ì¶”ë¡ 
    batch_size = 32
    num_samples = len(v_data)
    results = []
    
    print(f"[API] ğŸ“Š ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {num_samples}")
    
    # ë°°ì¹˜ ì²˜ë¦¬
    for i in range(0, num_samples, batch_size):
        v_batch = v_data[i:i+batch_size].to(DEVICE)
        a_batch = a_data[i:i+batch_size].to(DEVICE)
        t_batch = t_data[i:i+batch_size].to(DEVICE)
        
        _, g_feats = g_model(v_batch, return_feature=True)
        _, a_feats = a_model(a_batch, return_feature=True)
        
        if hasattr(t_model, 'forward'):
             _, t_feats = t_model(t_batch, return_feature=True)
        else:
             t_feats = t_batch 

        logits = f_model(g_feats, a_feats, t_feats)
        probs = torch.sigmoid(logits).cpu().numpy().flatten()
        
        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°
        for j, prob in enumerate(probs):
            idx = i + j
            start, end = segments[idx]
            
            results.append({
                "start": round(start, 2),
                "end": round(end, 2),
                "score": float(prob)
            })
            
    # 4. ê²°ê³¼ ì €ì¥ (JSON)
    save_path = video_path.replace(".mp4", "_results.json")
    if save_path == video_path: 
        save_path = video_path + "_results.json"
        
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4)
    print(f"[API] ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼(ì ìˆ˜)ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")

    # 5. ê°•ì¡° êµ¬ê°„ í•„í„°ë§
    emphasized_segments = [r for r in results if r['score'] >= 0.5]
    print(f"[API] ì´ {len(emphasized_segments)}/{len(results)} êµ¬ê°„ ê°•ì¡°ë¨.")
    
    return results, save_path


# ===== Flask í—¬í¼ í•¨ìˆ˜ =====
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


# ===== Flask ë¼ìš°íŠ¸ =====
@app.route('/')
def index():
    return '''
    <!doctype html>
    <html>
    <head>
        <title>AI Presentation Coach</title>
        <style>
            body { font-family: sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
            .container { border: 1px solid #ddd; padding: 20px; border-radius: 8px; }
            button { background-color: #007bff; color: white; border: none; padding: 10px 20px; cursor: pointer; }
            button:hover { background-color: #0056b3; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸš€ AI í”„ë ˆì  í…Œì´ì…˜ ì½”ì¹˜ ì„œë²„</h1>
            <p>ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.</p>
            
            <h3>ì˜ìƒ ë¶„ì„ í…ŒìŠ¤íŠ¸</h3>
            <form action="/analyze_presentation" method="post" enctype="multipart/form-data">
                <input type="file" name="video" accept=".mp4,.avi,.mov,.mkv">
                <br><br>
                <button type="submit">ë¶„ì„ ì‹œì‘ (JSON ê²°ê³¼ ë°˜í™˜)</button>
            </form>
        </div>
    </body>
    </html>
    '''


@app.route('/analyze_presentation', methods=['POST'])
def analyze_presentation():
    """
    HTMLì—ì„œ ì˜ìƒì„ ì—…ë¡œë“œ ë°›ì•„ 06ë²ˆ íŒŒì¼ì˜ inference_fusion í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê³ 
    JSON ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # 1. íŒŒì¼ í™•ì¸
        if 'video' not in request.files:
            return jsonify({"success": False, "message": "ì˜ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}), 400
        
        file = request.files['video']
        if file.filename == '':
            return jsonify({"success": False, "message": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400
        
        if not allowed_file(file.filename):
            return jsonify({"success": False, "message": "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."}), 400
        
        # 2. íŒŒì¼ ì €ì¥
        video_id = str(uuid.uuid4())[:8]
        filename = f"{video_id}_{secure_filename(file.filename)}"
        video_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(video_path)
        print(f"[API] Video saved: {video_path}")
        
        # 3. 06ë²ˆ íŒŒì¼ì˜ inference_fusion í•¨ìˆ˜ ì‹¤í–‰
        results, result_json_path = inference_fusion(video_path)
        
        if results is None:
            return jsonify({
                "success": False, 
                "message": "ì¶”ë¡  ì‹¤íŒ¨"
            }), 500
        
        # 4. ì‘ë‹µ ë°˜í™˜
        return jsonify({
            "success": True,
            "video_id": video_id,
            "video_path": video_path,
            "result_json_path": result_json_path,
            "total_segments": len(results),
            "emphasized_segments": len([r for r in results if r['score'] >= 0.5]),
            "results": results  # ì „ì²´ ê²°ê³¼ í¬í•¨
        })
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"success": False, "message": str(e)}), 500


@app.route('/videos/<path:filename>')
def serve_video(filename):
    """ë¶„ì„ëœ ì˜ìƒ íŒŒì¼ ì œê³µ"""
    return send_from_directory(UPLOAD_FOLDER, filename)


@app.route('/results/<path:filename>')
def serve_result(filename):
    """ë¶„ì„ ê²°ê³¼ JSON ì œê³µ"""
    return send_from_directory(RESULT_FOLDER, filename)


if __name__ == '__main__':
    os.makedirs(UPLOAD_FOLDER, exist_ok=True)
    os.makedirs(RESULT_FOLDER, exist_ok=True)
    
    print("[Server] ì„œë²„ ì‹œì‘ ì¤‘...")
    print(f"[Server] ì—…ë¡œë“œ í´ë”: {UPLOAD_FOLDER}")
    print(f"[Server] ê²°ê³¼ í´ë”: {RESULT_FOLDER}")
    print(f"[Server] ëª¨ë¸ í´ë”: {MODEL_FOLDER}")
    
    app.run(debug=True, port=5000, host='0.0.0.0')
