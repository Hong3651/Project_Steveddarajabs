{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. ì œìŠ¤ì²˜ ëª¨ë¸ í•™ìŠµ\n",
    "### 3D CNN (r3d_18) ê¸°ë°˜ ê°•ì¡° êµ¬ê°„ ê²€ì¶œ\n",
    "- CSVì—ì„œ ë¼ë²¨ ë¡œë“œ\n",
    "- Negative êµ¬ê°„ ìë™ ìƒì„±\n",
    "- í•™ìŠµ ë° ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T16:05:58.811371Z",
     "start_time": "2025-12-07T16:05:58.803299Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import GESTURE_CONFIG, GESTURE_MODEL_PATH, ensure_dirs\n",
    "from utils import (\n",
    "    prepare_all_entries, train_val_split,\n",
    "    VideoEmphasisDataset,\n",
    "    create_gesture_model,\n",
    "    train_one_epoch, evaluate\n",
    ")\n",
    "\n",
    "ensure_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T07:58:44.966058Z",
     "start_time": "2025-12-07T07:58:44.957309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_len: 16, batch_size: 180, epochs: 10\n"
     ]
    }
   ],
   "source": [
    "LABEL_CSV = \"/home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\"  \n",
    "clip_len = GESTURE_CONFIG[\"clip_len\"]\n",
    "resize_hw = GESTURE_CONFIG[\"resize_hw\"]\n",
    "batch_size = GESTURE_CONFIG[\"batch_size\"]\n",
    "num_epochs = GESTURE_CONFIG[\"num_epochs\"]\n",
    "lr = GESTURE_CONFIG[\"lr\"]\n",
    "\n",
    "print(f\"clip_len: {clip_len}, batch_size: {batch_size}, epochs: {num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T07:58:45.499455Z",
     "start_time": "2025-12-07T07:58:45.019958Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prepare_all_entries() got an unexpected keyword argument 'num_neg_per_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CSV ë¡œë“œ + Negative ìƒì„±\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m entries_all \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_all_entries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLABEL_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neg_per_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: prepare_all_entries() got an unexpected keyword argument 'num_neg_per_pos'"
     ]
    }
   ],
   "source": [
    "# CSV ë¡œë“œ + Negative ìƒì„±\n",
    "entries_all = prepare_all_entries(LABEL_CSV, num_neg_per_pos=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T07:58:45.518885Z",
     "start_time": "2025-12-07T07:58:45.512106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train / Val ë¶„í• \n",
    "train_entries, val_entries = train_val_split(entries_all, split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T07:58:45.587819Z",
     "start_time": "2025-12-07T07:58:45.576819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 7653\n",
      "Val dataset: 1914\n"
     ]
    }
   ],
   "source": [
    "# Dataset ìƒì„±\n",
    "train_dataset = VideoEmphasisDataset(train_entries, clip_len=clip_len, resize_hw=resize_hw)\n",
    "val_dataset = VideoEmphasisDataset(val_entries, clip_len=clip_len, resize_hw=resize_hw)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Val dataset: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T07:58:46.445787Z",
     "start_time": "2025-12-07T07:58:45.642750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([3, 32, 112, 112])\n",
      "y: 0\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œ í™•ì¸\n",
    "x0, y0 = train_dataset[0]\n",
    "print(f\"x shape: {x0.shape}\")  \n",
    "print(f\"y: {y0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T07:58:51.590202Z",
     "start_time": "2025-12-07T07:58:46.538659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x: torch.Size([180, 3, 32, 112, 112])\n",
      "batch_y: tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader ìƒì„±\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "# ë°°ì¹˜ í™•ì¸\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(f\"batch_x: {batch_x.shape}\")  \n",
    "print(f\"batch_y: {batch_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T07:58:52.665172Z",
     "start_time": "2025-12-07T07:58:51.812103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ 2ê°œì˜ GPUë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤! DataParallelì„ ì ìš©í•©ë‹ˆë‹¤.\n",
      "Train set - Positive: 2324, Negative: 5329\n",
      "Using class weights: [neg=1.0, pos=2.29]\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ìƒì„± (r3d_18 with Kinetics400 pretrained)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_gesture_model(num_classes=2, pretrained=True)\n",
    "if torch.cuda.device_count() > 1:\n",
    "        print(f\"ğŸ”¥ {torch.cuda.device_count()}ê°œì˜ GPUë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤! DataParallelì„ ì ìš©í•©ë‹ˆë‹¤.\")\n",
    "        model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "pos_count = sum(1 for e in train_entries if e['label'] == 1)\n",
    "neg_count = sum(1 for e in train_entries if e['label'] == 0)\n",
    "\n",
    "print(f\"Train set - Positive: {pos_count}, Negative: {neg_count}\")\n",
    "\n",
    "if neg_count > 0 and pos_count > 0:\n",
    "    pos_weight = neg_count / pos_count\n",
    "    class_weights = torch.tensor([1.0, pos_weight]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    print(f\"Using class weights: [neg=1.0, pos={pos_weight:.2f}]\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(\"Using standard CrossEntropyLoss (no class weights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T07:58:52.787952Z",
     "start_time": "2025-12-07T07:58:52.779987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW (lr=0.0001, weight_decay=1e-4)\n"
     ]
    }
   ],
   "source": [
    "# ì†ì‹¤í•¨ìˆ˜ / ì˜µí‹°ë§ˆì´ì €\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "print(f\"Optimizer: AdamW (lr={lr}, weight_decay=1e-4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T13:21:33.546265Z",
     "start_time": "2025-12-07T07:58:52.848187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n",
      "[h264 @ 0x2d27dbc0] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n",
      "[h264 @ 0x2be8bbc0] mmco: unref short failure\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/ai_project/utils/models.py\", line 52, in forward\n    features = self.backbone(x)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torchvision/models/video/resnet.py\", line 253, in forward\n    x = self.layer1(x)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torchvision/models/video/resnet.py\", line 113, in forward\n    out = self.conv2(out)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n    return F.batch_norm(\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2822, in batch_norm\n    return torch.batch_norm(\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.15 GiB. GPU 0 has a total capacity of 22.37 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 20.55 GiB memory in use. Of the allocated memory 20.32 GiB is allocated by PyTorch, and 13.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m save_path \u001b[38;5;241m=\u001b[39m GESTURE_MODEL_PATH\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 5\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ai_project/utils/helpers.py:863\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    861\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    862\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 863\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[1;32m    865\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:212\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:126\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 126\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/ai_project/utils/models.py\", line 52, in forward\n    features = self.backbone(x)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torchvision/models/video/resnet.py\", line 253, in forward\n    x = self.layer1(x)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torchvision/models/video/resnet.py\", line 113, in forward\n    out = self.conv2(out)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n    return F.batch_norm(\n  File \"/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/functional.py\", line 2822, in batch_norm\n    return torch.batch_norm(\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.15 GiB. GPU 0 has a total capacity of 22.37 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 20.55 GiB memory in use. Of the allocated memory 20.32 GiB is allocated by PyTorch, and 13.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "save_path = GESTURE_MODEL_PATH\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n",
    "          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\")\n",
    "\n",
    "    # Best ëª¨ë¸ ì €ì¥\n",
    "    if val_acc >= best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"  -> best model saved to: {save_path}\")\n",
    "\n",
    "print(f\"\\nìµœì¢… best val acc: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í•™ìŠµ ì™„ë£Œ!\n",
    "ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ `06_inference.ipynb`ì—ì„œ ì¶”ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
