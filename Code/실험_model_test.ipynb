{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e3600f",
   "metadata": {},
   "source": [
    "#csv í•©ì¹˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f35b763833f660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T16:06:25.589418Z",
     "start_time": "2025-12-07T16:06:25.347502Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def merge_csv_files(\n",
    "    csv_dir: str,\n",
    "    output_path: str = None,\n",
    "    pattern: str = \"*.csv\",\n",
    "    exclude_files: Optional[List[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  CSV íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        csv_dir: CSV íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì €ì¥ ì•ˆ í•¨)\n",
    "        pattern: íŒŒì¼ íŒ¨í„´ (ê¸°ë³¸: *.csv)\n",
    "        exclude_files: ì œì™¸í•  íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "    Returns:\n",
    "        í•©ì³ì§„ DataFrame\n",
    "    \"\"\"\n",
    "    if exclude_files is None:\n",
    "        exclude_files = []\n",
    "\n",
    "    # CSV íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "    csv_files = glob.glob(os.path.join(csv_dir, pattern))\n",
    "    csv_files = [f for f in csv_files if os.path.basename(f) not in exclude_files]\n",
    "    csv_files.sort()\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"[WARNING] '{csv_dir}'ì—ì„œ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"[INFO] {len(csv_files)}ê°œì˜ CSV íŒŒì¼ ë°œê²¬:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "    # ê° CSV ë¡œë“œ ë° í†µí•©\n",
    "    dfs = []\n",
    "    for csv_path in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df['source_file'] = os.path.basename(csv_path)  # ì¶œì²˜ íŒŒì¼ëª… ì¶”ê°€\n",
    "            dfs.append(df)\n",
    "            print(f\"  âœ“ {os.path.basename(csv_path)}: {len(df)} entries\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— {os.path.basename(csv_path)}: ë¡œë“œ ì‹¤íŒ¨ - {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"[ERROR] ë¡œë“œëœ CSVê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # í•©ì¹˜ê¸°\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # í†µê³„ ì¶œë ¥\n",
    "    print(f\"\\n[ê²°ê³¼ ìš”ì•½]\")\n",
    "    print(f\"  ì´ entries: {len(merged_df)}\")\n",
    "    if 'label' in merged_df.columns:\n",
    "        pos_count = (merged_df['label'] == 1).sum()\n",
    "        neg_count = (merged_df['label'] == 0).sum()\n",
    "        print(f\"  Positive (ê°•ì¡°): {pos_count}\")\n",
    "        print(f\"  Negative: {neg_count}\")\n",
    "\n",
    "    if 'video_path' in merged_df.columns:\n",
    "        unique_videos = merged_df['video_path'].nunique()\n",
    "        print(f\"  ê³ ìœ  ì˜ìƒ ìˆ˜: {unique_videos}\")\n",
    "\n",
    "    # ì €ì¥\n",
    "    if output_path:\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # source_file ì»¬ëŸ¼ ì œì™¸í•˜ê³  ì €ì¥ (ì›ë³¸ í¬ë§· ìœ ì§€)\n",
    "        save_df = merged_df.drop(columns=['source_file'], errors='ignore')\n",
    "        save_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        print(f\"\\n[ì €ì¥ ì™„ë£Œ] {output_path}\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fde24c74d32a433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T07:57:06.730233Z",
     "start_time": "2025-12-07T07:57:06.639551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 53ê°œì˜ CSV íŒŒì¼ ë°œê²¬:\n",
      "  - 1.csv\n",
      "  - 10.csv\n",
      "  - 11.csv\n",
      "  - 12.csv\n",
      "  - 13.csv\n",
      "  - 14.csv\n",
      "  - 15.csv\n",
      "  - 16.csv\n",
      "  - 17.csv\n",
      "  - 18.csv\n",
      "  - 19.csv\n",
      "  - 2.csv\n",
      "  - 20.csv\n",
      "  - 3.csv\n",
      "  - 4.csv\n",
      "  - 41.csv\n",
      "  - 42.csv\n",
      "  - 43.csv\n",
      "  - 44.csv\n",
      "  - 45.csv\n",
      "  - 46.csv\n",
      "  - 47.csv\n",
      "  - 48.csv\n",
      "  - 49.csv\n",
      "  - 5.csv\n",
      "  - 50.csv\n",
      "  - 51.csv\n",
      "  - 52.csv\n",
      "  - 53.csv\n",
      "  - 54.csv\n",
      "  - 55.csv\n",
      "  - 56.csv\n",
      "  - 57.csv\n",
      "  - 58.csv\n",
      "  - 59.csv\n",
      "  - 6.csv\n",
      "  - 60.csv\n",
      "  - 7.csv\n",
      "  - 8.csv\n",
      "  - 9.csv\n",
      "  - 90.csv\n",
      "  - 91.csv\n",
      "  - 92.csv\n",
      "  - ë‚ ì”¨1.csv\n",
      "  - ë¹ ë”ë„ˆìŠ¤1.csv\n",
      "  - ì„¤ë¯¼ì„1.csv\n",
      "  - ì„¸ë°”ì‹œ1.csv\n",
      "  - ìŠ¤í‹°ë¸Œì¡ìŠ¤1_test.csv\n",
      "  - ìŠ¤í‹°ë¸Œì¡ìŠ¤2.csv\n",
      "  - ìŠ¤í‹°ë¸Œì¡ìŠ¤3.csv\n",
      "  - ìµœíƒœì„±1.csv\n",
      "  - ìµœíƒœì„±2_test.csv\n",
      "  - ìµœíƒœì„±3.csv\n",
      "  âœ“ 1.csv: 34 entries\n",
      "  âœ“ 10.csv: 25 entries\n",
      "  âœ“ 11.csv: 35 entries\n",
      "  âœ“ 12.csv: 33 entries\n",
      "  âœ“ 13.csv: 23 entries\n",
      "  âœ“ 14.csv: 28 entries\n",
      "  âœ“ 15.csv: 21 entries\n",
      "  âœ“ 16.csv: 23 entries\n",
      "  âœ“ 17.csv: 16 entries\n",
      "  âœ“ 18.csv: 20 entries\n",
      "  âœ“ 19.csv: 37 entries\n",
      "  âœ“ 2.csv: 29 entries\n",
      "  âœ“ 20.csv: 18 entries\n",
      "  âœ“ 3.csv: 18 entries\n",
      "  âœ“ 4.csv: 50 entries\n",
      "  âœ“ 41.csv: 56 entries\n",
      "  âœ“ 42.csv: 6 entries\n",
      "  âœ“ 43.csv: 59 entries\n",
      "  âœ“ 44.csv: 71 entries\n",
      "  âœ“ 45.csv: 79 entries\n",
      "  âœ“ 46.csv: 48 entries\n",
      "  âœ“ 47.csv: 172 entries\n",
      "  âœ“ 48.csv: 108 entries\n",
      "  âœ“ 49.csv: 72 entries\n",
      "  âœ“ 5.csv: 34 entries\n",
      "  âœ“ 50.csv: 268 entries\n",
      "  âœ“ 51.csv: 8 entries\n",
      "  âœ“ 52.csv: 48 entries\n",
      "  âœ“ 53.csv: 19 entries\n",
      "  âœ“ 54.csv: 395 entries\n",
      "  âœ“ 55.csv: 102 entries\n",
      "  âœ“ 56.csv: 60 entries\n",
      "  âœ“ 57.csv: 108 entries\n",
      "  âœ“ 58.csv: 184 entries\n",
      "  âœ“ 59.csv: 41 entries\n",
      "  âœ“ 6.csv: 38 entries\n",
      "  âœ“ 60.csv: 18 entries\n",
      "  âœ“ 7.csv: 33 entries\n",
      "  âœ“ 8.csv: 46 entries\n",
      "  âœ“ 9.csv: 35 entries\n",
      "  âœ“ 90.csv: 37 entries\n",
      "  âœ“ 91.csv: 37 entries\n",
      "  âœ“ 92.csv: 35 entries\n",
      "  âœ“ ë‚ ì”¨1.csv: 6 entries\n",
      "  âœ“ ë¹ ë”ë„ˆìŠ¤1.csv: 16 entries\n",
      "  âœ“ ì„¤ë¯¼ì„1.csv: 27 entries\n",
      "  âœ“ ì„¸ë°”ì‹œ1.csv: 36 entries\n",
      "  âœ“ ìŠ¤í‹°ë¸Œì¡ìŠ¤1_test.csv: 45 entries\n",
      "  âœ“ ìŠ¤í‹°ë¸Œì¡ìŠ¤2.csv: 16 entries\n",
      "  âœ“ ìŠ¤í‹°ë¸Œì¡ìŠ¤3.csv: 27 entries\n",
      "  âœ“ ìµœíƒœì„±1.csv: 64 entries\n",
      "  âœ“ ìµœíƒœì„±2_test.csv: 52 entries\n",
      "  âœ“ ìµœíƒœì„±3.csv: 14 entries\n",
      "\n",
      "[ê²°ê³¼ ìš”ì•½]\n",
      "  ì´ entries: 2930\n",
      "  Positive (ê°•ì¡°): 2930\n",
      "  Negative: 0\n",
      "  ê³ ìœ  ì˜ìƒ ìˆ˜: 53\n",
      "\n",
      "[ì €ì¥ ì™„ë£Œ] /home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\n",
      "  video_id                   video_path  start_sec  end_sec  label source_file\n",
      "0     v001  /home/piai/ë°”íƒ•í™”ë©´/ì˜ìƒraw/1.mp4      9.109    9.776      1       1.csv\n",
      "1     v001  /home/piai/ë°”íƒ•í™”ë©´/ì˜ìƒraw/1.mp4     19.753   20.454      1       1.csv\n",
      "2     v001  /home/piai/ë°”íƒ•í™”ë©´/ì˜ìƒraw/1.mp4     33.934   35.002      1       1.csv\n",
      "3     v001  /home/piai/ë°”íƒ•í™”ë©´/ì˜ìƒraw/1.mp4     37.871   39.873      1       1.csv\n",
      "4     v001  /home/piai/ë°”íƒ•í™”ë©´/ì˜ìƒraw/1.mp4     53.120   53.587      1       1.csv\n",
      "5     v001  /home/piai/ë°”íƒ•í™”ë©´/ì˜ìƒraw/1.mp4     57.658   58.358      1       1.csv\n",
      "6     v001  /home/piai/ë°”íƒ•í™”ë©´/ì˜ìƒraw/1.mp4     63.463   69.102      1       1.csv\n",
      "7     v001  /home/piai/ë°”íƒ•í™”ë©´/ì˜ìƒraw/1.mp4     79.680   80.180      1       1.csv\n",
      "8     v001  /home/piai/ë°”íƒ•í™”ë©´/ì˜ìƒraw/1.mp4     93.527   95.295      1       1.csv\n",
      "9     v001  /home/piai/ë°”íƒ•í™”ë©´/ì˜ìƒraw/1.mp4    100.968  102.369      1       1.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== ì‹¤í–‰ =====\n",
    "if __name__ == \"__main__\":\n",
    "    # ì„¤ì • - ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”\n",
    "    CSV_DIR = \"/home/stu/ai_project/ì˜ìƒcsv\"  # CSV íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”\n",
    "    OUTPUT_PATH = \"/home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\"  # í•©ì¹œ ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "    # í•©ì¹˜ê¸°ì—ì„œ ì œì™¸í•  íŒŒì¼ (ì´ë¯¸ í•©ì³ì§„ íŒŒì¼ ë“±)\n",
    "    EXCLUDE_FILES = [\"ìµœì¢…csv.csv\"]\n",
    "\n",
    "    # ì‹¤í–‰\n",
    "    merged = merge_csv_files(\n",
    "        csv_dir=CSV_DIR,\n",
    "        output_path=OUTPUT_PATH,\n",
    "        exclude_files=EXCLUDE_FILES\n",
    "    )\n",
    "\n",
    "    # ë¯¸ë¦¬ë³´ê¸°\n",
    "    if not merged.empty:\n",
    "        print(merged.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc8a82b",
   "metadata": {},
   "source": [
    "### csv ê²½ë¡œ ìˆ˜ì •í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d8237c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê²½ë¡œ ìˆ˜ì • ì™„ë£Œ! (ìƒìœ„ 5ê°œ ë¯¸ë¦¬ë³´ê¸°):\n",
      "/home/stu/ai_project/ì˜ìƒraw/1.mp4\n",
      "/home/stu/ai_project/ì˜ìƒraw/1.mp4\n",
      "/home/stu/ai_project/ì˜ìƒraw/1.mp4\n",
      "/home/stu/ai_project/ì˜ìƒraw/1.mp4\n",
      "/home/stu/ai_project/ì˜ìƒraw/1.mp4\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ê²½ë¡œ ì •ë¦¬ ë! ì €ì¥ëœ íŒŒì¼: /home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "input_path = \"/home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# ëª©í‘œ ê²½ë¡œ (ì„œë²„ì˜ ì‹¤ì œ ì˜ìƒ ìœ„ì¹˜)\n",
    "NEW_DIR = \"/home/stu/ai_project/ì˜ìƒraw\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ğŸ§¹ ì²­ì†Œ ë° ê²½ë¡œ ì¬ì„¤ì • í•¨ìˆ˜\n",
    "# ---------------------------------------------------------\n",
    "def clean_and_update_path(raw_path):\n",
    "    # 1. ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    path_str = str(raw_path)\n",
    "    \n",
    "    # 2. ìœˆë„ìš° ê²½ë¡œ(\\)ë¥¼ ë¦¬ëˆ…ìŠ¤ ê²½ë¡œ(/)ë¡œ í†µì¼ (ê·¸ë˜ì•¼ ìë¥´ê¸° ì‰¬ì›€)\n",
    "    # ì˜ˆ: C:\\Users\\uni71\\Desktop\\ì˜ìƒraw\\53.mp4 -> C:/Users/uni71/Desktop/ì˜ìƒraw/53.mp4\n",
    "    path_str = path_str.replace('\\\\', '/')\n",
    "    \n",
    "    # 3. ê²½ë¡œ ë‹¤ ë–¼ê³  'íŒŒì¼ëª…'ë§Œ ì¶”ì¶œ\n",
    "    # ì˜ˆ: .../53.mp4 -> 53.mp4 / .../43 -> 43\n",
    "    filename = path_str.split('/')[-1]\n",
    "    \n",
    "    # 4. í™•ì¥ì(.mp4) ì—†ìœ¼ë©´ ë¶™ì—¬ì£¼ê¸°\n",
    "    if not filename.lower().endswith('.mp4'):\n",
    "        filename += '.mp4'\n",
    "        \n",
    "    # 5. ìš°ë¦¬ê°€ ì›í•˜ëŠ” ìƒˆ ê²½ë¡œì™€ í•©ì¹˜ê¸°\n",
    "    # ê²°ê³¼: /home/stu/ai_project/ì˜ìƒraw/53.mp4\n",
    "    return os.path.join(NEW_DIR, filename)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 2. í•¨ìˆ˜ ì ìš©\n",
    "df[\"video_path\"] = df[\"video_path\"].apply(clean_and_update_path)\n",
    "\n",
    "# 3. ê²°ê³¼ í™•ì¸ (ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤ê°€ ì˜ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸)\n",
    "print(\"âœ… ê²½ë¡œ ìˆ˜ì • ì™„ë£Œ! (ìƒìœ„ 5ê°œ ë¯¸ë¦¬ë³´ê¸°):\")\n",
    "for path in df[\"video_path\"].head().tolist():\n",
    "    print(path)\n",
    "\n",
    "# 4. ì €ì¥í•˜ê¸°\n",
    "save_path = \"/home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\"\n",
    "df.to_csv(save_path, index=False)\n",
    "print(f\"\\nğŸ‰ ëª¨ë“  ê²½ë¡œ ì •ë¦¬ ë! ì €ì¥ëœ íŒŒì¼: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62fe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ Device: cuda\n",
      "[INFO] Loaded 82 positive entries from /home/stu/ai_project/ì˜ìƒcsv/100.csv\n",
      "[INFO] Generated 3 negative entries (Ratio 1:1)\n",
      "âœ… ìµœì¢… ë°ì´í„°ì…‹: ì´ 85ê°œ (Pos: 82, Neg: 3)\n",
      "\n",
      "ğŸ§  Loading Model: /home/stu/ai_project/ëª¨ë¸ì§‘í•©/gesture_model.pt\n",
      "Loaded gesture model from: /home/stu/ai_project/ëª¨ë¸ì§‘í•©/gesture_model.pt\n",
      "ğŸš€ Evaluating...\n",
      "\n",
      "==================================================\n",
      "ğŸ† [ìµœì¢… ì„±ì í‘œ] ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼\n",
      "==================================================\n",
      "ğŸ“Š ì •í™•ë„ (Accuracy): 83.53%\n",
      "\n",
      "ğŸ“‹ ìƒì„¸ ë¦¬í¬íŠ¸ (Precision, Recall, F1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " ë¹„ê°•ì¡°(Normal)     0.0000    0.0000    0.0000         3\n",
      "ê°•ì¡°(Emphasis)     0.9595    0.8659    0.9103        82\n",
      "\n",
      "    accuracy                         0.8353        85\n",
      "   macro avg     0.4797    0.4329    0.4551        85\n",
      "weighted avg     0.9256    0.8353    0.8781        85\n",
      "\n",
      "ğŸ” ì˜¤ë‹µ ë…¸íŠ¸ (Confusion Matrix):\n",
      "--------------------------------------------------\n",
      "               [ì˜ˆì¸¡: ë¹„ê°•ì¡°]   [ì˜ˆì¸¡: ê°•ì¡°]\n",
      "[ì‹¤ì œ: ë¹„ê°•ì¡°]      0     (ì •ë‹µ)      3     (ì„¤ë ˆë°œ)\n",
      "[ì‹¤ì œ: ê°•ì¡°]        11    (ë†“ì¹¨)      71    (ì •ë‹µ)\n",
      "--------------------------------------------------\n",
      "ğŸ‘‰ 'ì†Œê·¹ì  íƒì§€' ê²½í–¥: ì‹¤ì œ ê°•ì¡° êµ¬ê°„ì„ ë†“ì¹˜ëŠ” ê²½ìš°ê°€ ë” ë§ìŠµë‹ˆë‹¤.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# ê¸°ì¡´ í”„ë¡œì íŠ¸ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¬ ê²ƒë“¤\n",
    "from config import GESTURE_CONFIG\n",
    "from utils import load_gesture_model, VideoEmphasisDataset\n",
    "\n",
    "# =============================================================================\n",
    "# 1. ì„¤ì • (ê²½ë¡œ ë° ëª¨ë¸ ì§€ì •)\n",
    "# =============================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# í‰ê°€í•  CSV íŒŒì¼\n",
    "TEST_CSV_PATH = \"/home/stu/ai_project/ì˜ìƒcsv/100.csv\"\n",
    "\n",
    "# í‰ê°€í•  ëª¨ë¸ ê²½ë¡œ\n",
    "MODEL_PATH = \"/home/stu/ai_project/ëª¨ë¸ì§‘í•©/gesture_model.pt\"\n",
    "\n",
    "# =============================================================================\n",
    "# 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ì œê³µí•´ì£¼ì‹  ì½”ë“œ ë°˜ì˜)\n",
    "# =============================================================================\n",
    "\n",
    "def load_label_entries(csv_path):\n",
    "    entries = []\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"[WARN] CSV file not found: {csv_path}\")\n",
    "        return []\n",
    "\n",
    "    # ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•œ ì²˜ë¦¬ (utf-8 ì‹œë„ í›„ cp949 ì‹œë„)\n",
    "    try:\n",
    "        f = open(csv_path, newline='', encoding='utf-8')\n",
    "        reader = csv.DictReader(f)\n",
    "        next(reader) # í—¤ë” ì½ê¸° í…ŒìŠ¤íŠ¸\n",
    "        f.seek(0)\n",
    "    except UnicodeDecodeError:\n",
    "        f.close()\n",
    "        f = open(csv_path, newline='', encoding='cp949')\n",
    "\n",
    "    with f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            entries.append({\n",
    "                \"video_id\": row.get(\"video_id\", \"unknown\"),\n",
    "                \"video_path\": row[\"video_path\"],\n",
    "                \"start_sec\": float(row[\"start_sec\"]),\n",
    "                \"end_sec\": float(row[\"end_sec\"]),\n",
    "                \"label\": int(row[\"label\"]),\n",
    "            })\n",
    "    print(f\"[INFO] Loaded {len(entries)} positive entries from {csv_path}\")\n",
    "    return entries\n",
    "\n",
    "def generate_negative_entries(positive_entries, num_neg_per_pos=1, clip_duration=2.0, min_gap=0.5):\n",
    "    if not positive_entries:\n",
    "        return []\n",
    "\n",
    "    by_video = defaultdict(list)\n",
    "    for e in positive_entries:\n",
    "        by_video[e[\"video_path\"]].append(e)\n",
    "\n",
    "    all_negative = []\n",
    "\n",
    "    for video_path, pos_list in by_video.items():\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"[WARNING] Cannot open video: {video_path}\")\n",
    "            continue\n",
    "\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        video_duration = total_frames / fps if fps > 0 else 0\n",
    "        cap.release()\n",
    "\n",
    "        if video_duration < clip_duration:\n",
    "            continue\n",
    "\n",
    "        # ì‹œê°„ìˆœ ì •ë ¬\n",
    "        sorted_pos = sorted(pos_list, key=lambda x: x['start_sec'])\n",
    "        video_id = sorted_pos[0].get('video_id', 'unknown')\n",
    "        negative_candidates = []\n",
    "\n",
    "        # 1. ì²« ë²ˆì§¸ positive ì´ì „ êµ¬ê°„\n",
    "        first_start = sorted_pos[0]['start_sec']\n",
    "        if first_start > clip_duration + min_gap:\n",
    "            t = 0.0\n",
    "            while t + clip_duration <= first_start - min_gap:\n",
    "                negative_candidates.append({\n",
    "                    'video_id': video_id, 'video_path': video_path,\n",
    "                    'start_sec': round(t, 3), 'end_sec': round(t + clip_duration, 3),\n",
    "                    'label': 0\n",
    "                })\n",
    "                t += clip_duration\n",
    "\n",
    "        # 2. Positive êµ¬ê°„ ì‚¬ì´ì‚¬ì´\n",
    "        for i in range(len(sorted_pos) - 1):\n",
    "            gap_start = sorted_pos[i]['end_sec'] + min_gap\n",
    "            gap_end = sorted_pos[i + 1]['start_sec'] - min_gap\n",
    "\n",
    "            if gap_end - gap_start >= clip_duration:\n",
    "                t = gap_start\n",
    "                while t + clip_duration <= gap_end:\n",
    "                    negative_candidates.append({\n",
    "                        'video_id': video_id, 'video_path': video_path,\n",
    "                        'start_sec': round(t, 3), 'end_sec': round(t + clip_duration, 3),\n",
    "                        'label': 0\n",
    "                    })\n",
    "                    t += clip_duration\n",
    "\n",
    "        # 3. ë§ˆì§€ë§‰ positive ì´í›„ êµ¬ê°„\n",
    "        last_end = sorted_pos[-1]['end_sec']\n",
    "        if video_duration - last_end > clip_duration + min_gap:\n",
    "            t = last_end + min_gap\n",
    "            while t + clip_duration <= video_duration - min_gap:\n",
    "                negative_candidates.append({\n",
    "                    'video_id': video_id, 'video_path': video_path,\n",
    "                    'start_sec': round(t, 3), 'end_sec': round(t + clip_duration, 3),\n",
    "                    'label': 0\n",
    "                })\n",
    "                t += clip_duration\n",
    "\n",
    "        # ìƒ˜í”Œë§ (Positive ê°œìˆ˜ * ë¹„ìœ¨ ë§Œí¼)\n",
    "        target_num = int(len(pos_list) * num_neg_per_pos)\n",
    "        if len(negative_candidates) > target_num:\n",
    "            sampled = random.sample(negative_candidates, target_num)\n",
    "        else:\n",
    "            sampled = negative_candidates\n",
    "\n",
    "        all_negative.extend(sampled)\n",
    "\n",
    "    print(f\"[INFO] Generated {len(all_negative)} negative entries (Ratio {num_neg_per_pos}:1)\")\n",
    "    return all_negative\n",
    "\n",
    "def prepare_all_entries(csv_path, num_neg_per_pos=1, shuffle=True):\n",
    "    # 1. ê¸ì • ë°ì´í„° ë¡œë“œ\n",
    "    pos = load_label_entries(csv_path)\n",
    "    \n",
    "    # 2. ë¶€ì • ë°ì´í„° ìë™ ìƒì„±\n",
    "    neg = generate_negative_entries(pos, num_neg_per_pos=num_neg_per_pos)\n",
    "    \n",
    "    # 3. í†µí•©\n",
    "    all_e = pos + neg\n",
    "    if shuffle:\n",
    "        random.shuffle(all_e)\n",
    "        \n",
    "    print(f\"âœ… ìµœì¢… ë°ì´í„°ì…‹: ì´ {len(all_e)}ê°œ (Pos: {len(pos)}, Neg: {len(neg)})\")\n",
    "    return all_e, len(pos), len(neg)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ì‹¤í–‰ ë° í‰ê°€ ë¡œì§\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"ğŸ–¥ï¸ Device: {device}\")\n",
    "    \n",
    "    # 1. ë°ì´í„° ì¤€ë¹„ (ì œê³µí•´ì£¼ì‹  í•¨ìˆ˜ ì‚¬ìš©)\n",
    "    # num_neg_per_pos=1 : ê¸ì • 1ê°œë‹¹ ë¶€ì • 1ê°œ ìƒì„± (1:1 ë¹„ìœ¨)\n",
    "    all_entries, num_pos, num_neg = prepare_all_entries(TEST_CSV_PATH, num_neg_per_pos=1, shuffle=True)\n",
    "    \n",
    "    if len(all_entries) == 0:\n",
    "        print(\"âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        exit()\n",
    "\n",
    "    # 2. ë°ì´í„°ì…‹ & ë¡œë” ìƒì„±\n",
    "    test_dataset = VideoEmphasisDataset(\n",
    "        all_entries,\n",
    "        clip_len=GESTURE_CONFIG[\"clip_len\"],\n",
    "        resize_hw=GESTURE_CONFIG[\"resize_hw\"]\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 3. ëª¨ë¸ ë¡œë“œ\n",
    "    print(f\"\\nğŸ§  Loading Model: {MODEL_PATH}\")\n",
    "    model = load_gesture_model(MODEL_PATH, device)\n",
    "    model.eval()\n",
    "\n",
    "    # 4. ì¶”ë¡  ì‹¤í–‰\n",
    "    print(\"ğŸš€ Evaluating...\")\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    # 5. ê²°ê³¼ ë¦¬í¬íŠ¸\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ† [ìµœì¢… ì„±ì í‘œ] ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # ì •í™•ë„\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"ğŸ“Š ì •í™•ë„ (Accuracy): {acc * 100:.2f}%\")\n",
    "\n",
    "    # ìƒì„¸ ë¦¬í¬íŠ¸\n",
    "    target_names = ['ë¹„ê°•ì¡°(Normal)', 'ê°•ì¡°(Emphasis)']\n",
    "    print(\"\\nğŸ“‹ ìƒì„¸ ë¦¬í¬íŠ¸ (Precision, Recall, F1):\")\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names, digits=4, zero_division=0))\n",
    "\n",
    "    # í˜¼ë™ í–‰ë ¬\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # 2x2 í–‰ë ¬ì¼ ê²½ìš°ì—ë§Œ ìƒì„¸ ì¶œë ¥\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        print(\"ğŸ” ì˜¤ë‹µ ë…¸íŠ¸ (Confusion Matrix):\")\n",
    "        print(f\"--------------------------------------------------\")\n",
    "        print(f\"               [ì˜ˆì¸¡: ë¹„ê°•ì¡°]   [ì˜ˆì¸¡: ê°•ì¡°]\")\n",
    "        print(f\"[ì‹¤ì œ: ë¹„ê°•ì¡°]      {tn:<5} (ì •ë‹µ)      {fp:<5} (ì„¤ë ˆë°œ)\")\n",
    "        print(f\"[ì‹¤ì œ: ê°•ì¡°]        {fn:<5} (ë†“ì¹¨)      {tp:<5} (ì •ë‹µ)\")\n",
    "        print(f\"--------------------------------------------------\")\n",
    "        \n",
    "        if fp > fn:\n",
    "            print(\"ğŸ‘‰ 'ê³¼ì‰ íƒì§€' ê²½í–¥: ë¹„ê°•ì¡° êµ¬ê°„ì„ ê°•ì¡°ë¼ê³  ì˜ëª» ì¡ëŠ” ê²½ìš°ê°€ ë” ë§ìŠµë‹ˆë‹¤.\")\n",
    "        elif fn > fp:\n",
    "            print(\"ğŸ‘‰ 'ì†Œê·¹ì  íƒì§€' ê²½í–¥: ì‹¤ì œ ê°•ì¡° êµ¬ê°„ì„ ë†“ì¹˜ëŠ” ê²½ìš°ê°€ ë” ë§ìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(\"ğŸ‘‰ ê· í˜• ì¡íŒ ëª¨ë¸ì…ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"ğŸ” Confusion Matrix:\\n\", cm)\n",
    "    \n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74e32d",
   "metadata": {},
   "source": [
    "#ëª¨ë¸ë¹„êµ r3d_18ì„ ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c401b19003c940a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ Device: cuda\n",
      "   GPU: Tesla P40\n",
      "\n",
      "[1/6] ë°ì´í„° ì¤€ë¹„...\n",
      "  âœ… ë³„ë„ Test CSV ì‚¬ìš©: /home/stu/ai_project/ì˜ìƒcsv/ìŠ¤í‹°ë¸Œì¡ìŠ¤1_test.csv\n",
      "[INFO] Video Split: Train=16, Val=4, Test=0\n",
      "\n",
      "  ğŸ“Š ë°ì´í„° ë¶„í¬:\n",
      "     Train: 5789 (Pos: 2631, Neg: 3158)\n",
      "     Val:    784 (Pos: 299, Neg: 485)\n",
      "     Test:    45 (Pos: 45, Neg: 0)\n",
      "\n",
      "ğŸ”¥ Training: r3d_18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n",
      "[h264 @ 0x25e90a40] mmco: unref short failure\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 381\u001b[0m\n\u001b[1;32m    378\u001b[0m history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: []} \u001b[38;5;66;03m# íˆìŠ¤í† ë¦¬ ì €ì¥\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m--> 381\u001b[0m     t_loss, t_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     v_loss, v_acc, v_f1, _, _ \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, crit, device)\n\u001b[1;32m    384\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(t_loss)\n",
      "Cell \u001b[0;32mIn[15], line 294\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    292\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    293\u001b[0m total_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    295\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    296\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1420\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1420\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1422\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ¬ 3D CNN ëª¨ë¸ ë¹„êµ ì‹¤í—˜ (Train/Val/Test ë¶„ë¦¬ + Safe Negative Sampling)\n",
    "# r3d_18 vs mc3_18 vs r2plus1d_18 ê³µì •í•œ ì„±ëŠ¥ ë¹„êµ\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torchvision.models.video import (\n",
    "    r3d_18, R3D_18_Weights,\n",
    "    mc3_18, MC3_18_Weights,\n",
    "    r2plus1d_18, R2Plus1D_18_Weights\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“ ì„¤ì •\n",
    "# =============================================================================\n",
    "# â­ ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”!\n",
    "TRAIN_CSV_PATH = \"/home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\"\n",
    "TEST_CSV_PATH = \"/home/stu/ai_project/ì˜ìƒcsv/ìŠ¤í‹°ë¸Œì¡ìŠ¤1_test.csv\" # ì—†ìœ¼ë©´ None\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "CONFIG = {\n",
    "    \"clip_len\": 16,\n",
    "    \"resize_hw\": (112, 112),\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 10,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"num_neg_per_pos\": 2, # ê°•ì¡° 1ê°œë‹¹ ë¹„ê°•ì¡° 2ê°œ ìƒì„±\n",
    "    \"seed\": 42,\n",
    "    \"train_ratio\": 0.6,\n",
    "    \"val_ratio\": 0.2,\n",
    "    \"test_ratio\": 0.2,\n",
    "}\n",
    "\n",
    "# ì¬í˜„ì„±\n",
    "random.seed(CONFIG[\"seed\"])\n",
    "np.random.seed(CONFIG[\"seed\"])\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG[\"seed\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“Š ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ë“¤\n",
    "# =============================================================================\n",
    "def load_label_entries(csv_path):\n",
    "    \"\"\"CSVì—ì„œ ë¼ë²¨ ë¡œë“œ\"\"\"\n",
    "    import csv\n",
    "    entries = []\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"[ERROR] CSV not found: {csv_path}\")\n",
    "        return []\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            entries.append({\n",
    "                \"video_id\": row.get(\"video_id\", os.path.basename(row[\"video_path\"])),\n",
    "                \"video_path\": row[\"video_path\"],\n",
    "                \"start_sec\": float(row[\"start_sec\"]),\n",
    "                \"end_sec\": float(row[\"end_sec\"]),\n",
    "                \"label\": int(row[\"label\"]),\n",
    "            })\n",
    "    return entries\n",
    "\n",
    "# â˜… [í†µí•©ë¨] ì•ˆì „í•˜ê³  íš¨ìœ¨ì ì¸ ë¹„ê°•ì¡°(0) ìƒì„± í•¨ìˆ˜\n",
    "def generate_negative_entries(positive_entries, num_neg_per_pos=2, clip_duration=2.0, min_gap=0.1):\n",
    "    \"\"\"\n",
    "    ë¼ë²¨ëœ êµ¬ê°„(1)ì„ ì œì™¸í•œ ë¹ˆ ê³µê°„(Free Gaps)ì—ì„œ ì•ˆì „í•˜ê²Œ ë¹„ê°•ì¡°(0) êµ¬ê°„ ìƒì„±\n",
    "    \"\"\"\n",
    "    if not positive_entries:\n",
    "        return []\n",
    "\n",
    "    # ë¹„ë””ì˜¤ë³„ ê·¸ë£¹í™”\n",
    "    by_video = defaultdict(list)\n",
    "    for e in positive_entries:\n",
    "        by_video[e[\"video_path\"]].append(e)\n",
    "\n",
    "    all_negative = []\n",
    "\n",
    "    for video_path, pos_list in by_video.items():\n",
    "        # ë¹„ë””ì˜¤ ì •ë³´ ì½ê¸°\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            # print(f\"[WARNING] Cannot open video: {video_path}\")\n",
    "            continue\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        video_duration = total_frames / fps if fps > 0 else 0\n",
    "        cap.release()\n",
    "\n",
    "        if video_duration < clip_duration:\n",
    "            continue\n",
    "\n",
    "        # ì‚¬ìš© ë¶ˆê°€ êµ¬ê°„ (Blocked) ê³„ì‚°: ê°•ì¡° êµ¬ê°„ + ì•ë’¤ ë§ˆì§„(min_gap)\n",
    "        blocked = []\n",
    "        for e in pos_list:\n",
    "            blocked.append((max(0, e[\"start_sec\"] - min_gap), min(video_duration, e[\"end_sec\"] + min_gap)))\n",
    "        \n",
    "        # ì‹œê°„ìˆœ ì •ë ¬\n",
    "        blocked.sort(key=lambda x: x[0])\n",
    "\n",
    "        # ë¹ˆ êµ¬ê°„ (Free Gaps) ì°¾ê¸°\n",
    "        free_gaps = []\n",
    "        prev_end = 0.0\n",
    "        for bs, be in blocked:\n",
    "            if bs > prev_end + clip_duration: # í´ë¦½ í•˜ë‚˜ ë“¤ì–´ê°ˆ í‹ˆì´ ìˆìœ¼ë©´\n",
    "                free_gaps.append((prev_end, bs))\n",
    "            prev_end = max(prev_end, be)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ êµ¬ê°„ ì²´í¬\n",
    "        if prev_end + clip_duration < video_duration:\n",
    "            free_gaps.append((prev_end, video_duration))\n",
    "\n",
    "        # ìƒ˜í”Œë§ (í•„ìš”í•œ ê°œìˆ˜ë§Œí¼)\n",
    "        target_num = len(pos_list) * num_neg_per_pos\n",
    "        \n",
    "        # ëª¨ë“  ê°€ëŠ¥í•œ í›„ë³´ë¥¼ ë‹¤ ë§Œë“  ë’¤ ëœë¤ ì„ íƒ (ê· ë“± ë¶„í¬ íš¨ê³¼)\n",
    "        candidates = []\n",
    "        for gap_start, gap_end in free_gaps:\n",
    "            t = gap_start\n",
    "            while t + clip_duration <= gap_end:\n",
    "                candidates.append((t, t + clip_duration))\n",
    "                t += clip_duration \n",
    "        \n",
    "        # í›„ë³´ ì¤‘ì—ì„œ ëœë¤ ìƒ˜í”Œë§\n",
    "        if len(candidates) > target_num:\n",
    "            selected = random.sample(candidates, target_num)\n",
    "        else:\n",
    "            selected = candidates \n",
    "            \n",
    "        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "        video_id = pos_list[0].get('video_id', 'unknown')\n",
    "        for s, e in selected:\n",
    "            all_negative.append({\n",
    "                'video_id': video_id,\n",
    "                'video_path': video_path,\n",
    "                'start_sec': round(s, 3),\n",
    "                'end_sec': round(e, 3),\n",
    "                'label': 0\n",
    "            })\n",
    "\n",
    "    # print(f\"[INFO] Generated {len(all_negative)} negative entries\")\n",
    "    return all_negative\n",
    "\n",
    "\n",
    "def split_by_video_3way(entries, train_ratio=0.6, val_ratio=0.2, seed=42):\n",
    "    \"\"\"ë¹„ë””ì˜¤ ID ê¸°ì¤€ ë°ì´í„° ë¶„í• \"\"\"\n",
    "    random.seed(seed)\n",
    "    by_video = defaultdict(list)\n",
    "    for e in entries:\n",
    "        vid = e.get('video_id', e.get('video_path', 'unknown'))\n",
    "        by_video[vid].append(e)\n",
    "\n",
    "    video_ids = list(by_video.keys())\n",
    "    random.shuffle(video_ids)\n",
    "\n",
    "    n_videos = len(video_ids)\n",
    "    train_end = int(n_videos * train_ratio)\n",
    "    val_end = int(n_videos * (train_ratio + val_ratio))\n",
    "\n",
    "    train_vids = set(video_ids[:train_end])\n",
    "    val_vids = set(video_ids[train_end:val_end])\n",
    "    test_vids = set(video_ids[val_end:])\n",
    "\n",
    "    train_entries, val_entries, test_entries = [], [], []\n",
    "\n",
    "    for vid, ents in by_video.items():\n",
    "        if vid in train_vids: train_entries.extend(ents)\n",
    "        elif vid in val_vids: val_entries.extend(ents)\n",
    "        else: test_entries.extend(ents)\n",
    "\n",
    "    random.shuffle(train_entries)\n",
    "    random.shuffle(val_entries)\n",
    "    random.shuffle(test_entries)\n",
    "\n",
    "    print(f\"[INFO] Video Split: Train={len(train_vids)}, Val={len(val_vids)}, Test={len(test_vids)}\")\n",
    "    return train_entries, val_entries, test_entries\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“¦ Dataset & Model\n",
    "# =============================================================================\n",
    "class VideoEmphasisDataset(Dataset):\n",
    "    def __init__(self, entries, clip_len=16, resize_hw=(112, 112)):\n",
    "        self.entries = entries\n",
    "        self.clip_len = clip_len\n",
    "        self.resize_hw = resize_hw\n",
    "\n",
    "    def __len__(self): return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.entries[idx]\n",
    "        cap = cv2.VideoCapture(entry[\"video_path\"])\n",
    "        if not cap.isOpened():\n",
    "            return torch.zeros(3, self.clip_len, *self.resize_hw), torch.tensor(entry[\"label\"], dtype=torch.long)\n",
    "\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        start_frame = int(entry[\"start_sec\"] * fps)\n",
    "        end_frame = int(entry[\"end_sec\"] * fps)\n",
    "        \n",
    "        start_frame = max(0, min(start_frame, total_frames - 1))\n",
    "        end_frame = max(0, min(end_frame, total_frames - 1))\n",
    "        if end_frame <= start_frame: end_frame = min(start_frame + self.clip_len, total_frames - 1)\n",
    "        \n",
    "        seg_len = end_frame - start_frame + 1\n",
    "        if seg_len >= self.clip_len:\n",
    "            indices = np.linspace(start_frame, end_frame, num=self.clip_len, dtype=int)\n",
    "        else:\n",
    "            indices = np.zeros(self.clip_len, dtype=int)\n",
    "            if seg_len > 0:\n",
    "                base = np.linspace(start_frame, end_frame, num=seg_len, dtype=int)\n",
    "                repeat = int(np.ceil(self.clip_len / seg_len))\n",
    "                indices = np.tile(base, repeat)[:self.clip_len]\n",
    "\n",
    "        frames = []\n",
    "        for f in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, f)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: frame = np.zeros((*self.resize_hw, 3), dtype=np.uint8)\n",
    "            else: frame = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), self.resize_hw)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        \n",
    "        x = torch.from_numpy(np.stack(frames)).float().permute(3, 0, 1, 2)\n",
    "        x = (x / 255.0 - 0.5) / 0.5\n",
    "        return x, torch.tensor(entry[\"label\"], dtype=torch.long)\n",
    "\n",
    "class GestureModel3D(nn.Module):\n",
    "    def __init__(self, backbone_name='r3d_18', num_classes=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        if backbone_name == 'r3d_18':\n",
    "            weights = R3D_18_Weights.KINETICS400_V1 if pretrained else None\n",
    "            self.backbone = r3d_18(weights=weights)\n",
    "        elif backbone_name == 'mc3_18':\n",
    "            weights = MC3_18_Weights.KINETICS400_V1 if pretrained else None\n",
    "            self.backbone = mc3_18(weights=weights)\n",
    "        elif backbone_name == 'r2plus1d_18':\n",
    "            weights = R2Plus1D_18_Weights.KINETICS400_V1 if pretrained else None\n",
    "            self.backbone = r2plus1d_18(weights=weights)\n",
    "        \n",
    "        self.backbone.fc = nn.Identity()\n",
    "        # in_features ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°\n",
    "        dummy_input = torch.randn(1, 3, 16, 112, 112)\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(dummy_input)\n",
    "        in_features = out.shape[1]\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.backbone(x))\n",
    "    \n",
    "    def get_num_params(self): return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "def measure_inference_time(model, device, num_runs=10):\n",
    "    model.eval()\n",
    "    x = torch.randn(1, 3, 16, 112, 112).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5): model(x) # warmup\n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            s = time.time()\n",
    "            model(x)\n",
    "            if device.type == 'cuda': torch.cuda.synchronize()\n",
    "            times.append(time.time() - s)\n",
    "    return np.mean(times)*1000, np.std(times)*1000\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸš€ ì‹¤í—˜ ì‹¤í–‰ (Training Loop)\n",
    "# =============================================================================\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return total_loss/len(loader), correct/total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            total_loss += criterion(out, y).item()\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            targets.extend(y.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average='weighted', zero_division=0)\n",
    "    return total_loss/len(loader), acc, f1, preds, targets\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ ë° ë¶„í• \n",
    "print(\"\\n[1/6] ë°ì´í„° ì¤€ë¹„...\")\n",
    "pos_entries = load_label_entries(TRAIN_CSV_PATH)\n",
    "neg_entries = generate_negative_entries(pos_entries, CONFIG[\"num_neg_per_pos\"]) # â˜… ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©\n",
    "all_entries = pos_entries + neg_entries\n",
    "random.shuffle(all_entries)\n",
    "\n",
    "if TEST_CSV_PATH and os.path.exists(TEST_CSV_PATH):\n",
    "    print(f\"  âœ… ë³„ë„ Test CSV ì‚¬ìš©: {TEST_CSV_PATH}\")\n",
    "    test_pos = load_label_entries(TEST_CSV_PATH)\n",
    "    test_neg = generate_negative_entries(test_pos, CONFIG[\"num_neg_per_pos\"]) # â˜… ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©\n",
    "    test_entries = test_pos + test_neg\n",
    "    train_entries, val_entries, _ = split_by_video_3way(all_entries, 0.8, 0.2, CONFIG[\"seed\"])\n",
    "else:\n",
    "    train_entries, val_entries, test_entries = split_by_video_3way(all_entries, CONFIG[\"train_ratio\"], CONFIG[\"val_ratio\"], CONFIG[\"seed\"])\n",
    "\n",
    "def count_labels(entries):\n",
    "    pos = sum(1 for e in entries if e['label'] == 1)\n",
    "    neg = sum(1 for e in entries if e['label'] == 0)\n",
    "    return pos, neg\n",
    "\n",
    "train_pos, train_neg = count_labels(train_entries)\n",
    "val_pos, val_neg = count_labels(val_entries)\n",
    "test_pos, test_neg = count_labels(test_entries)\n",
    "\n",
    "print(f\"\\n  ğŸ“Š ë°ì´í„° ë¶„í¬:\")\n",
    "print(f\"     Train: {len(train_entries):4d} (Pos: {train_pos}, Neg: {train_neg})\")\n",
    "print(f\"     Val:   {len(val_entries):4d} (Pos: {val_pos}, Neg: {val_neg})\")\n",
    "print(f\"     Test:  {len(test_entries):4d} (Pos: {test_pos}, Neg: {test_neg})\")\n",
    "\n",
    "# ë°ì´í„°ë¡œë”\n",
    "train_loader = DataLoader(VideoEmphasisDataset(train_entries, CONFIG[\"clip_len\"], CONFIG[\"resize_hw\"]), \n",
    "                          batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(VideoEmphasisDataset(val_entries, CONFIG[\"clip_len\"], CONFIG[\"resize_hw\"]), \n",
    "                        batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(VideoEmphasisDataset(test_entries, CONFIG[\"clip_len\"], CONFIG[\"resize_hw\"]), \n",
    "                         batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n",
    "\n",
    "# 2. ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ\n",
    "MODELS = ['r3d_18', 'mc3_18', 'r2plus1d_18']\n",
    "results = {}\n",
    "test_results = {} # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ìš©\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (Train ê¸°ì¤€)\n",
    "pos_weight = train_neg / train_pos if train_pos > 0 else 1.0\n",
    "class_weights = torch.tensor([1.0, pos_weight]).to(device)\n",
    "\n",
    "for name in MODELS:\n",
    "    print(f\"\\nğŸ”¥ Training: {name}\")\n",
    "    model = GestureModel3D(name).to(device)\n",
    "    crit = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    opt = optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_state = None\n",
    "    \n",
    "    history = {'train_loss': [], 'val_f1': []} # íˆìŠ¤í† ë¦¬ ì €ì¥\n",
    "\n",
    "    for ep in range(CONFIG[\"num_epochs\"]):\n",
    "        t_loss, t_acc = train_one_epoch(model, train_loader, opt, crit, device)\n",
    "        v_loss, v_acc, v_f1, _, _ = evaluate(model, val_loader, crit, device)\n",
    "        \n",
    "        history['train_loss'].append(t_loss)\n",
    "        history['val_f1'].append(v_f1)\n",
    "        \n",
    "        if v_f1 > best_f1:\n",
    "            best_f1 = v_f1\n",
    "            best_state = model.state_dict().copy()\n",
    "            \n",
    "        print(f\"  Ep {ep+1}: Train {t_loss:.4f}/{t_acc:.2f} | Val {v_loss:.4f}/{v_acc:.2f} (F1 {v_f1:.2f})\")\n",
    "    \n",
    "    # í•™ìŠµ ì™„ë£Œ ì •ë³´ ì €ì¥\n",
    "    inf_ms, _ = measure_inference_time(model, device)\n",
    "    results[name] = {\n",
    "        'best_val_f1': best_f1, \n",
    "        'inf_time_ms': inf_ms, \n",
    "        'params': model.get_num_params(),\n",
    "        'history': history,\n",
    "        'train_time': 0 # (ì‹œê°„ ì¸¡ì • ë¡œì§ì€ ìƒëµë¨, í•„ìš”ì‹œ ì¶”ê°€)\n",
    "    }\n",
    "\n",
    "    # Test í‰ê°€\n",
    "    model.load_state_dict(best_state)\n",
    "    _, test_acc, test_f1, preds, labels = evaluate(model, test_loader, crit, device)\n",
    "    \n",
    "    test_results[name] = {'test_acc': test_acc, 'test_f1': test_f1, 'test_preds': preds, 'test_labels': labels}\n",
    "    print(f\"  âœ… {name} Test Result -> Acc: {test_acc:.2f}, F1: {test_f1:.2f}\")\n",
    "\n",
    "\n",
    "# 3. ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ† ìµœì¢… ê²°ê³¼ ìš”ì•½\")\n",
    "print(f\"{'Model':<15} {'Params(M)':<10} {'Time(ms)':<10} {'Val F1':<10} {'Test Acc':<10} {'Test F1':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for name in MODELS:\n",
    "    r = results[name]\n",
    "    tr = test_results[name]\n",
    "    print(f\"{name:<15} {r['params']/1e6:<10.2f} {r['inf_time_ms']:<10.2f} {r['best_val_f1']:<10.4f} {tr['test_acc']:<10.4f} {tr['test_f1']:<10.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ìµœê³  ëª¨ë¸ ì„ ì •\n",
    "best_model = max(test_results.keys(), key=lambda x: test_results[x]['test_f1'])\n",
    "print(f\"\\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (Test ê¸°ì¤€): {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359cd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ Device: cuda\n",
      "\n",
      "======================================================================\n",
      "ğŸ¬ Simple MLP Fusion ì¶”ë¡ \n",
      "======================================================================\n",
      "\n",
      "[1/5] ğŸ“¦ ëª¨ë¸ ë¡œë“œ...\n",
      "  âœ… Gesture Model ë¡œë“œ ì™„ë£Œ\n",
      "  âœ… Audio Model ë¡œë“œ ì™„ë£Œ\n",
      "  âœ… Text Tensors ë¡œë“œ ì™„ë£Œ: 46ê°œ ë¬¸ì¥\n",
      "\n",
      "[2/5] ğŸ¥ ë¹„ë””ì˜¤ ë¶„ì„...\n",
      "  ì˜ìƒ: test_jo.mp4\n",
      "  FPS: 29.97, ì´ í”„ë ˆì„: 4268, ê¸¸ì´: 142.41ì´ˆ\n",
      "\n",
      "[3/5] ğŸµ ì˜¤ë””ì˜¤ ì¶”ì¶œ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2537931/3663372167.py:242: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y_audio, sr = librosa.load(VIDEO_PATH, sr=SR)\n",
      "/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì˜¤ë””ì˜¤ ê¸¸ì´: 142.52ì´ˆ\n",
      "\n",
      "[4/5] ğŸ” í´ë¦½ë³„ ì¶”ë¡  ì¤‘...\n",
      "  âœ… 532ê°œ í´ë¦½ ë¶„ì„ ì™„ë£Œ\n",
      "\n",
      "[5/5] ğŸ“Š ê²°ê³¼ ë¶„ì„...\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ì „ì²´ í†µê³„\n",
      "============================================================\n",
      "  ì´ í´ë¦½ ìˆ˜: 532\n",
      "  ê°•ì¡° í´ë¦½ ìˆ˜: 83 (15.6%)\n",
      "\n",
      "  [Gesture] í‰ê· : 0.1680, ìµœëŒ€: 0.9898\n",
      "  [Audio]   í‰ê· : 0.7951, ìµœëŒ€: 0.7962\n",
      "  [Text]    í‰ê· : 0.3557, ìµœëŒ€: 0.3801\n",
      "  [Fusion]  í‰ê· : 0.3999, ìµœëŒ€: 0.6919\n",
      "\n",
      "  ê°€ì¤‘ì¹˜: Gesture=0.35, Audio=0.25, Text=0.4\n",
      "\n",
      "============================================================\n",
      "ğŸ† ì˜ìƒ ì´ì : 39.99 / 100\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ ê°•ì¡° êµ¬ê°„ (fusion >= 0.5):\n",
      "  1. 1.07s ~ 1.33s (ê¸¸ì´: 0.26s)\n",
      "  2. 2.14s ~ 2.40s (ê¸¸ì´: 0.26s)\n",
      "  3. 12.28s ~ 14.15s (ê¸¸ì´: 1.87s)\n",
      "  4. 16.28s ~ 16.55s (ê¸¸ì´: 0.27s)\n",
      "  5. 26.43s ~ 26.69s (ê¸¸ì´: 0.26s)\n",
      "  6. 34.43s ~ 35.50s (ê¸¸ì´: 1.07s)\n",
      "  7. 36.84s ~ 37.10s (ê¸¸ì´: 0.26s)\n",
      "  8. 39.77s ~ 40.31s (ê¸¸ì´: 0.54s)\n",
      "  9. 40.57s ~ 41.11s (ê¸¸ì´: 0.54s)\n",
      "  10. 47.78s ~ 48.31s (ê¸¸ì´: 0.53s)\n",
      "  ... ì™¸ 25ê°œ\n",
      "\n",
      "âœ… ê²°ê³¼ ì €ì¥: /home/stu/ai_project/ê²°ê³¼/test_jo_fusion_result.json\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ í´ë¦½ë³„ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ)\n",
      "============================================================\n",
      "  Start     End  Gesture    Audio     Text   Fusion   ê°•ì¡°\n",
      "------------------------------------------------------------\n",
      "   0.00    0.53   0.0135   0.7958   0.3330   0.3368     \n",
      "   0.27    0.80   0.0319   0.7944   0.3330   0.3429     \n",
      "   0.53    1.07   0.0843   0.7945   0.3330   0.3613     \n",
      "   0.80    1.33   0.4698   0.7954   0.3330   0.4965     \n",
      "   1.07    1.60   0.7548   0.7950   0.3330   0.5961    âœ“\n",
      "   1.33    1.87   0.0272   0.7946   0.3330   0.3414     \n",
      "   1.60    2.14   0.4650   0.7942   0.3330   0.4945     \n",
      "   1.87    2.40   0.0945   0.7959   0.3330   0.3653     \n",
      "   2.14    2.67   0.6475   0.7951   0.3330   0.5586    âœ“\n",
      "   2.40    2.94   0.2920   0.7959   0.3330   0.4344     \n",
      "\n",
      "ğŸ‰ ì¶”ë¡  ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import osnn\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# =============================================================================\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "VIDEO_PATH = \"/home/stu/ai_project/ì˜ìƒraw/test_jo.mp4\"\n",
    "MODEL_DIR = \"/home/stu/ai_project/ëª¨ë¸ì§‘í•©\"\n",
    "\n",
    "GESTURE_MODEL_PATH = os.path.join(MODEL_DIR, \"gesture_model.pt\")\n",
    "AUDIO_MODEL_PATH = os.path.join(MODEL_DIR, \"voice_model.pth\")\n",
    "TEXT_TENSOR_PATH = os.path.join(MODEL_DIR, \"text_jo_torch_text.pt\")\n",
    "AUDIO_SCALER_PATH = os.path.join(MODEL_DIR, \"audio_scaler.pkl\")  # ìˆìœ¼ë©´ ì‚¬ìš©\n",
    "STT_JSON_PATH = \"/home/stu/ai_project/ê²°ê³¼/text.json\"\n",
    "\n",
    "# ì¶œë ¥ ê²½ë¡œ\n",
    "OUTPUT_JSON = \"/home/stu/ai_project/ê²°ê³¼/test_jo_fusion_result.json\"\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ì„¤ì • ( ì¡°ì ˆ ê°€ëŠ¥)\n",
    "WEIGHTS = {\n",
    "    \"gesture\": 0.35,\n",
    "    \"audio\": 0.25,\n",
    "    \"text\": 0.4,\n",
    "}\n",
    "\n",
    "# Gesture ì„¤ì •\n",
    "CLIP_LEN = 16\n",
    "STRIDE = 8\n",
    "RESIZE_HW = (112, 112)\n",
    "\n",
    "# Audio ì„¤ì • (Mel-Spectrogram)\n",
    "SR = 16000\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 512\n",
    "N_MELS = 80\n",
    "WINDOW_SIZE = 50\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ Device: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ—ï¸ ëª¨ë¸ ì •ì˜\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Gesture Model (r3d_18 ê¸°ë°˜)\n",
    "from torchvision.models.video import r3d_18\n",
    "\n",
    "class GestureModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(GestureModel, self).__init__()\n",
    "        self.backbone = r3d_18(weights=None)\n",
    "        in_features = self.backbone.fc.in_features  # 512\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.classifier = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x, return_feature=False):\n",
    "        features = self.backbone(x)  # (B, 512)\n",
    "        logits = self.classifier(features)\n",
    "        if return_feature:\n",
    "            return logits, features\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 2. Audio Model (BiLSTM, Mel-Spec 80ì°¨ì›, 4í´ë˜ìŠ¤)\n",
    "class AudioBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size=80, hidden_size=64, num_layers=2, \n",
    "                 num_classes=4, dropout=0.2):\n",
    "        super(AudioBiLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size= input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.feature_dim = hidden_size * 2  # 128\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_feature=False):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        features = lstm_out[:, -1, :]  # (B, 128)\n",
    "        logits = self.classifier(features)\n",
    "        if return_feature:\n",
    "            return logits, features\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 3. Text Model (Dummy - 768ì°¨ì› ì…ë ¥)\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=2):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "        self.feature_dim = input_dim\n",
    "    \n",
    "    def forward(self, x, return_feature=False):\n",
    "        logits = self.fc(x)\n",
    "        if return_feature:\n",
    "            return logits, x\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 4. Simple Fusion MLP\n",
    "class SimpleFusionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    ê°„ë‹¨í•œ Fusion MLP\n",
    "    Input: gesture_feat(512) + audio_feat(128) + text_feat(768) = 1408\n",
    "    Output: ê°•ì¡° í™•ë¥  (0~1)\n",
    "    \"\"\"\n",
    "    def __init__(self, gesture_dim=512, audio_dim=128, text_dim=768, hidden_dim=256):\n",
    "        super(SimpleFusionMLP, self).__init__()\n",
    "        \n",
    "        input_dim = gesture_dim + audio_dim + text_dim  # 1408\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, 1)  # ì´ì§„ ë¶„ë¥˜\n",
    "        )\n",
    "    \n",
    "    def forward(self, gesture_feat, audio_feat, text_feat):\n",
    "        combined = torch.cat([gesture_feat, audio_feat, text_feat], dim=1)\n",
    "        return self.net(combined)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "# =============================================================================\n",
    "\n",
    "def extract_mel_spectrogram(y_audio, sr=16000, n_mels=80, n_fft=1024, hop_length=512):\n",
    "    \"\"\"Mel-Spectrogram ì¶”ì¶œ\"\"\"\n",
    "    y_audio = librosa.util.normalize(y_audio)\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y_audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length\n",
    "    )\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    return mel_db.T  # (Time, n_mels)\n",
    "\n",
    "\n",
    "def pad_or_truncate(features, window_size):\n",
    "    \"\"\"ìœˆë„ìš° í¬ê¸°ì— ë§ê²Œ íŒ¨ë”©/ìë¥´ê¸°\"\"\"\n",
    "    if len(features) < window_size:\n",
    "        pad_len = window_size - len(features)\n",
    "        features = np.pad(features, ((0, pad_len), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        features = features[:window_size]\n",
    "    return features\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸš€ ë©”ì¸ ì‹¤í–‰\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¬ Simple MLP Fusion ì¶”ë¡ \")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. ëª¨ë¸ ë¡œë“œ\n",
    "print(\"\\n[1/5] ğŸ“¦ ëª¨ë¸ ë¡œë“œ...\")\n",
    "\n",
    "# Gesture Model\n",
    "gesture_model = GestureModel(num_classes=2).to(device)\n",
    "if os.path.exists(GESTURE_MODEL_PATH):\n",
    "    state = torch.load(GESTURE_MODEL_PATH, map_location=device)\n",
    "    # key ì´ë¦„ í˜¸í™˜ì„± ì²˜ë¦¬\n",
    "    new_state = {}\n",
    "    for k, v in state.items():\n",
    "        if 'fc' in k and 'backbone' not in k:\n",
    "            new_state[k.replace('fc', 'classifier')] = v\n",
    "        else:\n",
    "            new_state[k] = v\n",
    "    gesture_model.load_state_dict(new_state, strict=False)\n",
    "    print(f\"  âœ… Gesture Model ë¡œë“œ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(f\"  âš ï¸ Gesture Model ì—†ìŒ: {GESTURE_MODEL_PATH}\")\n",
    "gesture_model.eval()\n",
    "\n",
    "# Audio Model\n",
    "audio_model = AudioBiLSTM(\n",
    "    input_size=N_MELS, hidden_size=64, num_layers=2, num_classes=4, dropout=0.2\n",
    ").to(device)\n",
    "if os.path.exists(AUDIO_MODEL_PATH):\n",
    "    audio_model.load_state_dict(torch.load(AUDIO_MODEL_PATH, map_location=device), strict=False)\n",
    "    print(f\"  âœ… Audio Model ë¡œë“œ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(f\"  âš ï¸ Audio Model ì—†ìŒ: {AUDIO_MODEL_PATH}\")\n",
    "audio_model.eval()\n",
    "\n",
    "# Text Tensors\n",
    "text_tensors = []\n",
    "if os.path.exists(TEXT_TENSOR_PATH):\n",
    "    text_tensors = torch.load(TEXT_TENSOR_PATH, map_location=device, weights_only=False)\n",
    "    if isinstance(text_tensors, list):\n",
    "        print(f\"  âœ… Text Tensors ë¡œë“œ ì™„ë£Œ: {len(text_tensors)}ê°œ ë¬¸ì¥\")\n",
    "    else:\n",
    "        print(f\"  âœ… Text Tensors ë¡œë“œ ì™„ë£Œ: shape={text_tensors.shape}\")\n",
    "else:\n",
    "    print(f\"  âš ï¸ Text Tensors ì—†ìŒ: {TEXT_TENSOR_PATH}\")\n",
    "\n",
    "# Text Model (Dummy)\n",
    "text_model = TextModel(input_dim=768, num_classes=2).to(device)\n",
    "text_model.eval()\n",
    "\n",
    "# Fusion MLP (í•™ìŠµ ì•ˆ ëœ ìƒíƒœ - ê°€ì¤‘ì¹˜ í•©ì‚°ìœ¼ë¡œ ëŒ€ì²´)\n",
    "fusion_model = SimpleFusionMLP().to(device)\n",
    "fusion_model.eval()\n",
    "\n",
    "# 2. ë¹„ë””ì˜¤ ì •ë³´ ë¡œë“œ\n",
    "print(\"\\n[2/5] ğŸ¥ ë¹„ë””ì˜¤ ë¶„ì„...\")\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {VIDEO_PATH}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "duration = total_frames / fps\n",
    "\n",
    "print(f\"  ì˜ìƒ: {os.path.basename(VIDEO_PATH)}\")\n",
    "print(f\"  FPS: {fps:.2f}, ì´ í”„ë ˆì„: {total_frames}, ê¸¸ì´: {duration:.2f}ì´ˆ\")\n",
    "\n",
    "# 3. ì˜¤ë””ì˜¤ ë¡œë“œ\n",
    "print(\"\\n[3/5] ğŸµ ì˜¤ë””ì˜¤ ì¶”ì¶œ...\")\n",
    "\n",
    "try:\n",
    "    y_audio, sr = librosa.load(VIDEO_PATH, sr=SR)\n",
    "    print(f\"  ì˜¤ë””ì˜¤ ê¸¸ì´: {len(y_audio)/SR:.2f}ì´ˆ\")\n",
    "except Exception as e:\n",
    "    print(f\"  âš ï¸ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    y_audio = np.zeros(int(duration * SR))\n",
    "\n",
    "# 4. í´ë¦½ë³„ ì¶”ë¡ \n",
    "print(\"\\n[4/5] ğŸ” í´ë¦½ë³„ ì¶”ë¡  ì¤‘...\")\n",
    "\n",
    "results = []\n",
    "current_text_idx = 0\n",
    "start_frame = 0\n",
    "stt_cursor = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    while start_frame + CLIP_LEN <= total_frames:\n",
    "        start_sec = start_frame / fps\n",
    "        end_sec = (start_frame + CLIP_LEN) / fps\n",
    "        \n",
    "        # ----- Gesture -----\n",
    "        frames = []\n",
    "        for i in range(start_frame, start_frame + CLIP_LEN):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, RESIZE_HW)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        if len(frames) < CLIP_LEN:\n",
    "            break\n",
    "        \n",
    "        # í…ì„œ ë³€í™˜\n",
    "        vid_np = np.array(frames, dtype=np.float32) / 255.0\n",
    "        vid_np = np.transpose(vid_np, (3, 0, 1, 2))  # (C, T, H, W)\n",
    "        vid_np = (vid_np - 0.5) / 0.5\n",
    "        vid_tensor = torch.from_numpy(vid_np).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Gesture ì¶”ë¡ \n",
    "        g_logits, g_feat = gesture_model(vid_tensor, return_feature=True)\n",
    "        g_prob = F.softmax(g_logits, dim=1)[0, 1].item()  # ê°•ì¡° í™•ë¥ \n",
    "        \n",
    "        # ----- Audio -----\n",
    "        audio_start = int(start_sec * SR)\n",
    "        audio_end = int(end_sec * SR)\n",
    "        y_clip = y_audio[audio_start:audio_end]\n",
    "        \n",
    "        if len(y_clip) > 0:\n",
    "            mel_feat = extract_mel_spectrogram(y_clip, SR, N_MELS, N_FFT, HOP_LENGTH)\n",
    "            mel_feat = pad_or_truncate(mel_feat, WINDOW_SIZE)\n",
    "            \n",
    "            # ê°„ë‹¨í•œ ìŠ¤ì¼€ì¼ë§\n",
    "            mel_feat = (mel_feat - mel_feat.mean()) / (mel_feat.std() + 1e-6)\n",
    "            \n",
    "            aud_tensor = torch.from_numpy(mel_feat).float().unsqueeze(0).to(device)\n",
    "            a_logits, a_feat = audio_model(aud_tensor, return_feature=True)\n",
    "            \n",
    "            # Audio: 4í´ë˜ìŠ¤ ì¤‘ ë¹„ê°•ì¡°(0)ê°€ ì•„ë‹Œ í™•ë¥  í•©\n",
    "            a_probs = F.softmax(a_logits, dim=1)[0]\n",
    "            a_class = torch.argmax(a_probs).item()\n",
    "            a_prob = 1.0 - a_probs[0].item()  # ê°•ì¡° í™•ë¥  (1 - ë¹„ê°•ì¡°)\n",
    "        else:\n",
    "            a_feat = torch.zeros(1, 128).to(device)\n",
    "            a_prob = 0.0\n",
    "            a_class = 0\n",
    "        \n",
    "        # ----- Text -----\n",
    "        t_feat = torch.zeros(1, 768).to(device)\n",
    "        t_prob = 0.0\n",
    "        matched_text_idx = -1\n",
    "        if len(text_tensors) > 0 and len(stt_segments) > 0:\n",
    "            # í˜„ì¬ í´ë¦½ ì‹œê°„(start_sec)ì´ í¬í•¨ë˜ëŠ” STT ì„¸ê·¸ë¨¼íŠ¸ ì°¾ê¸°\n",
    "            # (stt_segmentsëŠ” ì‹œê°„ìˆœ ì •ë ¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)\n",
    "            for i in range(stt_cursor, len(stt_segments)):\n",
    "                seg = stt_segments[i]\n",
    "                # ì„¸ê·¸ë¨¼íŠ¸: { \"start\": 0.0, \"end\": 2.5, \"text\": \"...\" }\n",
    "                seg_start = seg.get('start', 0)\n",
    "                seg_end = seg.get('end', 0)\n",
    "                \n",
    "                # í´ë¦½ì˜ ì¤‘ê°„ ì§€ì ì´ ìë§‰ êµ¬ê°„ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸\n",
    "                clip_mid = (start_sec + end_sec) / 2\n",
    "                \n",
    "                if seg_start <= clip_mid <= seg_end:\n",
    "                    matched_text_idx = i\n",
    "                    stt_cursor = i  # ë‹¤ìŒ ê²€ìƒ‰ì€ ì—¬ê¸°ì„œë¶€í„°\n",
    "                    break\n",
    "                \n",
    "                if seg_start > end_sec: # ì´ë¯¸ ì§€ë‚˜ê°\n",
    "                    break\n",
    "            \n",
    "            # ë§¤ì¹­ëœ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í•´ë‹¹ í…ì„œ ì‚¬ìš©\n",
    "            if matched_text_idx != -1 and matched_text_idx < len(text_tensors):\n",
    "                t_tensor = text_tensors[matched_text_idx]\n",
    "                \n",
    "                # ì°¨ì› ë§ì¶”ê¸°\n",
    "                if isinstance(t_tensor, torch.Tensor):\n",
    "                    if t_tensor.dim() == 3: t_feat = t_tensor.mean(dim=1).to(device)\n",
    "                    elif t_tensor.dim() == 2: t_feat = t_tensor.mean(dim=0, keepdim=True).to(device)\n",
    "                    else: t_feat = t_tensor.unsqueeze(0).to(device)\n",
    "                \n",
    "                # ì„ì‹œ: í…ìŠ¤íŠ¸ ì ìˆ˜ (ëª¨ë¸ì´ ìˆë‹¤ë©´ ëª¨ë¸ ì¶œë ¥, ì—†ìœ¼ë©´ íœ´ë¦¬ìŠ¤í‹±)\n",
    "                # t_logits, _ = text_model(t_feat, return_feature=True)\n",
    "                # t_prob = F.softmax(t_logits, dim=1)[0, 1].item()\n",
    "                \n",
    "                # í…ì„œì˜ í¬ê¸°(norm)ë¥¼ 'ì¤‘ìš”ë„'ë¡œ ê°€ì •í•˜ëŠ” ì„ì‹œ ë¡œì§ (ì‹¤ì œ ëª¨ë¸ ì‚¬ìš© ì‹œ êµì²´)\n",
    "                t_prob = min(1.0, t_feat.norm().item() / 50.0)\n",
    "        \n",
    "        # ----- Fusion (ê°€ì¤‘ì¹˜ í•©ì‚°) -----\n",
    "        fusion_score = (\n",
    "            WEIGHTS[\"gesture\"] * g_prob +\n",
    "            WEIGHTS[\"audio\"] * a_prob +\n",
    "            WEIGHTS[\"text\"] * t_prob\n",
    "        )\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        results.append({\n",
    "            \"start_sec\": round(start_sec, 2),\n",
    "            \"end_sec\": round(end_sec, 2),\n",
    "            \"gesture_prob\": round(g_prob, 4),\n",
    "            \"audio_prob\": round(a_prob, 4),\n",
    "            \"audio_class\": a_class,\n",
    "            \"text_prob\": round(t_prob, 4),\n",
    "            \"fusion_score\": round(fusion_score, 4),\n",
    "            \"is_emphasis\": fusion_score >= 0.5\n",
    "        })\n",
    "        \n",
    "        start_frame += STRIDE\n",
    "\n",
    "cap.release()\n",
    "print(f\"  âœ… {len(results)}ê°œ í´ë¦½ ë¶„ì„ ì™„ë£Œ\")\n",
    "\n",
    "# 5. ê²°ê³¼ ì €ì¥ ë° í†µê³„\n",
    "print(\"\\n[5/5] ğŸ“Š ê²°ê³¼ ë¶„ì„...\")\n",
    "\n",
    "# í†µê³„ ê³„ì‚°\n",
    "gesture_scores = [r[\"gesture_prob\"] for r in results]\n",
    "audio_scores = [r[\"audio_prob\"] for r in results]\n",
    "text_scores = [r[\"text_prob\"] for r in results]\n",
    "fusion_scores = [r[\"fusion_score\"] for r in results]\n",
    "emphasis_count = sum(1 for r in results if r[\"is_emphasis\"])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š ì „ì²´ í†µê³„\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  ì´ í´ë¦½ ìˆ˜: {len(results)}\")\n",
    "print(f\"  ê°•ì¡° í´ë¦½ ìˆ˜: {emphasis_count} ({emphasis_count/len(results)*100:.1f}%)\")\n",
    "print(f\"\\n  [Gesture] í‰ê· : {np.mean(gesture_scores):.4f}, ìµœëŒ€: {np.max(gesture_scores):.4f}\")\n",
    "print(f\"  [Audio]   í‰ê· : {np.mean(audio_scores):.4f}, ìµœëŒ€: {np.max(audio_scores):.4f}\")\n",
    "print(f\"  [Text]    í‰ê· : {np.mean(text_scores):.4f}, ìµœëŒ€: {np.max(text_scores):.4f}\")\n",
    "print(f\"  [Fusion]  í‰ê· : {np.mean(fusion_scores):.4f}, ìµœëŒ€: {np.max(fusion_scores):.4f}\")\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ì •ë³´\n",
    "print(f\"\\n  ê°€ì¤‘ì¹˜: Gesture={WEIGHTS['gesture']}, Audio={WEIGHTS['audio']}, Text={WEIGHTS['text']}\")\n",
    "\n",
    "# ì´ì  ê³„ì‚° (ì „ì²´ í‰ê· )\n",
    "total_score = np.mean(fusion_scores) * 100\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ† ì˜ìƒ ì´ì : {total_score:.2f} / 100\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# ê°•ì¡° êµ¬ê°„ ìš”ì•½\n",
    "print(f\"\\nğŸ“‹ ê°•ì¡° êµ¬ê°„ (fusion >= 0.5):\")\n",
    "emphasis_segments = []\n",
    "in_emphasis = False\n",
    "seg_start = 0\n",
    "\n",
    "for r in results:\n",
    "    if r[\"is_emphasis\"] and not in_emphasis:\n",
    "        seg_start = r[\"start_sec\"]\n",
    "        in_emphasis = True\n",
    "    elif not r[\"is_emphasis\"] and in_emphasis:\n",
    "        emphasis_segments.append((seg_start, r[\"start_sec\"]))\n",
    "        in_emphasis = False\n",
    "\n",
    "if in_emphasis:\n",
    "    emphasis_segments.append((seg_start, results[-1][\"end_sec\"]))\n",
    "\n",
    "if emphasis_segments:\n",
    "    for i, (s, e) in enumerate(emphasis_segments[:10]):  # ìµœëŒ€ 10ê°œ\n",
    "        print(f\"  {i+1}. {s:.2f}s ~ {e:.2f}s (ê¸¸ì´: {e-s:.2f}s)\")\n",
    "    if len(emphasis_segments) > 10:\n",
    "        print(f\"  ... ì™¸ {len(emphasis_segments) - 10}ê°œ\")\n",
    "else:\n",
    "    print(\"  (ê°•ì¡° êµ¬ê°„ ì—†ìŒ)\")\n",
    "\n",
    "# JSON ì €ì¥\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "\n",
    "output_data = {\n",
    "    \"video_path\": VIDEO_PATH,\n",
    "    \"duration_sec\": round(duration, 2),\n",
    "    \"total_clips\": len(results),\n",
    "    \"emphasis_clips\": emphasis_count,\n",
    "    \"weights\": WEIGHTS,\n",
    "    \"statistics\": {\n",
    "        \"gesture_mean\": round(np.mean(gesture_scores), 4),\n",
    "        \"audio_mean\": round(np.mean(audio_scores), 4),\n",
    "        \"text_mean\": round(np.mean(text_scores), 4),\n",
    "        \"fusion_mean\": round(np.mean(fusion_scores), 4),\n",
    "    },\n",
    "    \"total_score\": round(total_score, 2),\n",
    "    \"emphasis_segments\": [{\"start_sec\": s, \"end_sec\": e} for s, e in emphasis_segments],\n",
    "    \"clips\": results\n",
    "}\n",
    "\n",
    "with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nâœ… ê²°ê³¼ ì €ì¥: {OUTPUT_JSON}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¬ Simple MLP Fusion ì¶”ë¡  (STT ì—°ë™ ë²„ì „)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# â˜… [í•µì‹¬ ì¶”ê°€] STT JSON ë¡œë“œ\n",
    "# -------------------------------------------------------------\n",
    "stt_segments = []\n",
    "if os.path.exists(STT_JSON_PATH):\n",
    "    with open(STT_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        stt_data = json.load(f)\n",
    "        # êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ (ì˜ˆ: data['segments'] ë˜ëŠ” ë°”ë¡œ ë¦¬ìŠ¤íŠ¸)\n",
    "        if isinstance(stt_data, dict) and 'segments' in stt_data:\n",
    "            stt_segments = stt_data['segments']\n",
    "        elif isinstance(stt_data, list):\n",
    "            stt_segments = stt_data\n",
    "        \n",
    "    print(f\"  âœ… STT ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(stt_segments)}ê°œ ë¬¸ì¥\")\n",
    "else:\n",
    "    print(f\"  âš ï¸ STT íŒŒì¼ ì—†ìŒ: {STT_JSON_PATH} (í…ìŠ¤íŠ¸ ë§¤ì¹­ ë¶ˆê°€ëŠ¥)\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# =============================================================================\n",
    "# ğŸ“‹ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“‹ í´ë¦½ë³„ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Start':>7} {'End':>7} {'Gesture':>8} {'Audio':>8} {'Text':>8} {'Fusion':>8} {'ê°•ì¡°':>4}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for r in results[:10]:\n",
    "    emph = \"âœ“\" if r[\"is_emphasis\"] else \"\"\n",
    "    print(f\"{r['start_sec']:>7.2f} {r['end_sec']:>7.2f} \"\n",
    "          f\"{r['gesture_prob']:>8.4f} {r['audio_prob']:>8.4f} \"\n",
    "          f\"{r['text_prob']:>8.4f} {r['fusion_score']:>8.4f} {emph:>4}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ ì¶”ë¡  ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc13ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ Device: cuda\n",
      "\n",
      "======================================================================\n",
      "ğŸ¬ ë©€í‹°ëª¨ë‹¬ ë°œí‘œ ë¶„ì„ ì‹œìŠ¤í…œ\n",
      "======================================================================\n",
      "\n",
      "[1/6] ğŸ“¦ ëª¨ë¸ ë¡œë“œ...\n",
      "  âœ… Gesture Model ë¡œë“œ\n",
      "  âœ… Audio Model ë¡œë“œ\n",
      "  âœ… Text Tensors ë¡œë“œ: 46ê°œ\n",
      "\n",
      "[2/6] ğŸ“ STT ë°ì´í„° ë¡œë“œ...\n",
      "âš ï¸ STT íŒŒì¼ ì—†ìŒ: /home/stu/ai_project/ê²°ê³¼/text.json\n",
      "  âœ… 0ê°œ ë¬¸ì¥ ë¡œë“œ\n",
      "\n",
      "[3/6] ğŸ¥ ë¹„ë””ì˜¤ ë¶„ì„...\n",
      "  ì˜ìƒ: test_jo.mp4\n",
      "  ê¸¸ì´: 142.41ì´ˆ, FPS: 29.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2537931/1807707730.py:510: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y_audio, _ = librosa.load(VIDEO_PATH, sr=SR)\n",
      "/home/stu/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì˜¤ë””ì˜¤: 142.52ì´ˆ\n",
      "\n",
      "[4/6] ğŸ” ë¬¸ì¥ë³„ ë¶„ì„ ì¤‘...\n",
      "  âœ… 0ê°œ ë¬¸ì¥ ë¶„ì„ ì™„ë£Œ\n",
      "\n",
      "[5/6] ğŸ¤– í”¼ë“œë°± ìƒì„±...\n",
      "  ê°•ì¡° êµ¬ê°„: 0ê°œ\n",
      "  âš ï¸ LLM API ì‚¬ìš© ë¶ˆê°€, ë¡œì»¬ í”¼ë“œë°± ìƒì„±...\n",
      "  âœ… í”¼ë“œë°± ìƒì„± ì™„ë£Œ\n",
      "\n",
      "[6/6] ğŸ¥ ì‹œê°í™” ì˜ìƒ ìƒì„±...\n",
      "  ğŸ¥ ì˜ìƒ ìƒì„± ì¤‘... (4268 frames)\n",
      "    ì§„í–‰ë¥ : 11.7%\n",
      "    ì§„í–‰ë¥ : 23.4%\n",
      "    ì§„í–‰ë¥ : 35.1%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 727\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m full_result\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 727\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 666\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# 6. ì‹œê°í™” ì˜ìƒ ìƒì„±\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[6/6] ğŸ¥ ì‹œê°í™” ì˜ìƒ ìƒì„±...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 666\u001b[0m \u001b[43mcreate_visualization_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVIDEO_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_emphasis_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_VIDEO\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ“ ê²°ê³¼ ì €ì¥...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 437\u001b[0m, in \u001b[0;36mcreate_visualization_video\u001b[0;34m(video_path, emphasis_data, output_path)\u001b[0m\n\u001b[1;32m    434\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n\u001b[1;32m    435\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39maddWeighted(overlay, alpha, frame, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 437\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m frame_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# ì§„í–‰ë¥  í‘œì‹œ\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # =============================================================================\n",
    "# # ğŸ¬ ë©€í‹°ëª¨ë‹¬ ë°œí‘œ ë¶„ì„ + LLM í”¼ë“œë°± + ì‹œê°í™” ì˜ìƒ ìƒì„±\n",
    "# # =============================================================================\n",
    "# #\n",
    "# # íŒŒì´í”„ë¼ì¸:\n",
    "# # 1. STT JSON ë¡œë“œ (ë¬¸ì¥ + ì‹œê°„ ì •ë³´)\n",
    "# # 2. ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (Gesture + Audio + Text)\n",
    "# # 3. ê°•ì¡° êµ¬ê°„ ì¶”ì¶œ + LLM í”¼ë“œë°± ìƒì„±\n",
    "# # 4. ì‹œê°í™” ì˜ìƒ ìƒì„±\n",
    "# #\n",
    "# # =============================================================================\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import librosa\n",
    "# from datetime import datetime\n",
    "\n",
    "# # =============================================================================\n",
    "# # âš™ï¸ ì„¤ì •\n",
    "# # =============================================================================\n",
    "# # ê²½ë¡œ ì„¤ì •\n",
    "# VIDEO_PATH = \"/home/stu/ai_project/ì˜ìƒraw/test_jo.mp4\"\n",
    "# MODEL_DIR = \"/home/stu/ai_project/ëª¨ë¸ì§‘í•©\"\n",
    "# STT_JSON_PATH = \"/home/stu/ai_project/ê²°ê³¼/text.json\"\n",
    "\n",
    "# GESTURE_MODEL_PATH = os.path.join(MODEL_DIR, \"gesture_model.pt\")\n",
    "# AUDIO_MODEL_PATH = os.path.join(MODEL_DIR, \"voice_model.pth\")\n",
    "# TEXT_TENSOR_PATH = os.path.join(MODEL_DIR, \"text_jo_torch_text.pt\")\n",
    "\n",
    "# # ì¶œë ¥ ê²½ë¡œ\n",
    "# OUTPUT_DIR = \"/home/stu/ai_project/ê²°ê³¼\"\n",
    "# OUTPUT_JSON = os.path.join(OUTPUT_DIR, \"test_jo_analysis_result.json\")\n",
    "# OUTPUT_VIDEO = os.path.join(OUTPUT_DIR, \"test_jo_visualization.mp4\")\n",
    "# OUTPUT_FEEDBACK = os.path.join(OUTPUT_DIR, \"test_jo_llm_feedback.json\")\n",
    "\n",
    "# # ê°€ì¤‘ì¹˜ ì„¤ì • (â­ ì¡°ì ˆ ê°€ëŠ¥!)\n",
    "# WEIGHTS = {\n",
    "#     \"gesture\": 0.4,\n",
    "#     \"audio\": 0.35,\n",
    "#     \"text\": 0.25,\n",
    "# }\n",
    "\n",
    "# # ê°•ì¡° ì„ê³„ê°’\n",
    "# EMPHASIS_THRESHOLD = 0.3\n",
    "\n",
    "# # Gesture ì„¤ì •\n",
    "# CLIP_LEN = 16\n",
    "# STRIDE = 8\n",
    "# RESIZE_HW = (112, 112)\n",
    "\n",
    "# # Audio ì„¤ì • (Mel-Spectrogram)\n",
    "# SR = 16000\n",
    "# N_FFT = 1024\n",
    "# HOP_LENGTH = 512\n",
    "# N_MELS = 80\n",
    "# WINDOW_SIZE = 50\n",
    "\n",
    "# # LLM API ì„¤ì • (â­ ë³¸ì¸ API í‚¤ë¡œ ë³€ê²½!)\n",
    "# # OpenAI ë˜ëŠ” Claude API ì‚¬ìš©\n",
    "# USE_OPENAI = True  # Falseë©´ Claude ì‚¬ìš©\n",
    "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
    "# ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key-here\")\n",
    "\n",
    "# # ë””ë°”ì´ìŠ¤\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"ğŸ–¥ï¸ Device: {device}\")\n",
    "\n",
    "# # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ—ï¸ ëª¨ë¸ ì •ì˜\n",
    "# # =============================================================================\n",
    "# from torchvision.models.video import r3d_18\n",
    "\n",
    "# class GestureModel(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(GestureModel, self).__init__()\n",
    "#         self.backbone = r3d_18(weights=None)\n",
    "#         in_features = self.backbone.fc.in_features\n",
    "#         self.backbone.fc = nn.Identity()\n",
    "#         self.classifier = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "#     def forward(self, x, return_feature=False):\n",
    "#         features = self.backbone(x)\n",
    "#         logits = self.classifier(features)\n",
    "#         if return_feature:\n",
    "#             return logits, features\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# class AudioBiLSTM(nn.Module):\n",
    "#     def __init__(self, input_dim=80, hidden_dim=64, num_layers=2, \n",
    "#                  num_classes=4, dropout=0.2):\n",
    "#         super(AudioBiLSTM, self).__init__()\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size=input_dim, hidden_size=hidden_dim,\n",
    "#             num_layers=num_layers, batch_first=True,\n",
    "#             dropout=dropout if num_layers > 1 else 0, bidirectional=True\n",
    "#         )\n",
    "#         self.feature_dim = hidden_dim * 2\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(self.feature_dim, 64), nn.ReLU(),\n",
    "#             nn.Dropout(dropout), nn.Linear(64, num_classes)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x, return_feature=False):\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         features = lstm_out[:, -1, :]\n",
    "#         logits = self.classifier(features)\n",
    "#         if return_feature:\n",
    "#             return logits, features\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "# # =============================================================================\n",
    "# def extract_mel_spectrogram(y_audio, sr=16000, n_mels=80, n_fft=1024, hop_length=512):\n",
    "#     \"\"\"Mel-Spectrogram ì¶”ì¶œ\"\"\"\n",
    "#     y_audio = librosa.util.normalize(y_audio)\n",
    "#     mel = librosa.feature.melspectrogram(\n",
    "#         y=y_audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length\n",
    "#     )\n",
    "#     mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "#     return mel_db.T\n",
    "\n",
    "\n",
    "# def pad_or_truncate(features, window_size):\n",
    "#     \"\"\"ìœˆë„ìš° í¬ê¸°ì— ë§ê²Œ íŒ¨ë”©/ìë¥´ê¸°\"\"\"\n",
    "#     if len(features) < window_size:\n",
    "#         pad_len = window_size - len(features)\n",
    "#         features = np.pad(features, ((0, pad_len), (0, 0)), mode='constant')\n",
    "#     else:\n",
    "#         features = features[:window_size]\n",
    "#     return features\n",
    "\n",
    "\n",
    "# def load_stt_json(path):\n",
    "#     \"\"\"STT JSON ë¡œë“œ (ë”•ì…”ë„ˆë¦¬ ì•ˆì— ë¦¬ìŠ¤íŠ¸ ë“¤ì–´ìˆëŠ” êµ¬ì¡°ê¹Œì§€ ìë™ ì²˜ë¦¬)\"\"\"\n",
    "#     if not os.path.exists(path):\n",
    "#         print(f\"âš ï¸ STT íŒŒì¼ ì—†ìŒ: {path}\")\n",
    "#         return []\n",
    "    \n",
    "#     with open(path, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     # 1) ìµœìƒë‹¨ì´ ë°”ë¡œ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°\n",
    "#     if isinstance(data, list):\n",
    "#         print(f\"âœ… STT: top-level list, count={len(data)}\")\n",
    "#         return data\n",
    "\n",
    "#     # 2) ìµœìƒë‹¨ì´ dictì¸ ê²½ìš° â†’ ì•ˆì— ë“¤ì–´ìˆëŠ” listë¥¼ ì°¾ì•„ì„œ ì‚¬ìš©\n",
    "#     if isinstance(data, dict):\n",
    "#         list_keys = [k for k, v in data.items() if isinstance(v, list)]\n",
    "\n",
    "#         if not list_keys:\n",
    "#             print(f\"âš ï¸ STT JSON ì•ˆì— ë¦¬ìŠ¤íŠ¸ íƒ€ì…ì´ ì—†ìŒ. keys={list(data.keys())}\")\n",
    "#             return []\n",
    "\n",
    "#         # ë¦¬ìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ ê°œë©´ ì²« ë²ˆì§¸ë¥¼ ì‚¬ìš© (í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ ì›í•˜ëŠ” í‚¤ ê³¨ë¼ì„œ ì“°ë©´ ë¨)\n",
    "#         key = list_keys[0]\n",
    "#         segments = data[key]\n",
    "#         print(f\"âœ… STT: data['{key}'] ì‚¬ìš©, count={len(segments)}\")\n",
    "#         return segments\n",
    "\n",
    "#     print(f\"âš ï¸ STT JSON í˜•ì‹ ì¸ì‹ ë¶ˆê°€: {type(data)}\")\n",
    "#     return []\n",
    "\n",
    "\n",
    "# def find_key_word(text):\n",
    "#     \"\"\"ë¬¸ì¥ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)\"\"\"\n",
    "#     # ëª…ì‚¬/ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)\n",
    "#     # ì‹¤ì œë¡œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš© ê¶Œì¥\n",
    "#     words = text.replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").split()\n",
    "    \n",
    "#     # 2ê¸€ì ì´ìƒ, ì¡°ì‚¬ ì œì™¸\n",
    "#     stopwords = [\"ì´\", \"ê°€\", \"ì„\", \"ë¥¼\", \"ì€\", \"ëŠ”\", \"ì—\", \"ì—ì„œ\", \"ìœ¼ë¡œ\", \"ë¡œ\", \"ì™€\", \"ê³¼\", \n",
    "#                  \"ì˜\", \"ë„\", \"ë§Œ\", \"ê¹Œì§€\", \"ë¶€í„°\", \"ì´ëŸ°\", \"ê·¸ëŸ°\", \"ì €ëŸ°\", \"í•˜ëŠ”\", \"ìˆëŠ”\"]\n",
    "    \n",
    "#     keywords = [w for w in words if len(w) >= 2 and w not in stopwords]\n",
    "    \n",
    "#     # ë§ˆì§€ë§‰ ì£¼ìš” ë‹¨ì–´ ë°˜í™˜ (ë¬¸ì¥ ëì— í•µì‹¬ì´ ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŒ)\n",
    "#     if keywords:\n",
    "#         return keywords[-1]\n",
    "#     return words[-1] if words else \"\"\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ¤– LLM í”¼ë“œë°± ìƒì„±\n",
    "# # =============================================================================\n",
    "# def generate_llm_feedback_openai(emphasis_segments, video_duration):\n",
    "#     \"\"\"OpenAI APIë¡œ í”¼ë“œë°± ìƒì„±\"\"\"\n",
    "#     try:\n",
    "#         import openai\n",
    "#         openai.api_key = OPENAI_API_KEY\n",
    "        \n",
    "#         # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "#         segments_text = \"\\n\".join([\n",
    "#             f\"- [{s['start_sec']:.1f}s ~ {s['end_sec']:.1f}s] \"\n",
    "#             f\"(ê°•ì¡°ì ìˆ˜: {s['emphasis_score']:.2f}) \\\"{s['text']}\\\"\"\n",
    "#             for s in emphasis_segments\n",
    "#         ])\n",
    "        \n",
    "#         prompt = f\"\"\"ë‹¹ì‹ ì€ í”„ë ˆì  í…Œì´ì…˜ ì½”ì¹­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "\n",
    "# ë‹¤ìŒì€ ë°œí‘œ ì˜ìƒì—ì„œ ê°•ì¡°ê°€ í•„ìš”í•œ êµ¬ê°„ë“¤ì…ë‹ˆë‹¤:\n",
    "# ì˜ìƒ ê¸¸ì´: {video_duration:.1f}ì´ˆ\n",
    "\n",
    "# ê°•ì¡° êµ¬ê°„:\n",
    "# {segments_text}\n",
    "\n",
    "# ê° êµ¬ê°„ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:\n",
    "# 1. ì™œ ì´ ë¶€ë¶„ì´ ê°•ì¡°ë˜ì–´ì•¼ í•˜ëŠ”ì§€ (reason)\n",
    "# 2. ì–´ë–¤ í‚¤ì›Œë“œê°€ í•µì‹¬ì¸ì§€ (key_word)\n",
    "# 3. ê°œì„ ì„ ìœ„í•œ ì¡°ì–¸\n",
    "\n",
    "# JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n",
    "# [\n",
    "#   {{\"index\": 1, \"reason\": \"ì´ìœ \", \"key_word\": \"í‚¤ì›Œë“œ\", \"advice\": \"ì¡°ì–¸\"}},\n",
    "#   ...\n",
    "# ]\n",
    "# \"\"\"\n",
    "        \n",
    "#         response = openai.ChatCompletion.create(\n",
    "#             model=\"gpt-4\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#             temperature=0.7,\n",
    "#             max_tokens=2000\n",
    "#         )\n",
    "        \n",
    "#         result = response.choices[0].message.content\n",
    "        \n",
    "#         # JSON íŒŒì‹± ì‹œë„\n",
    "#         try:\n",
    "#             # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "#             import re\n",
    "#             json_match = re.search(r'\\[.*\\]', result, re.DOTALL)\n",
    "#             if json_match:\n",
    "#                 return json.loads(json_match.group())\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#         return result\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ OpenAI API ì—ëŸ¬: {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "# def generate_llm_feedback_anthropic(emphasis_segments, video_duration):\n",
    "#     \"\"\"Anthropic Claude APIë¡œ í”¼ë“œë°± ìƒì„±\"\"\"\n",
    "#     try:\n",
    "#         import anthropic\n",
    "#         client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "        \n",
    "#         segments_text = \"\\n\".join([\n",
    "#             f\"- [{s['start_sec']:.1f}s ~ {s['end_sec']:.1f}s] \"\n",
    "#             f\"(ê°•ì¡°ì ìˆ˜: {s['emphasis_score']:.2f}) \\\"{s['text']}\\\"\"\n",
    "#             for s in emphasis_segments\n",
    "#         ])\n",
    "        \n",
    "#         prompt = f\"\"\"ë‹¹ì‹ ì€ í”„ë ˆì  í…Œì´ì…˜ ì½”ì¹­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "\n",
    "# ë‹¤ìŒì€ ë°œí‘œ ì˜ìƒì—ì„œ ê°•ì¡°ê°€ í•„ìš”í•œ êµ¬ê°„ë“¤ì…ë‹ˆë‹¤:\n",
    "# ì˜ìƒ ê¸¸ì´: {video_duration:.1f}ì´ˆ\n",
    "\n",
    "# ê°•ì¡° êµ¬ê°„:\n",
    "# {segments_text}\n",
    "\n",
    "# ê° êµ¬ê°„ì— ëŒ€í•´ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:\n",
    "# [\n",
    "#   {{\"index\": 1, \"reason\": \"ì™œ ê°•ì¡°í•´ì•¼ í•˜ëŠ”ì§€\", \"key_word\": \"í•µì‹¬ í‚¤ì›Œë“œ\", \"advice\": \"ê°œì„  ì¡°ì–¸\"}},\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "# JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.\"\"\"\n",
    "\n",
    "#         response = client.messages.create(\n",
    "#             model=\"claude-sonnet-4-20250514\",\n",
    "#             max_tokens=2000,\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#         )\n",
    "        \n",
    "#         result = response.content[0].text\n",
    "        \n",
    "#         try:\n",
    "#             import re\n",
    "#             json_match = re.search(r'\\[.*\\]', result, re.DOTALL)\n",
    "#             if json_match:\n",
    "#                 return json.loads(json_match.group())\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#         return result\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ Anthropic API ì—ëŸ¬: {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "# def generate_feedback_local(emphasis_segments):\n",
    "#     \"\"\"LLM ì—†ì´ ë¡œì»¬ì—ì„œ í”¼ë“œë°± ìƒì„± (ê·œì¹™ ê¸°ë°˜)\"\"\"\n",
    "#     feedback = []\n",
    "    \n",
    "#     reason_templates = [\n",
    "#         \"ì£¼ì œ ë„ì… ë¬¸ì¥ìœ¼ë¡œ ì²­ì¤‘ì˜ ê³µê° ìœ ë„\",\n",
    "#         \"í•µì‹¬ ê°œë… ì„¤ëª…ìœ¼ë¡œ ì¤‘ìš”ë„ ë†’ìŒ\",\n",
    "#         \"ê²°ë¡  ë˜ëŠ” ìš”ì•½ ë¶€ë¶„ìœ¼ë¡œ ê°•ì¡° í•„ìš”\",\n",
    "#         \"ì§ˆë¬¸ í˜•ì‹ìœ¼ë¡œ ì²­ì¤‘ ì°¸ì—¬ ìœ ë„\",\n",
    "#         \"êµ¬ì²´ì  ì˜ˆì‹œë¡œ ì´í•´ë„ í–¥ìƒ\",\n",
    "#         \"ì „í™˜ êµ¬ê°„ìœ¼ë¡œ íë¦„ ê°•ì¡°\",\n",
    "#         \"í•µì‹¬ ì£¼ì¥ ì œì‹œ\",\n",
    "#         \"ê°ì •ì  í˜¸ì†Œë¡œ ì²­ì¤‘ ëª°ì… ìœ ë„\",\n",
    "#     ]\n",
    "    \n",
    "#     for i, seg in enumerate(emphasis_segments):\n",
    "#         text = seg.get(\"text\", \"\")\n",
    "#         score = seg.get(\"emphasis_score\", 0)\n",
    "        \n",
    "#         # í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "#         key_word = find_key_word(text)\n",
    "        \n",
    "#         # ì´ìœ  ì„ íƒ (ë¬¸ì¥ íŠ¹ì„±ì— ë”°ë¼)\n",
    "#         if \"?\" in text:\n",
    "#             reason = \"ì§ˆë¬¸ í˜•ì‹ìœ¼ë¡œ ì²­ì¤‘ ì°¸ì—¬ ìœ ë„ ë° ë¬¸ì œ ì œê¸°\"\n",
    "#         elif any(kw in text for kw in [\"ê·¸ëŸ¬ë‹ˆê¹Œ\", \"ì¦‰\", \"ê²°êµ­\"]):\n",
    "#             reason = \"ìš”ì•½/ì •ë¦¬ êµ¬ê°„ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ê°•ì¡°\"\n",
    "#         elif any(kw in text for kw in [\"ì˜ˆë¥¼ ë“¤ì–´\", \"ì˜ˆì‹œ\", \"ì‚¬ë¡€\"]):\n",
    "#             reason = \"êµ¬ì²´ì  ì˜ˆì‹œë¡œ ì´í•´ë„ í–¥ìƒ\"\n",
    "#         elif score > 0.7:\n",
    "#             reason = \"ë†’ì€ ê°•ì¡° ì ìˆ˜ë¡œ í•µì‹¬ ë©”ì‹œì§€ ì „ë‹¬\"\n",
    "#         else:\n",
    "#             reason = reason_templates[i % len(reason_templates)]\n",
    "        \n",
    "#         feedback.append({\n",
    "#             \"index\": i + 1,\n",
    "#             \"start_sec\": seg[\"start_sec\"],\n",
    "#             \"end_sec\": seg[\"end_sec\"],\n",
    "#             \"text\": text,\n",
    "#             \"emphasis_score\": round(score, 2),\n",
    "#             \"key_word\": key_word,\n",
    "#             \"reason\": reason\n",
    "#         })\n",
    "    \n",
    "#     return feedback\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸ¥ ì‹œê°í™” ì˜ìƒ ìƒì„±\n",
    "# # =============================================================================\n",
    "# def create_visualization_video(video_path, emphasis_data, output_path):\n",
    "#     \"\"\"ê°•ì¡° êµ¬ê°„ ì‹œê°í™” ì˜ìƒ ìƒì„±\"\"\"\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     if not cap.isOpened():\n",
    "#         print(f\"âš ï¸ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}\")\n",
    "#         return False\n",
    "    \n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "#     width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "#     # VideoWriter\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "#     # ê°•ì¡° êµ¬ê°„ì„ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ì¤€ë¹„\n",
    "#     def get_emphasis_info(current_sec):\n",
    "#         for seg in emphasis_data:\n",
    "#             if seg[\"start_sec\"] <= current_sec <= seg[\"end_sec\"]:\n",
    "#                 return seg\n",
    "#         return None\n",
    "    \n",
    "#     print(f\"  ğŸ¥ ì˜ìƒ ìƒì„± ì¤‘... ({total_frames} frames)\")\n",
    "    \n",
    "#     frame_idx = 0\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "        \n",
    "#         current_sec = frame_idx / fps\n",
    "#         seg_info = get_emphasis_info(current_sec)\n",
    "        \n",
    "#         # ì˜¤ë²„ë ˆì´\n",
    "#         overlay = frame.copy()\n",
    "        \n",
    "#         if seg_info:\n",
    "#             # ê°•ì¡° êµ¬ê°„: ë¹¨ê°„ í…Œë‘ë¦¬ + ìƒë‹¨ ë°”\n",
    "#             color = (0, 0, 255)  # BGR - ë¹¨ê°•\n",
    "#             score = seg_info.get(\"emphasis_score\", 0)\n",
    "#             text = seg_info.get(\"text\", \"\")[:50]  # ìµœëŒ€ 50ì\n",
    "#             key_word = seg_info.get(\"key_word\", \"\")\n",
    "            \n",
    "#             # ìƒë‹¨ ë°”\n",
    "#             cv2.rectangle(overlay, (0, 0), (width, 80), color, -1)\n",
    "            \n",
    "#             # í…ìŠ¤íŠ¸\n",
    "#             cv2.putText(overlay, f\"EMPHASIS ({score:.2f})\", (10, 30),\n",
    "#                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "            \n",
    "#             if key_word:\n",
    "#                 cv2.putText(overlay, f\"Key: {key_word}\", (10, 60),\n",
    "#                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "            \n",
    "#             # í…Œë‘ë¦¬\n",
    "#             cv2.rectangle(overlay, (5, 5), (width-5, height-5), color, 4)\n",
    "            \n",
    "#             # í•˜ë‹¨ ìë§‰ ë°”\n",
    "#             cv2.rectangle(overlay, (0, height-60), (width, height), (0, 0, 0), -1)\n",
    "#             # í•œê¸€ í‘œì‹œë¥¼ ìœ„í•´ ì˜ì–´ë¡œ ëŒ€ì²´ (OpenCV í•œê¸€ ì œí•œ)\n",
    "#             cv2.putText(overlay, text[:40], (10, height-25),\n",
    "#                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "#         else:\n",
    "#             # ì¼ë°˜ êµ¬ê°„: ë…¹ìƒ‰ ìƒë‹¨ ë°”\n",
    "#             color = (0, 128, 0)  # BGR - ë…¹ìƒ‰\n",
    "#             cv2.rectangle(overlay, (0, 0), (width, 40), color, -1)\n",
    "#             cv2.putText(overlay, \"NORMAL\", (10, 28),\n",
    "#                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "#         # íƒ€ì„ì½”ë“œ\n",
    "#         time_str = f\"{int(current_sec//60):02d}:{int(current_sec%60):02d}\"\n",
    "#         cv2.putText(overlay, time_str, (width-80, 28),\n",
    "#                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "#         # ë¸”ë Œë”©\n",
    "#         alpha = 0.7\n",
    "#         frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "        \n",
    "#         out.write(frame)\n",
    "#         frame_idx += 1\n",
    "        \n",
    "#         # ì§„í–‰ë¥  í‘œì‹œ\n",
    "#         if frame_idx % 500 == 0:\n",
    "#             progress = frame_idx / total_frames * 100\n",
    "#             print(f\"    ì§„í–‰ë¥ : {progress:.1f}%\")\n",
    "    \n",
    "#     cap.release()\n",
    "#     out.release()\n",
    "    \n",
    "#     print(f\"  âœ… ì˜ìƒ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "#     return True\n",
    "\n",
    "\n",
    "# # =============================================================================\n",
    "# # ğŸš€ ë©”ì¸ ì‹¤í–‰\n",
    "# # =============================================================================\n",
    "# def main():\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"ğŸ¬ ë©€í‹°ëª¨ë‹¬ ë°œí‘œ ë¶„ì„ ì‹œìŠ¤í…œ\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     # 1. ëª¨ë¸ ë¡œë“œ\n",
    "#     print(\"\\n[1/6] ğŸ“¦ ëª¨ë¸ ë¡œë“œ...\")\n",
    "    \n",
    "#     # Gesture Model\n",
    "#     gesture_model = GestureModel(num_classes=2).to(device)\n",
    "#     if os.path.exists(GESTURE_MODEL_PATH):\n",
    "#         state = torch.load(GESTURE_MODEL_PATH, map_location=device)\n",
    "#         new_state = {}\n",
    "#         for k, v in state.items():\n",
    "#             if 'fc' in k and 'backbone' not in k:\n",
    "#                 new_state[k.replace('fc', 'classifier')] = v\n",
    "#             else:\n",
    "#                 new_state[k] = v\n",
    "#         gesture_model.load_state_dict(new_state, strict=False)\n",
    "#         print(f\"  âœ… Gesture Model ë¡œë“œ\")\n",
    "#     gesture_model.eval()\n",
    "    \n",
    "#     # Audio Model\n",
    "#     audio_model = AudioBiLSTM(\n",
    "#         input_dim=N_MELS, hidden_dim=64, num_layers=2, num_classes=4\n",
    "#     ).to(device)\n",
    "#     if os.path.exists(AUDIO_MODEL_PATH):\n",
    "#         audio_model.load_state_dict(\n",
    "#             torch.load(AUDIO_MODEL_PATH, map_location=device), strict=False\n",
    "#         )\n",
    "#         print(f\"  âœ… Audio Model ë¡œë“œ\")\n",
    "#     audio_model.eval()\n",
    "    \n",
    "#     # Text Tensors\n",
    "#     text_tensors = []\n",
    "#     if os.path.exists(TEXT_TENSOR_PATH):\n",
    "#         text_tensors = torch.load(TEXT_TENSOR_PATH, map_location=device, weights_only=False)\n",
    "#         print(f\"  âœ… Text Tensors ë¡œë“œ: {len(text_tensors)}ê°œ\")\n",
    "    \n",
    "#     # 2. STT ë¡œë“œ\n",
    "#     print(\"\\n[2/6] ğŸ“ STT ë°ì´í„° ë¡œë“œ...\")\n",
    "#     stt_segments = load_stt_json(STT_JSON_PATH)\n",
    "#     print(f\"  âœ… {len(stt_segments)}ê°œ ë¬¸ì¥ ë¡œë“œ\")\n",
    "    \n",
    "#     # 3. ë¹„ë””ì˜¤ ì •ë³´\n",
    "#     print(\"\\n[3/6] ğŸ¥ ë¹„ë””ì˜¤ ë¶„ì„...\")\n",
    "#     cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     duration = total_frames / fps\n",
    "#     print(f\"  ì˜ìƒ: {os.path.basename(VIDEO_PATH)}\")\n",
    "#     print(f\"  ê¸¸ì´: {duration:.2f}ì´ˆ, FPS: {fps:.2f}\")\n",
    "    \n",
    "#     # ì˜¤ë””ì˜¤ ë¡œë“œ\n",
    "#     try:\n",
    "#         y_audio, _ = librosa.load(VIDEO_PATH, sr=SR)\n",
    "#         print(f\"  ì˜¤ë””ì˜¤: {len(y_audio)/SR:.2f}ì´ˆ\")\n",
    "#     except:\n",
    "#         y_audio = np.zeros(int(duration * SR))\n",
    "#         print(f\"  âš ï¸ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨, ë”ë¯¸ ì‚¬ìš©\")\n",
    "    \n",
    "#     # 4. ë¬¸ì¥ë³„ ë©€í‹°ëª¨ë‹¬ ë¶„ì„\n",
    "#     print(\"\\n[4/6] ğŸ” ë¬¸ì¥ë³„ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "#     analysis_results = []\n",
    "    \n",
    "#     for idx, stt_seg in enumerate(stt_segments):\n",
    "#         # STT í˜•ì‹ì— ë”°ë¼ í‚¤ ì´ë¦„ ì¡°ì •\n",
    "#         start_sec = stt_seg.get(\"start\", stt_seg.get(\"start_sec\", 0))\n",
    "#         end_sec = stt_seg.get(\"end\", stt_seg.get(\"end_sec\", 0))\n",
    "#         text = stt_seg.get(\"text\", stt_seg.get(\"corrected\", \"\"))\n",
    "        \n",
    "#         if end_sec <= start_sec:\n",
    "#             continue\n",
    "        \n",
    "#         # ----- Gesture ë¶„ì„ -----\n",
    "#         start_frame = int(start_sec * fps)\n",
    "#         end_frame = int(end_sec * fps)\n",
    "        \n",
    "#         gesture_probs = []\n",
    "#         for f_start in range(start_frame, max(start_frame + 1, end_frame - CLIP_LEN), STRIDE):\n",
    "#             frames = []\n",
    "#             for i in range(f_start, min(f_start + CLIP_LEN, end_frame)):\n",
    "#                 cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "#                 ret, frame = cap.read()\n",
    "#                 if ret:\n",
    "#                     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#                     frame = cv2.resize(frame, RESIZE_HW)\n",
    "#                     frames.append(frame)\n",
    "            \n",
    "#             if len(frames) >= CLIP_LEN // 2:\n",
    "#                 # íŒ¨ë”©\n",
    "#                 while len(frames) < CLIP_LEN:\n",
    "#                     frames.append(frames[-1])\n",
    "                \n",
    "#                 vid_np = np.array(frames[:CLIP_LEN], dtype=np.float32) / 255.0\n",
    "#                 vid_np = np.transpose(vid_np, (3, 0, 1, 2))\n",
    "#                 vid_np = (vid_np - 0.5) / 0.5\n",
    "#                 vid_tensor = torch.from_numpy(vid_np).unsqueeze(0).to(device)\n",
    "                \n",
    "#                 with torch.no_grad():\n",
    "#                     g_logits = gesture_model(vid_tensor)\n",
    "#                     g_prob = F.softmax(g_logits, dim=1)[0, 1].item()\n",
    "#                     gesture_probs.append(g_prob)\n",
    "        \n",
    "#         gesture_score = np.mean(gesture_probs) if gesture_probs else 0.0\n",
    "        \n",
    "#         # ----- Audio ë¶„ì„ -----\n",
    "#         audio_start = int(start_sec * SR)\n",
    "#         audio_end = int(end_sec * SR)\n",
    "#         y_clip = y_audio[audio_start:audio_end]\n",
    "        \n",
    "#         if len(y_clip) > 0:\n",
    "#             mel_feat = extract_mel_spectrogram(y_clip, SR, N_MELS, N_FFT, HOP_LENGTH)\n",
    "#             mel_feat = pad_or_truncate(mel_feat, WINDOW_SIZE)\n",
    "#             mel_feat = (mel_feat - mel_feat.mean()) / (mel_feat.std() + 1e-6)\n",
    "            \n",
    "#             aud_tensor = torch.from_numpy(mel_feat).float().unsqueeze(0).to(device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 a_logits = audio_model(aud_tensor)\n",
    "#                 a_probs = F.softmax(a_logits, dim=1)[0]\n",
    "#                 audio_score = 1.0 - a_probs[0].item()  # ë¹„ê°•ì¡° ì œì™¸\n",
    "#         else:\n",
    "#             audio_score = 0.0\n",
    "        \n",
    "#         # ----- Text ë¶„ì„ -----\n",
    "#         text_score = 0.0\n",
    "#         if idx < len(text_tensors):\n",
    "#             t_tensor = text_tensors[idx]\n",
    "#             if isinstance(t_tensor, torch.Tensor):\n",
    "#                 # í…ì„œ norm ê¸°ë°˜ ì ìˆ˜\n",
    "#                 if t_tensor.dim() >= 2:\n",
    "#                     t_feat = t_tensor.mean(dim=-2) if t_tensor.dim() == 3 else t_tensor\n",
    "#                 else:\n",
    "#                     t_feat = t_tensor\n",
    "#                 text_score = min(1.0, t_feat.norm().item() / 50.0)\n",
    "        \n",
    "#         # ----- Fusion -----\n",
    "#         fusion_score = (\n",
    "#             WEIGHTS[\"gesture\"] * gesture_score +\n",
    "#             WEIGHTS[\"audio\"] * audio_score +\n",
    "#             WEIGHTS[\"text\"] * text_score\n",
    "#         )\n",
    "        \n",
    "#         analysis_results.append({\n",
    "#             \"index\": idx + 1,\n",
    "#             \"start_sec\": round(start_sec, 2),\n",
    "#             \"end_sec\": round(end_sec, 2),\n",
    "#             \"text\": text,\n",
    "#             \"gesture_score\": round(gesture_score, 4),\n",
    "#             \"audio_score\": round(audio_score, 4),\n",
    "#             \"text_score\": round(text_score, 4),\n",
    "#             \"emphasis_score\": round(fusion_score, 4),\n",
    "#             \"is_emphasis\": bool(fusion_score >= EMPHASIS_THRESHOLD)\n",
    "#         })\n",
    "#         start_frame += STRIDE\n",
    "#         # ì§„í–‰ë¥ \n",
    "#         if (idx + 1) % 5 == 0:\n",
    "#             print(f\"  ì§„í–‰: {idx + 1}/{len(stt_segments)}\")\n",
    "    \n",
    "#     cap.release()\n",
    "#     print(f\"  âœ… {len(analysis_results)}ê°œ ë¬¸ì¥ ë¶„ì„ ì™„ë£Œ\")\n",
    "    \n",
    "#     # 5. ê°•ì¡° êµ¬ê°„ ì¶”ì¶œ + í”¼ë“œë°± ìƒì„±\n",
    "#     print(\"\\n[5/6] ğŸ¤– í”¼ë“œë°± ìƒì„±...\")\n",
    "    \n",
    "#     emphasis_segments = [r for r in analysis_results if r[\"is_emphasis\"]]\n",
    "#     print(f\"  ê°•ì¡° êµ¬ê°„: {len(emphasis_segments)}ê°œ\")\n",
    "    \n",
    "#     # LLM í”¼ë“œë°± ì‹œë„\n",
    "#     llm_feedback = None\n",
    "#     if USE_OPENAI and OPENAI_API_KEY != \"your-api-key-here\":\n",
    "#         print(\"  OpenAI APIë¡œ í”¼ë“œë°± ìƒì„± ì¤‘...\")\n",
    "#         llm_feedback = generate_llm_feedback_openai(emphasis_segments, duration)\n",
    "#     elif not USE_OPENAI and ANTHROPIC_API_KEY != \"your-api-key-here\":\n",
    "#         print(\"  Anthropic APIë¡œ í”¼ë“œë°± ìƒì„± ì¤‘...\")\n",
    "#         llm_feedback = generate_llm_feedback_anthropic(emphasis_segments, duration)\n",
    "    \n",
    "#     if llm_feedback is None:\n",
    "#         print(\"  âš ï¸ LLM API ì‚¬ìš© ë¶ˆê°€, ë¡œì»¬ í”¼ë“œë°± ìƒì„±...\")\n",
    "#         llm_feedback = generate_feedback_local(emphasis_segments)\n",
    "    \n",
    "#     # í”¼ë“œë°±ì„ ë¶„ì„ ê²°ê³¼ì— ë³‘í•©\n",
    "#     final_emphasis_data = []\n",
    "#     for i, seg in enumerate(emphasis_segments):\n",
    "#         result = {\n",
    "#             \"index\": i + 1,\n",
    "#             \"start_sec\": seg[\"start_sec\"],\n",
    "#             \"end_sec\": seg[\"end_sec\"],\n",
    "#             \"text\": seg[\"text\"],\n",
    "#             \"emphasis_score\": seg[\"emphasis_score\"],\n",
    "#             \"key_word\": find_key_word(seg[\"text\"]),\n",
    "#             \"reason\": \"\"\n",
    "#         }\n",
    "        \n",
    "#         # LLM í”¼ë“œë°±ì´ ìˆìœ¼ë©´ ë³‘í•©\n",
    "#         if isinstance(llm_feedback, list) and i < len(llm_feedback):\n",
    "#             fb = llm_feedback[i]\n",
    "#             if isinstance(fb, dict):\n",
    "#                 result[\"key_word\"] = fb.get(\"key_word\", result[\"key_word\"])\n",
    "#                 result[\"reason\"] = fb.get(\"reason\", \"\")\n",
    "#                 if \"advice\" in fb:\n",
    "#                     result[\"advice\"] = fb[\"advice\"]\n",
    "        \n",
    "#         final_emphasis_data.append(result)\n",
    "    \n",
    "#     print(f\"  âœ… í”¼ë“œë°± ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "#     # 6. ì‹œê°í™” ì˜ìƒ ìƒì„±\n",
    "#     print(\"\\n[6/6] ğŸ¥ ì‹œê°í™” ì˜ìƒ ìƒì„±...\")\n",
    "#     create_visualization_video(VIDEO_PATH, final_emphasis_data, OUTPUT_VIDEO)\n",
    "    \n",
    "#     # ê²°ê³¼ ì €ì¥\n",
    "#     print(\"\\nğŸ“ ê²°ê³¼ ì €ì¥...\")\n",
    "    \n",
    "#     # ì „ì²´ ë¶„ì„ ê²°ê³¼\n",
    "#     full_result = {\n",
    "#         \"video_path\": VIDEO_PATH,\n",
    "#         \"duration_sec\": round(duration, 2),\n",
    "#         \"analysis_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#         \"weights\": WEIGHTS,\n",
    "#         \"threshold\": EMPHASIS_THRESHOLD,\n",
    "#         \"total_sentences\": len(analysis_results),\n",
    "#         \"emphasis_count\": len(emphasis_segments),\n",
    "#         \"statistics\": {\n",
    "#             \"gesture_mean\": round(np.mean([r[\"gesture_score\"] for r in analysis_results]), 4),\n",
    "#             \"audio_mean\": round(np.mean([r[\"audio_score\"] for r in analysis_results]), 4),\n",
    "#             \"text_mean\": round(np.mean([r[\"text_score\"] for r in analysis_results]), 4),\n",
    "#             \"emphasis_mean\": round(np.mean([r[\"emphasis_score\"] for r in analysis_results]), 4),\n",
    "#         },\n",
    "#         \"total_score\": round(np.mean([r[\"emphasis_score\"] for r in analysis_results]) * 100, 2),\n",
    "#         \"emphasis_segments\": final_emphasis_data,\n",
    "#         \"all_sentences\": analysis_results\n",
    "#     }\n",
    "    \n",
    "#     with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(full_result, f, indent=2, ensure_ascii=False)\n",
    "#     print(f\"  âœ… ë¶„ì„ ê²°ê³¼: {OUTPUT_JSON}\")\n",
    "    \n",
    "#     # LLM í”¼ë“œë°± ë³„ë„ ì €ì¥\n",
    "#     with open(OUTPUT_FEEDBACK, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(final_emphasis_data, f, indent=2, ensure_ascii=False)\n",
    "#     print(f\"  âœ… í”¼ë“œë°±: {OUTPUT_FEEDBACK}\")\n",
    "    \n",
    "#     # ìµœì¢… ìš”ì•½\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"ğŸ“Š ë¶„ì„ ì™„ë£Œ!\")\n",
    "#     print(\"=\"*70)\n",
    "#     print(f\"  ì˜ìƒ ê¸¸ì´: {duration:.2f}ì´ˆ\")\n",
    "#     print(f\"  ì´ ë¬¸ì¥: {len(analysis_results)}ê°œ\")\n",
    "#     print(f\"  ê°•ì¡° êµ¬ê°„: {len(emphasis_segments)}ê°œ ({len(emphasis_segments)/len(analysis_results)*100:.1f}%)\")\n",
    "#     print(f\"  ì´ì : {full_result['total_score']:.2f} / 100\")\n",
    "#     print(f\"\\nğŸ“‹ ê°•ì¡° êµ¬ê°„ (ì²˜ìŒ 5ê°œ):\")\n",
    "    \n",
    "#     for seg in final_emphasis_data[:5]:\n",
    "#         print(f\"  [{seg['index']}] {seg['start_sec']:.1f}s~{seg['end_sec']:.1f}s \"\n",
    "#               f\"(ì ìˆ˜: {seg['emphasis_score']:.2f})\")\n",
    "#         print(f\"      í…ìŠ¤íŠ¸: {seg['text'][:40]}...\")\n",
    "#         print(f\"      í‚¤ì›Œë“œ: {seg['key_word']}, ì´ìœ : {seg['reason'][:30]}...\")\n",
    "    \n",
    "#     print(f\"\\nğŸ“ ì¶œë ¥ íŒŒì¼:\")\n",
    "#     print(f\"  - ë¶„ì„ ê²°ê³¼: {OUTPUT_JSON}\")\n",
    "#     print(f\"  - í”¼ë“œë°±: {OUTPUT_FEEDBACK}\")\n",
    "#     print(f\"  - ì‹œê°í™” ì˜ìƒ: {OUTPUT_VIDEO}\")\n",
    "    \n",
    "#     print(\"\\nğŸ‰ ì™„ë£Œ!\")\n",
    "    \n",
    "#     return full_result\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "758de8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ Device: cuda\n",
      "[INFO] Loaded 2930 entries from /home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\n",
      "[INFO] Generated 2845 negative entries\n",
      "[INFO] Total entries: 5775 (pos: 2930, neg: 2845)\n",
      "ğŸ“¦ í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì„±ê³µ (46ê°œ)\n",
      "ğŸ“¦ í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì„±ê³µ (46ê°œ)\n",
      "âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: Train 37 batches\n",
      "Loaded gesture model from: /home/stu/ai_project/ëª¨ë¸ì§‘í•©/gesture_model.pt\n",
      "[INFO] Loaded audio model from: /home/stu/ai_project/ëª¨ë¸ì§‘í•©/voice_model.pth\n",
      "âœ… ì „ë¬¸ê°€ ëª¨ë¸ ë¡œë“œ ë° Freeze ì™„ë£Œ\n",
      "\n",
      "âš¡ [Simple MLP] Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x29a47540] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2947a8c0] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x2958fa40] mmco: unref short failure\n",
      "[h264 @ 0x29a43ac0] mmco: unref short failure\n",
      "[h264 @ 0x29a43ac0] mmco: unref short failure\n",
      "[h264 @ 0x29a43ac0] mmco: unref short failure\n",
      "[h264 @ 0x29a43ac0] mmco: unref short failure\n",
      "[h264 @ 0x29a43ac0] mmco: unref short failure\n",
      "[h264 @ 0x29a43ac0] mmco: unref short failure\n",
      "[h264 @ 0x29a43ac0] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x29443600] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x296fcd00] mmco: unref short failure\n",
      "[h264 @ 0x344baa40] mmco: unref short failure\n",
      "[h264 @ 0x344baa40] mmco: unref short failure\n",
      "[h264 @ 0x344baa40] mmco: unref short failure\n",
      "[h264 @ 0x344baa40] mmco: unref short failure\n",
      "[h264 @ 0x333e92c0] mmco: unref short failure\n",
      "[h264 @ 0x333e92c0] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 1: Val Acc 84.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x35e33c40] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x36f87c00] mmco: unref short failure\n",
      "[h264 @ 0x361b65c0] mmco: unref short failure\n",
      "[h264 @ 0x361b65c0] mmco: unref short failure\n",
      "[h264 @ 0x361b65c0] mmco: unref short failure\n",
      "[h264 @ 0x361b65c0] mmco: unref short failure\n",
      "[h264 @ 0x361b65c0] mmco: unref short failure\n",
      "[h264 @ 0x361b65c0] mmco: unref short failure\n",
      "[h264 @ 0x361b65c0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x34de9fc0] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x35e06f80] mmco: unref short failure\n",
      "[h264 @ 0x3547db80] mmco: unref short failure\n",
      "[h264 @ 0x34dcca00] mmco: unref short failure\n",
      "[h264 @ 0x34dcca00] mmco: unref short failure\n",
      "[h264 @ 0x34dcca00] mmco: unref short failure\n",
      "[h264 @ 0x34dcca00] mmco: unref short failure\n",
      "[h264 @ 0x356db700] mmco: unref short failure\n",
      "[h264 @ 0x356db700] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 2: Val Acc 85.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x35d2e000] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x35ddd380] mmco: unref short failure\n",
      "[h264 @ 0x36125980] mmco: unref short failure\n",
      "[h264 @ 0x36125980] mmco: unref short failure\n",
      "[h264 @ 0x36125980] mmco: unref short failure\n",
      "[h264 @ 0x36125980] mmco: unref short failure\n",
      "[h264 @ 0x36125980] mmco: unref short failure\n",
      "[h264 @ 0x36125980] mmco: unref short failure\n",
      "[h264 @ 0x36125980] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x34ce8b80] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x35ea3540] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x351b0800] mmco: unref short failure\n",
      "[h264 @ 0x34e46bc0] mmco: unref short failure\n",
      "[h264 @ 0x34e46bc0] mmco: unref short failure\n",
      "[h264 @ 0x34e46bc0] mmco: unref short failure\n",
      "[h264 @ 0x34e46bc0] mmco: unref short failure\n",
      "[h264 @ 0x35ef85c0] mmco: unref short failure\n",
      "[h264 @ 0x35ef85c0] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 3: Val Acc 87.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x34e7fb40] mmco: unref short failure\n",
      "[h264 @ 0x3944bac0] mmco: unref short failure\n",
      "[h264 @ 0x3944bac0] mmco: unref short failure\n",
      "[h264 @ 0x3944bac0] mmco: unref short failure\n",
      "[h264 @ 0x3944bac0] mmco: unref short failure\n",
      "[h264 @ 0x3944bac0] mmco: unref short failure\n",
      "[h264 @ 0x3944bac0] mmco: unref short failure\n",
      "[h264 @ 0x3944bac0] mmco: unref short failure\n",
      "[h264 @ 0x34dcb5c0] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x35e05940] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x34c96ac0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x35dca3c0] mmco: unref short failure\n",
      "[h264 @ 0x34caecc0] mmco: unref short failure\n",
      "[h264 @ 0x34caecc0] mmco: unref short failure\n",
      "[h264 @ 0x34caecc0] mmco: unref short failure\n",
      "[h264 @ 0x34caecc0] mmco: unref short failure\n",
      "[h264 @ 0x34a16a80] mmco: unref short failure\n",
      "[h264 @ 0x34a16a80] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 4: Val Acc 88.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x34ddc640] mmco: unref short failure\n",
      "[h264 @ 0x35e26140] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x35e93480] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x353be540] mmco: unref short failure\n",
      "[h264 @ 0x36019b40] mmco: unref short failure\n",
      "[h264 @ 0x36019b40] mmco: unref short failure\n",
      "[h264 @ 0x36019b40] mmco: unref short failure\n",
      "[h264 @ 0x36019b40] mmco: unref short failure\n",
      "[h264 @ 0x36019b40] mmco: unref short failure\n",
      "[h264 @ 0x36019b40] mmco: unref short failure\n",
      "[h264 @ 0x36019b40] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35e773c0] mmco: unref short failure\n",
      "[h264 @ 0x35d67400] mmco: unref short failure\n",
      "[h264 @ 0x35d67400] mmco: unref short failure\n",
      "[h264 @ 0x35d67400] mmco: unref short failure\n",
      "[h264 @ 0x35d67400] mmco: unref short failure\n",
      "[h264 @ 0x35ee7180] mmco: unref short failure\n",
      "[h264 @ 0x35ee7180] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 5: Val Acc 90.30%\n",
      "\n",
      "âš¡ [Gated Fusion] Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x367b2100] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x34a1a440] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x35fe35c0] mmco: unref short failure\n",
      "[h264 @ 0x34a203c0] mmco: unref short failure\n",
      "[h264 @ 0x39071940] mmco: unref short failure\n",
      "[h264 @ 0x39071940] mmco: unref short failure\n",
      "[h264 @ 0x39071940] mmco: unref short failure\n",
      "[h264 @ 0x39071940] mmco: unref short failure\n",
      "[h264 @ 0x39071940] mmco: unref short failure\n",
      "[h264 @ 0x39071940] mmco: unref short failure\n",
      "[h264 @ 0x39071940] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x35deddc0] mmco: unref short failure\n",
      "[h264 @ 0x34c3b240] mmco: unref short failure\n",
      "[h264 @ 0x34c3b240] mmco: unref short failure\n",
      "[h264 @ 0x34c3b240] mmco: unref short failure\n",
      "[h264 @ 0x34c3b240] mmco: unref short failure\n",
      "[h264 @ 0x35d1d840] mmco: unref short failure\n",
      "[h264 @ 0x35d1d840] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 1: Val Acc 84.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34f5abc0] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x34c35d80] mmco: unref short failure\n",
      "[h264 @ 0x35c25a00] mmco: unref short failure\n",
      "[h264 @ 0x35c25a00] mmco: unref short failure\n",
      "[h264 @ 0x35c25a00] mmco: unref short failure\n",
      "[h264 @ 0x35c25a00] mmco: unref short failure\n",
      "[h264 @ 0x35c25a00] mmco: unref short failure\n",
      "[h264 @ 0x35c25a00] mmco: unref short failure\n",
      "[h264 @ 0x35c25a00] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x345d5000] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x3542aa00] mmco: unref short failure\n",
      "[h264 @ 0x35c7c400] mmco: unref short failure\n",
      "[h264 @ 0x34d04ec0] mmco: unref short failure\n",
      "[h264 @ 0x34d04ec0] mmco: unref short failure\n",
      "[h264 @ 0x34d04ec0] mmco: unref short failure\n",
      "[h264 @ 0x34d04ec0] mmco: unref short failure\n",
      "[h264 @ 0x35da37c0] mmco: unref short failure\n",
      "[h264 @ 0x35da37c0] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 2: Val Acc 84.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35bc4480] mmco: unref short failure\n",
      "[h264 @ 0x35c7bb40] mmco: unref short failure\n",
      "[h264 @ 0x35c7bb40] mmco: unref short failure\n",
      "[h264 @ 0x35c7bb40] mmco: unref short failure\n",
      "[h264 @ 0x35c7bb40] mmco: unref short failure\n",
      "[h264 @ 0x35c7bb40] mmco: unref short failure\n",
      "[h264 @ 0x35c7bb40] mmco: unref short failure\n",
      "[h264 @ 0x35c7bb40] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x3454e180] mmco: unref short failure\n",
      "[h264 @ 0x34b05200] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34bf57c0] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x34c3ef00] mmco: unref short failure\n",
      "[h264 @ 0x35b3d840] mmco: unref short failure\n",
      "[h264 @ 0x35b3d840] mmco: unref short failure\n",
      "[h264 @ 0x35b3d840] mmco: unref short failure\n",
      "[h264 @ 0x35b3d840] mmco: unref short failure\n",
      "[h264 @ 0x35cece00] mmco: unref short failure\n",
      "[h264 @ 0x35cece00] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 3: Val Acc 84.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x35d68900] mmco: unref short failure\n",
      "[h264 @ 0x35d68900] mmco: unref short failure\n",
      "[h264 @ 0x35d68900] mmco: unref short failure\n",
      "[h264 @ 0x35d68900] mmco: unref short failure\n",
      "[h264 @ 0x35d68900] mmco: unref short failure\n",
      "[h264 @ 0x35d68900] mmco: unref short failure\n",
      "[h264 @ 0x35d68900] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x346083c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x34b368c0] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35cd2640] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35d87f80] mmco: unref short failure\n",
      "[h264 @ 0x35c787c0] mmco: unref short failure\n",
      "[h264 @ 0x3628e100] mmco: unref short failure\n",
      "[h264 @ 0x3628e100] mmco: unref short failure\n",
      "[h264 @ 0x3628e100] mmco: unref short failure\n",
      "[h264 @ 0x3628e100] mmco: unref short failure\n",
      "[h264 @ 0x34c6bcc0] mmco: unref short failure\n",
      "[h264 @ 0x34c6bcc0] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 4: Val Acc 86.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x35d23f80] mmco: unref short failure\n",
      "[h264 @ 0x35d23f80] mmco: unref short failure\n",
      "[h264 @ 0x35d23f80] mmco: unref short failure\n",
      "[h264 @ 0x35d23f80] mmco: unref short failure\n",
      "[h264 @ 0x35d23f80] mmco: unref short failure\n",
      "[h264 @ 0x35d23f80] mmco: unref short failure\n",
      "[h264 @ 0x35d23f80] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x34b34d40] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x35b35d00] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33d1c300] mmco: unref short failure\n",
      "[h264 @ 0x33f105c0] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34d5f400] mmco: unref short failure\n",
      "[h264 @ 0x34c46c00] mmco: unref short failure\n",
      "[h264 @ 0x34c46c00] mmco: unref short failure\n",
      "[h264 @ 0x34c46c00] mmco: unref short failure\n",
      "[h264 @ 0x34c46c00] mmco: unref short failure\n",
      "[h264 @ 0x34d13d80] mmco: unref short failure\n",
      "[h264 @ 0x34d13d80] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 5: Val Acc 86.49%\n",
      "\n",
      "âš¡ [Transformer] Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x34065cc0] mmco: unref short failure\n",
      "[h264 @ 0x35434200] mmco: unref short failure\n",
      "[h264 @ 0x35434200] mmco: unref short failure\n",
      "[h264 @ 0x35434200] mmco: unref short failure\n",
      "[h264 @ 0x35434200] mmco: unref short failure\n",
      "[h264 @ 0x35434200] mmco: unref short failure\n",
      "[h264 @ 0x35434200] mmco: unref short failure\n",
      "[h264 @ 0x35434200] mmco: unref short failure\n",
      "[h264 @ 0x3520c4c0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x34068cc0] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x35329600] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x36845d80] mmco: unref short failure\n",
      "[h264 @ 0x35093000] mmco: unref short failure\n",
      "[h264 @ 0x35093000] mmco: unref short failure\n",
      "[h264 @ 0x35093000] mmco: unref short failure\n",
      "[h264 @ 0x35093000] mmco: unref short failure\n",
      "[h264 @ 0x35160280] mmco: unref short failure\n",
      "[h264 @ 0x35160280] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 1: Val Acc 86.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x34085880] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x3410ad40] mmco: unref short failure\n",
      "[h264 @ 0x33e17840] mmco: unref short failure\n",
      "[h264 @ 0x33e17840] mmco: unref short failure\n",
      "[h264 @ 0x33e17840] mmco: unref short failure\n",
      "[h264 @ 0x33e17840] mmco: unref short failure\n",
      "[h264 @ 0x33e17840] mmco: unref short failure\n",
      "[h264 @ 0x33e17840] mmco: unref short failure\n",
      "[h264 @ 0x33e17840] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x35165600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x33eeb600] mmco: unref short failure\n",
      "[h264 @ 0x34f93dc0] mmco: unref short failure\n",
      "[h264 @ 0x33ce7840] mmco: unref short failure\n",
      "[h264 @ 0x33ce7840] mmco: unref short failure\n",
      "[h264 @ 0x33ce7840] mmco: unref short failure\n",
      "[h264 @ 0x33ce7840] mmco: unref short failure\n",
      "[h264 @ 0x340a3880] mmco: unref short failure\n",
      "[h264 @ 0x340a3880] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 2: Val Acc 87.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x33a56a00] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x3515f540] mmco: unref short failure\n",
      "[h264 @ 0x33ee95c0] mmco: unref short failure\n",
      "[h264 @ 0x33ee95c0] mmco: unref short failure\n",
      "[h264 @ 0x33ee95c0] mmco: unref short failure\n",
      "[h264 @ 0x33ee95c0] mmco: unref short failure\n",
      "[h264 @ 0x33ee95c0] mmco: unref short failure\n",
      "[h264 @ 0x33ee95c0] mmco: unref short failure\n",
      "[h264 @ 0x33ee95c0] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x34421580] mmco: unref short failure\n",
      "[h264 @ 0x33fa2ec0] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x350b5a40] mmco: unref short failure\n",
      "[h264 @ 0x34356280] mmco: unref short failure\n",
      "[h264 @ 0x34356280] mmco: unref short failure\n",
      "[h264 @ 0x34356280] mmco: unref short failure\n",
      "[h264 @ 0x34356280] mmco: unref short failure\n",
      "[h264 @ 0x33ccd780] mmco: unref short failure\n",
      "[h264 @ 0x33ccd780] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 3: Val Acc 87.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33f11ac0] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x33d2ca00] mmco: unref short failure\n",
      "[h264 @ 0x3408f200] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x34752600] mmco: unref short failure\n",
      "[h264 @ 0x352b57c0] mmco: unref short failure\n",
      "[h264 @ 0x352b57c0] mmco: unref short failure\n",
      "[h264 @ 0x352b57c0] mmco: unref short failure\n",
      "[h264 @ 0x352b57c0] mmco: unref short failure\n",
      "[h264 @ 0x352b57c0] mmco: unref short failure\n",
      "[h264 @ 0x352b57c0] mmco: unref short failure\n",
      "[h264 @ 0x352b57c0] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x33be8a80] mmco: unref short failure\n",
      "[h264 @ 0x35035180] mmco: unref short failure\n",
      "[h264 @ 0x35035180] mmco: unref short failure\n",
      "[h264 @ 0x35035180] mmco: unref short failure\n",
      "[h264 @ 0x35035180] mmco: unref short failure\n",
      "[h264 @ 0x33f03540] mmco: unref short failure\n",
      "[h264 @ 0x33f03540] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 4: Val Acc 89.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x352239c0] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x33e5e640] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x341eec40] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x350577c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33bea1c0] mmco: unref short failure\n",
      "[h264 @ 0x33f44180] mmco: unref short failure\n",
      "[h264 @ 0x33f44180] mmco: unref short failure\n",
      "[h264 @ 0x33f44180] mmco: unref short failure\n",
      "[h264 @ 0x33f44180] mmco: unref short failure\n",
      "[h264 @ 0x33f44180] mmco: unref short failure\n",
      "[h264 @ 0x33f44180] mmco: unref short failure\n",
      "[h264 @ 0x33f44180] mmco: unref short failure\n",
      "[h264 @ 0x340950c0] mmco: unref short failure\n",
      "[h264 @ 0x340950c0] mmco: unref short failure\n",
      "[h264 @ 0x340950c0] mmco: unref short failure\n",
      "[h264 @ 0x340950c0] mmco: unref short failure\n",
      "[h264 @ 0x35fa0440] mmco: unref short failure\n",
      "[h264 @ 0x35fa0440] mmco: unref short failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ep 5: Val Acc 88.74%\n",
      "\n",
      "========================================\n",
      "ğŸ† ìµœì¢… ê²°ê³¼ ë¹„êµ\n",
      "========================================\n",
      "          Model   Best Acc\n",
      "0    Simple MLP  90.303030\n",
      "1  Gated Fusion  86.493506\n",
      "2   Transformer  89.350649\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from config import (\n",
    "    GESTURE_CONFIG, AUDIO_CONFIG, FUSION_CONFIG,\n",
    "    GESTURE_MODEL_PATH, AUDIO_MODEL_PATH, MODEL_DIR, \n",
    "    ensure_dirs, AUDIO_SCALER_PATH\n",
    ")\n",
    "from utils import (\n",
    "    prepare_all_entries, train_val_split,\n",
    "    VideoEmphasisDataset, AudioEmphasisDataset,\n",
    "    load_gesture_model, load_audio_model, AudioFeatureExtractor\n",
    ")\n",
    "\n",
    "# ğŸ”‡ ê²½ê³  ë©”ì‹œì§€ ë„ê¸° (Librosa ë“±)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ› ï¸ 1. ì„¤ì • ë° ë°ì´í„° ë¡œë“œ\n",
    "# =============================================================================\n",
    "CSV_PATH = \"/home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\"\n",
    "batch_size = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ Device: {device}\")\n",
    "\n",
    "# â˜… [ìˆ˜ì •ë¨] 3ì°¨ì› í…ì„œ ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€\n",
    "class TextEmphasisDataset(Dataset):\n",
    "    def __init__(self, entries, text_tensor_dir=MODEL_DIR): \n",
    "        self.entries = entries\n",
    "        self.input_dim = 1024  # RoBERTa Largeìš©\n",
    "        self.global_tensor_path = os.path.join(text_tensor_dir, \"text_jo_torch_text.pt\")\n",
    "        self.global_tensors = None\n",
    "        \n",
    "        if os.path.exists(self.global_tensor_path):\n",
    "            try: \n",
    "                self.global_tensors = torch.load(self.global_tensor_path, map_location='cpu')\n",
    "                print(f\"ğŸ“¦ í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì„±ê³µ ({len(self.global_tensors)}ê°œ)\")\n",
    "            except: \n",
    "                print(\"âš ï¸ í…ìŠ¤íŠ¸ í…ì„œ ë¡œë“œ ì‹¤íŒ¨\")\n",
    "\n",
    "    def __len__(self): return len(self.entries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.zeros(self.input_dim) # ê¸°ë³¸ê°’\n",
    "        \n",
    "        if self.global_tensors is not None and idx < len(self.global_tensors):\n",
    "            tensor_data = self.global_tensors[idx]\n",
    "            \n",
    "            if isinstance(tensor_data, torch.Tensor):\n",
    "                # â˜… [í•µì‹¬ ìˆ˜ì •] ì°¨ì› ì¶•ì†Œ ë¡œì§ ê°•í™”\n",
    "                # 3ì°¨ì›ì¸ ê²½ìš°: [1, 128, 1024] -> [128, 1024]\n",
    "                if tensor_data.dim() == 3:\n",
    "                    tensor_data = tensor_data.squeeze(0)\n",
    "                \n",
    "                # 2ì°¨ì›ì¸ ê²½ìš°: [128, 1024] -> [1024] (í‰ê· )\n",
    "                if tensor_data.dim() == 2:\n",
    "                    x = tensor_data.mean(dim=0)\n",
    "                elif tensor_data.dim() == 1:\n",
    "                    x = tensor_data\n",
    "                    \n",
    "        y = torch.tensor(self.entries[idx]['label'], dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, entries, audio_extractor):\n",
    "        self.entries = entries\n",
    "        self.video_dataset = VideoEmphasisDataset(entries, clip_len=16, resize_hw=(112, 112))\n",
    "        self.audio_dataset = AudioEmphasisDataset(entries, extractor=audio_extractor, config=AUDIO_CONFIG)\n",
    "        self.text_dataset = TextEmphasisDataset(entries)\n",
    "\n",
    "    def __len__(self): return len(self.entries)\n",
    "    def __getitem__(self, idx):\n",
    "        X_vid, y_vid = self.video_dataset[idx]\n",
    "        X_aud, _ = self.audio_dataset[idx]\n",
    "        X_txt, _ = self.text_dataset[idx]\n",
    "        return X_vid, X_aud, X_txt, y_vid.float()\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "audio_extractor = AudioFeatureExtractor(AUDIO_CONFIG)\n",
    "if os.path.exists(AUDIO_SCALER_PATH): audio_extractor.load_scaler(AUDIO_SCALER_PATH)\n",
    "\n",
    "all_entries = prepare_all_entries(CSV_PATH)\n",
    "train_entries, val_entries = train_val_split(all_entries, split_ratio=0.8)\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜\n",
    "labels = np.array([e['label'] for e in all_entries])\n",
    "pos_weight_val = np.sum(labels==0) / np.sum(labels==1) if np.sum(labels==1) > 0 else 1.0\n",
    "pos_weight = torch.tensor([pos_weight_val]).to(device)\n",
    "\n",
    "train_loader = DataLoader(FusionDataset(train_entries, audio_extractor), batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(FusionDataset(val_entries, audio_extractor), batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "print(f\"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: Train {len(train_loader)} batches\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ¤– 2. ì „ë¬¸ê°€ ëª¨ë¸ ë¡œë“œ\n",
    "# =============================================================================\n",
    "class TextBERTModel(nn.Module):\n",
    "    def __init__(self, input_dim=1024): \n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, 2)\n",
    "    def forward(self, x, return_feature=False):\n",
    "        return (self.fc(x), x) if return_feature else self.fc(x)\n",
    "\n",
    "g_model = load_gesture_model(GESTURE_MODEL_PATH, device)\n",
    "a_model = load_audio_model(AUDIO_MODEL_PATH, device)\n",
    "t_model = TextBERTModel(input_dim=1024).to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    g_model = nn.DataParallel(g_model); a_model = nn.DataParallel(a_model); t_model = nn.DataParallel(t_model)\n",
    "\n",
    "for m in [g_model, a_model, t_model]:\n",
    "    for p in m.parameters(): p.requires_grad = False\n",
    "    m.eval()\n",
    "print(\"âœ… ì „ë¬¸ê°€ ëª¨ë¸ ë¡œë“œ ë° Freeze ì™„ë£Œ\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ§  3. Fusion ëª¨ë¸ ì •ì˜ (3ê°€ì§€)\n",
    "# =============================================================================\n",
    "# â‘  Simple MLP\n",
    "class SimpleFusionMLP(nn.Module):\n",
    "    def __init__(self, g_dim=512, a_dim=128, t_dim=1024, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(g_dim+a_dim+t_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2), nn.ReLU(), nn.Linear(hidden_dim//2, 1)\n",
    "        )\n",
    "    def forward(self, g, a, t): return self.net(torch.cat([g, a, t], dim=1))\n",
    "\n",
    "# â‘¡ Gated Fusion\n",
    "class GatedFusionNet(nn.Module):\n",
    "    def __init__(self, g_dim=512, a_dim=128, t_dim=1024, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.g_proj = nn.Linear(g_dim, hidden_dim); self.a_proj = nn.Linear(a_dim, hidden_dim); self.t_proj = nn.Linear(t_dim, hidden_dim)\n",
    "        self.gate_fc = nn.Linear(hidden_dim*3, 3)\n",
    "        self.classifier = nn.Sequential(nn.Linear(hidden_dim, hidden_dim//2), nn.ReLU(), nn.Dropout(0.2), nn.Linear(hidden_dim//2, 1))\n",
    "    def forward(self, g, a, t):\n",
    "        g_h, a_h, t_h = F.relu(self.g_proj(g)), F.relu(self.a_proj(a)), F.relu(self.t_proj(t))\n",
    "        gates = F.softmax(self.gate_fc(torch.cat([g_h, a_h, t_h], dim=1)), dim=1)\n",
    "        return self.classifier(gates[:,0:1]*g_h + gates[:,1:2]*a_h + gates[:,2:3]*t_h)\n",
    "\n",
    "# â‘¢ Transformer Fusion\n",
    "class TransformerFusionNet(nn.Module):\n",
    "    def __init__(self, g_dim=512, a_dim=128, t_dim=1024, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.projs = nn.ModuleList([nn.Linear(d, embed_dim) for d in [g_dim, a_dim, t_dim]])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(embed_dim, 4, batch_first=True), 2)\n",
    "        self.classifier = nn.Linear(embed_dim, 1)\n",
    "    def forward(self, g, a, t):\n",
    "        emb = torch.stack([l(x) for l, x in zip(self.projs, [g, a, t])], dim=1)\n",
    "        seq = torch.cat([self.cls_token.expand(g.size(0), -1, -1), emb], dim=1)\n",
    "        return self.classifier(self.transformer(seq)[:, 0, :])\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸš€ 4. í•™ìŠµ ë° ë¹„êµ ì‹¤í–‰\n",
    "# =============================================================================\n",
    "def train_eval(name, model, epochs=5):\n",
    "    print(f\"\\nâš¡ [{name}] Training...\")\n",
    "    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    crit = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for v, a, t, y in train_loader:\n",
    "            v, a, t, y = v.to(device), a.to(device), t.to(device), y.to(device).unsqueeze(1)\n",
    "            with torch.no_grad(): _, gf = g_model(v, True); _, af = a_model(a, True); _, tf = t_model(t, True)\n",
    "            opt.zero_grad(); loss = crit(model(gf, af, tf), y); loss.backward(); opt.step()\n",
    "            \n",
    "        model.eval()\n",
    "        corr = 0; total = 0\n",
    "        with torch.no_grad():\n",
    "            for v, a, t, y in val_loader:\n",
    "                v, a, t, y = v.to(device), a.to(device), t.to(device), y.to(device).unsqueeze(1)\n",
    "                _, gf = g_model(v, True); _, af = a_model(a, True); _, tf = t_model(t, True)\n",
    "                preds = (torch.sigmoid(model(gf, af, tf)) >= 0.5).float()\n",
    "                corr += (preds == y).sum().item(); total += y.size(0)\n",
    "        acc = 100*corr/total\n",
    "        if acc > best_acc: best_acc = acc\n",
    "        print(f\"   Ep {ep+1}: Val Acc {acc:.2f}%\")\n",
    "    return {\"Model\": name, \"Best Acc\": best_acc}\n",
    "\n",
    "results = []\n",
    "for name, model in [\n",
    "    (\"Simple MLP\", SimpleFusionMLP()), \n",
    "    (\"Gated Fusion\", GatedFusionNet()), \n",
    "    (\"Transformer\", TransformerFusionNet())\n",
    "]:\n",
    "    results.append(train_eval(name, model))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40 + \"\\nğŸ† ìµœì¢… ê²°ê³¼ ë¹„êµ\\n\" + \"=\"*40)\n",
    "print(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3c0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ì¥ì¹˜: cuda\n",
      "ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\n",
      "ì„¤ì • ì™„ë£Œ! ì´ì œ ì•„ë˜ ì‹¤í—˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ====================================================\n",
    "# 0. ì‚¬ì „ ì„¤ì • (CONFIG & Device)\n",
    "# ====================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ì‚¬ìš© ì¥ì¹˜: {device}\")\n",
    "\n",
    "CONFIG = {\n",
    "    'batch_size': 4,       # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì ˆ)\n",
    "    'num_classes': 5,      # ë¶„ë¥˜í•  í´ë˜ìŠ¤ ê°œìˆ˜ (ì˜ˆ: ê°•ì¡°, ë¹„ê°•ì¡° ë“±)\n",
    "    'num_frames': 16,      # 3D CNN ì…ë ¥ í”„ë ˆì„ ìˆ˜\n",
    "    'image_size': 112      # ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°\n",
    "}\n",
    "\n",
    "# ====================================================\n",
    "# 1. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (GestureModel3D)\n",
    "# ====================================================\n",
    "class GestureModel3D(nn.Module):\n",
    "    def __init__(self, backbone_name='r3d_18', num_classes=CONFIG['num_classes']):\n",
    "        super(GestureModel3D, self).__init__()\n",
    "        \n",
    "        # torchvisionì—ì„œ ì œê³µí•˜ëŠ” 3D CNN ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        if backbone_name == 'r3d_18':\n",
    "            self.backbone = torchvision.models.video.r3d_18(pretrained=True)\n",
    "        elif backbone_name == 'mc3_18':\n",
    "            self.backbone = torchvision.models.video.mc3_18(pretrained=True)\n",
    "        elif backbone_name == 'r2plus1d_18':\n",
    "            self.backbone = torchvision.models.video.r2plus1d_18(pretrained=True)\n",
    "        else:\n",
    "            raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {backbone_name}\")\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ FC ë ˆì´ì–´ë¥¼ ìš°ë¦¬ ë°ì´í„° í´ë˜ìŠ¤ ìˆ˜ì— ë§ê²Œ êµì²´\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# ====================================================\n",
    "# 2. (ì„ì‹œ) ë°ì´í„°ì…‹ ì •ì˜ ë° ë¡œë” ìƒì„±\n",
    "# ====================================================\n",
    "# ì‹¤ì œ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì´ ë¶€ë¶„ì„ ì‹¤ì œ Dataset í´ë˜ìŠ¤ë¡œ êµì²´í•˜ì„¸ìš”.\n",
    "class DummyVideoDataset(Dataset):\n",
    "    def __init__(self, length=200):\n",
    "        self.length = length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 3D CNN ì…ë ¥ í˜•íƒœ: (C, Frames, H, W) -> (3, 16, 112, 112)\n",
    "        # ëœë¤í•œ í…ì„œ ìƒì„± (ì‹¤ì œ ì˜ìƒ ë°ì´í„° ëŒ€ì‹ )\n",
    "        frames = torch.randn(3, CONFIG['num_frames'], CONFIG['image_size'], CONFIG['image_size'])\n",
    "        label = torch.randint(0, CONFIG['num_classes'], (1,)).item()\n",
    "        return frames, label\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "print(\"ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "train_dataset = DummyVideoDataset(length=200) # í•™ìŠµìš© 200ê°œ (ì‹¤í—˜ ë¹„ìœ¨ í…ŒìŠ¤íŠ¸ìš©)\n",
    "val_dataset = DummyVideoDataset(length=50)    # ê²€ì¦ìš© 50ê°œ\n",
    "\n",
    "# ê²€ì¦ ë¡œë” ìƒì„± (val_loader)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"ì„¤ì • ì™„ë£Œ! ì´ì œ ì•„ë˜ ì‹¤í—˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f16ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì‚¬ìš© ì¥ì¹˜: cuda\n",
      "âœ… ëª¨ë¸ ë° ì„¤ì • ì¤€ë¹„ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# ====================================================\n",
    "# [ì„¤ì •] ë°ì´í„°ì…‹ í´ë˜ìŠ¤ê°€ ìˆëŠ” íŒŒì¼ import\n",
    "# ====================================================\n",
    "# ë§Œì•½ ë°ì´í„°ì…‹ ì½”ë“œê°€ 'my_dataset.py'ë¼ëŠ” íŒŒì¼ì— ìˆë‹¤ë©´:\n",
    "# from my_dataset import VideoEmphasisDataset \n",
    "# \n",
    "# (ë§Œì•½ ë°©ê¸ˆ ì£¼í”¼í„° ë…¸íŠ¸ë¶ ìœ„ ì…€ì— ë³µì‚¬í•´ë‘ì…¨ë‹¤ë©´, ë³„ë„ import ì—†ì´ ë°”ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤!)\n",
    "\n",
    "# ====================================================\n",
    "# 1. ì„¤ì •ê°’ (CONFIG)\n",
    "# ====================================================\n",
    "CONFIG = {\n",
    "    'batch_size': 120,       # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì ˆ (4~16 ì¶”ì²œ)\n",
    "    'num_classes': 2,      # (ê°•ì¡° O / ê°•ì¡° X) -> 2ê°œ í´ë˜ìŠ¤ì¸ ê²½ìš°\n",
    "    'clip_len': 16,        # í”„ë ˆì„ ìˆ˜ (Dataset ê¸°ë³¸ê°’ê³¼ ì¼ì¹˜)\n",
    "    'image_size': 112,     # ì´ë¯¸ì§€ í¬ê¸°\n",
    "    'learning_rate': 1e-4\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸš€ ì‚¬ìš© ì¥ì¹˜: {device}\")\n",
    "\n",
    "# ====================================================\n",
    "# 2. ëª¨ë¸ ì •ì˜ (GestureModel3D)\n",
    "# ====================================================\n",
    "class GestureModel3D(nn.Module):\n",
    "    def __init__(self, backbone_name='r3d_18', num_classes=CONFIG['num_classes']):\n",
    "        super(GestureModel3D, self).__init__()\n",
    "        \n",
    "        if backbone_name == 'r3d_18':\n",
    "            self.backbone = torchvision.models.video.r3d_18(weights='DEFAULT')\n",
    "        elif backbone_name == 'mc3_18':\n",
    "            self.backbone = torchvision.models.video.mc3_18(weights='DEFAULT')\n",
    "        else:\n",
    "            raise ValueError(\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\")\n",
    "        \n",
    "        # FC ë ˆì´ì–´ êµì²´\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë° ì„¤ì • ì¤€ë¹„ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6af11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VideoEmphasisDataset í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# =============================================================================\n",
    "# 1. ì´ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš” (Dataset í´ë˜ìŠ¤ ì •ì˜)\n",
    "# =============================================================================\n",
    "class VideoEmphasisDataset(Dataset):\n",
    "    def __init__(self, entries, clip_len=16, resize_hw=(112, 112)):\n",
    "        self.entries = entries\n",
    "        self.clip_len = clip_len\n",
    "        self.resize_hw = resize_hw\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.entries[idx]\n",
    "        video_path = entry[\"video_path\"]\n",
    "        start_sec = entry[\"start_sec\"]\n",
    "        end_sec = entry[\"end_sec\"]\n",
    "        label = entry[\"label\"]\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            # ì˜ìƒì´ ì•ˆ ì—´ë¦¬ë©´ 0ìœ¼ë¡œ ì±„ìš´ í…ì„œ ë°˜í™˜ (ì—ëŸ¬ ë°©ì§€)\n",
    "            frames = np.zeros((3, self.clip_len, self.resize_hw[0], self.resize_hw[1]), dtype=np.float32)\n",
    "            return torch.from_numpy(frames), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        start_frame = int(start_sec * fps)\n",
    "        end_frame = int(end_sec * fps)\n",
    "\n",
    "        start_frame = max(0, min(start_frame, total_frames - 1))\n",
    "        end_frame = max(0, min(end_frame, total_frames - 1))\n",
    "\n",
    "        if end_frame <= start_frame:\n",
    "            end_frame = min(start_frame + self.clip_len, total_frames - 1)\n",
    "\n",
    "        seg_len = end_frame - start_frame + 1\n",
    "\n",
    "        if seg_len >= self.clip_len:\n",
    "            indices = np.linspace(start_frame, end_frame, num=self.clip_len, dtype=int)\n",
    "        else:\n",
    "            base_indices = np.linspace(start_frame, end_frame, num=seg_len, dtype=int)\n",
    "            if seg_len > 0:\n",
    "                repeat = int(np.ceil(self.clip_len / seg_len))\n",
    "                indices = np.tile(base_indices, repeat)[:self.clip_len]\n",
    "            else:\n",
    "                indices = np.zeros(self.clip_len, dtype=int)\n",
    "\n",
    "        frames = []\n",
    "        for f_idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, f_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                if len(frames) > 0:\n",
    "                    frame = frames[-1]\n",
    "                else:\n",
    "                    frame = np.zeros((self.resize_hw[0], self.resize_hw[1], 3), dtype=np.uint8)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, self.resize_hw)\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # í…ì„œ ë³€í™˜\n",
    "        frames = np.stack(frames, axis=0)            # (T, H, W, C)\n",
    "        frames = frames.astype(np.float32) / 255.0   # [0, 1]\n",
    "        frames = np.transpose(frames, (3, 0, 1, 2))  # (C, T, H, W)\n",
    "        frames = (frames - 0.5) / 0.5                # [-1, 1]\n",
    "\n",
    "        x = torch.from_numpy(frames)\n",
    "        y = torch.tensor(label, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "print(\"âœ… VideoEmphasisDataset í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49bbdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: /home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\n",
      "   -> ë°ì´í„°í”„ë ˆì„ ë¡œë“œ ì„±ê³µ! (ìƒìœ„ ì»¬ëŸ¼: ['video_id', 'video_path', 'start_sec', 'end_sec', 'label'])\n",
      "ğŸ‰ ë¦¬ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ! ì´ 2930ê°œì˜ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ë°ì´í„°ì…‹ ë° ë¡œë” ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ì‹¤í—˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "# ====================================================\n",
    "# 3. ë°ì´í„° ì¤€ë¹„ (CSV -> Entries ë¦¬ìŠ¤íŠ¸ ë³€í™˜)\n",
    "# ====================================================\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì •\n",
    "csv_path = \"/home/stu/ai_project/ì˜ìƒcsv/ìµœì¢…csv.csv\"\n",
    "\n",
    "# 2. CSV íŒŒì¼ ì½ê¸°\n",
    "if os.path.exists(csv_path):\n",
    "    print(f\"âœ… íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"   -> ë°ì´í„°í”„ë ˆì„ ë¡œë“œ ì„±ê³µ! (ìƒìœ„ ì»¬ëŸ¼: {df.columns.tolist()})\")\n",
    "    \n",
    "    entries = []\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # ğŸ”¥ [ì¤‘ìš”] ì—¬ê¸° ì•„ë˜ ë¶€ë¶„ì„ CSV ì»¬ëŸ¼ëª…ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤!\n",
    "    # -------------------------------------------------------------------------\n",
    "    try:\n",
    "        for idx, row in df.iterrows():\n",
    "            # ë§Œì•½ ì—ëŸ¬ê°€ ë‚˜ë©´, ì•„ë˜ 'ë”°ì˜´í‘œ ì•ˆì˜ ì´ë¦„'ì„ CSV ì‹¤ì œ ì»¬ëŸ¼ëª…ê³¼ ë˜‘ê°™ì´ ë°”ê¾¸ì„¸ìš”.\n",
    "            entries.append({\n",
    "                \"video_path\": row['video_path'],  # ì˜ìƒ ê²½ë¡œ ì»¬ëŸ¼ëª…\n",
    "                \"start_sec\": row['start_sec'],        # ì‹œì‘ ì‹œê°„ ì»¬ëŸ¼ëª…\n",
    "                \"end_sec\": row['end_sec'],            # ì¢…ë£Œ ì‹œê°„ ì»¬ëŸ¼ëª…\n",
    "                \"label\": row['label']             # ë¼ë²¨ ì»¬ëŸ¼ëª…\n",
    "            })\n",
    "        print(f\"ğŸ‰ ë¦¬ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ! ì´ {len(entries)}ê°œì˜ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\"\\nâŒ [ì˜¤ë¥˜ ë°œìƒ] CSVì— '{e}' ë¼ëŠ” ì´ë¦„ì˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "        print(f\"ğŸ‘‰ í˜„ì¬ CSVì˜ ì»¬ëŸ¼ ì´ë¦„ë“¤: {df.columns.tolist()}\")\n",
    "        print(\"   ì½”ë“œì˜ row['...'] ë¶€ë¶„ ì´ë¦„ì„ ìœ„ ëª©ë¡ì— ìˆëŠ” ê²ƒìœ¼ë¡œ ê³ ì³ì£¼ì„¸ìš”.\")\n",
    "        entries = [] # ì—ëŸ¬ë‚˜ë©´ ì´ˆê¸°í™”\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_path}\")\n",
    "    entries = []\n",
    "\n",
    "# 3. ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„± (ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)\n",
    "if entries:\n",
    "    # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    full_dataset = VideoEmphasisDataset(entries, clip_len=CONFIG['clip_len'], resize_hw=(112, 112))\n",
    "\n",
    "    # Train/Val ë¶„ë¦¬ (8:2)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì—ëŸ¬ë‚˜ë¯€ë¡œ ì˜ˆì™¸ì²˜ë¦¬\n",
    "    if val_size == 0:\n",
    "        print(\"âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ì„œ ë‚˜ëˆ„ì§€ ì•Šê³  ì „ì²´ë¥¼ í•™ìŠµì— ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        train_dataset = full_dataset\n",
    "        val_dataset = full_dataset\n",
    "    else:\n",
    "        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    # ê²€ì¦ìš© ë¡œë” (ê³ ì •)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "    print(\"âœ… ë°ì´í„°ì…‹ ë° ë¡œë” ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ì‹¤í—˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"â›” ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35eb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì‹¤í—˜ ì‹œì‘: ë°ì´í„° ë¹„ìœ¨ [0.2, 0.4, 0.6, 0.8, 1.0]\n",
      "\n",
      "âš¡ [Data: 20%] ìƒ˜í”Œ ìˆ˜: 468ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x11dd2100] mmco: unref short failure\n",
      "[h264 @ 0x11dd2100] mmco: unref short failure\n",
      "[h264 @ 0x11dd2100] mmco: unref short failure\n",
      "[h264 @ 0x11dd2100] mmco: unref short failure\n",
      "[h264 @ 0x11c66b40] mmco: unref short failure\n",
      "[h264 @ 0x11c66b40] mmco: unref short failure\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 22.37 GiB of which 423.19 MiB is free. Process 3886228 has 1.45 GiB memory in use. Including non-PyTorch memory, this process has 20.50 GiB memory in use. Of the allocated memory 20.32 GiB is allocated by PyTorch, and 23.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y)\n\u001b[1;32m     39\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 52\u001b[0m, in \u001b[0;36mGestureModel3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torchvision/models/video/resnet.py:251\u001b[0m, in \u001b[0;36mVideoResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    254\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:725\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_project_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:720\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    710\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    719\u001b[0m     )\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 22.37 GiB of which 423.19 MiB is free. Process 3886228 has 1.45 GiB memory in use. Including non-PyTorch memory, this process has 20.50 GiB memory in use. Of the allocated memory 20.32 GiB is allocated by PyTorch, and 23.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 4. ì‹¤í—˜ ì‹œì‘\n",
    "# ====================================================\n",
    "RATIOS = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "EPOCHS_PER_RATIO = 3\n",
    "MODELS_TO_COMPARE = [\"r3d_18\", \"mc3_18\"]\n",
    "experiment_results = []\n",
    "\n",
    "print(f\"ğŸ“Š ì‹¤í—˜ ì‹œì‘: ë°ì´í„° ë¹„ìœ¨ {RATIOS}\")\n",
    "\n",
    "for ratio in RATIOS:\n",
    "    # ë°ì´í„°ì…‹ ì„œë¸Œì…‹ ìƒì„± (ë¹„ìœ¨ë§Œí¼ ëœë¤ ìƒ˜í”Œë§)\n",
    "    total_len = len(train_dataset)\n",
    "    if total_len == 0: break # ë°ì´í„° ì—†ìœ¼ë©´ ì¤‘ë‹¨\n",
    "    \n",
    "    subset_len = int(total_len * ratio)\n",
    "    if subset_len == 0: subset_len = 1 # ìµœì†Œ 1ê°œëŠ” ë³´ì¥\n",
    "    \n",
    "    indices = np.random.choice(total_len, subset_len, replace=False)\n",
    "    subset = Subset(train_dataset, indices)\n",
    "    \n",
    "    curr_loader = DataLoader(subset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "    print(f\"\\nâš¡ [Data: {int(ratio*100)}%] ìƒ˜í”Œ ìˆ˜: {len(subset)}ê°œ\")\n",
    "\n",
    "    for model_name in MODELS_TO_COMPARE:\n",
    "        # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        model = GestureModel3D(backbone_name=model_name).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # í•™ìŠµ\n",
    "        model.train()\n",
    "        for ep in range(EPOCHS_PER_RATIO):\n",
    "            for x, y in curr_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # í‰ê°€\n",
    "        model.eval()\n",
    "        preds_list, targets_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                out = model(x)\n",
    "                preds = torch.argmax(out, dim=1).cpu().numpy()\n",
    "                targets = y.cpu().numpy()\n",
    "                preds_list.extend(preds)\n",
    "                targets_list.extend(targets)\n",
    "        \n",
    "        acc = accuracy_score(targets_list, preds_list)\n",
    "        f1 = f1_score(targets_list, preds_list, average='weighted', zero_division=0)\n",
    "        \n",
    "        experiment_results.append({\n",
    "            \"Ratio\": int(ratio * 100),\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": acc,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "        print(f\"   ğŸ‘‰ {model_name}: Acc {acc:.4f}, F1 {f1:.4f}\")\n",
    "\n",
    "# ====================================================\n",
    "# 5. ê²°ê³¼ ì‹œê°í™”\n",
    "# ====================================================\n",
    "if experiment_results:\n",
    "    df = pd.DataFrame(experiment_results)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    sns.lineplot(ax=axes[0], data=df, x=\"Ratio\", y=\"Accuracy\", hue=\"Model\", marker='o')\n",
    "    axes[0].set_title(\"Accuracy by Data Size\")\n",
    "    axes[0].set_ylim(0, 1.0)\n",
    "    \n",
    "    sns.lineplot(ax=axes[1], data=df, x=\"Ratio\", y=\"F1-Score\", hue=\"Model\", marker='o')\n",
    "    axes[1].set_title(\"F1-Score by Data Size\")\n",
    "    axes[1].set_ylim(0, 1.0)\n",
    "    \n",
    "    plt.show()\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"âš ï¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84bed4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
